[
  {
    "objectID": "timeseries.html",
    "href": "timeseries.html",
    "title": "My Portfolio",
    "section": "",
    "text": "1. Install and Load Necessary Packages\ninstall.packages(\"fpp2\")\ninstall.packages(\"TTR\")\nlibrary(fpp2)\nlibrary(TTR)\ndata(\"elecdemand\")\n\n\n2. Plot the Time Series Data\nautoplot(elecdemand[, \"Demand\"], main = \"Electricity Demand in Victoria, Australia\",\n         xlab = \"Time\", ylab = \"Demand (MW)\", col = \"blue\")\n\n\n3. Experiment with Different Window Sizes\n# Calculate moving averages with different window sizes\nmoving_avg_24 &lt;- SMA(elecdemand[, \"Demand\"], n = 24)  # 12 hours\nmoving_avg_96 &lt;- SMA(elecdemand[, \"Demand\"], n = 96)  # 48 hours\nmoving_avg_192 &lt;- SMA(elecdemand[, \"Demand\"], n = 192)  # 96 hours\n\n# Plot original data and moving averages with different window sizes\nplot(elecdemand[, \"Demand\"], type = \"l\", col = \"blue\", main = \"Electricity Demand with Moving Averages\",\n     xlab = \"Time\", ylab = \"Demand (MW)\")\nlines(moving_avg_24, col = \"red\", lty = 2)\nlines(moving_avg_96, col = \"green\", lty = 3)\nlines(moving_avg_192, col = \"orange\", lty = 4)\nlegend(\"topright\", legend = c(\"Original Data\", \"24-period MA\", \"96-period MA\", \"192-period MA\"),\n       col = c(\"blue\", \"red\", \"green\", \"orange\"), lty = c(1, 2, 3, 4), lwd = 2)\n\n\n4. Create and Plot the Moving Average Model\n# Calculate the moving average with a window size of 48 (24 hours, since data is half-hourly)\nmoving_avg &lt;- SMA(elecdemand[, \"Demand\"], n = 48)\n\n# Plot the original time series and the moving average\nplot(elecdemand[, \"Demand\"], type = \"l\", col = \"blue\", main = \"Electricity Demand with Moving Average\",\n     xlab = \"Time\", ylab = \"Demand (MW)\")\nlines(moving_avg, col = \"red\")\nlegend(\"topright\", legend = c(\"Original Data\", \"48-period MA\"), col = c(\"blue\", \"red\"), lty = 1, lwd = 2)\n\n\n5. Evaluate Model Performance\n# Calculate Mean Absolute Error (MAE)\nmae &lt;- mean(abs(elecdemand[, \"Demand\"] - moving_avg), na.rm = TRUE)\n\n# Calculate Root Mean Squared Error (RMSE)\nrmse &lt;- sqrt(mean((elecdemand[, \"Demand\"] - moving_avg)^2, na.rm = TRUE))\n\nprint(paste(\"Mean Absolute Error (MAE):\", round(mae, 2)))\nprint(paste(\"Root Mean Squared Error (RMSE):\", round(rmse, 2)))"
  },
  {
    "objectID": "begin.html",
    "href": "begin.html",
    "title": "Fear Not!",
    "section": "",
    "text": "If you’re new to RStudio, you’ve come to the right place. Beyond my advanced projects, I’ve created this beginner-friendly page to share some basic computations. Whether you’re an employer or a colleague visiting, I’m delighted to have your interest.\nWe all have our first “sense of horror” moment when we first encounter RStudio. For me, it was during my first year in a biostatistics class with my teacher, Tessa. She had us working on projects using RStudio during class, and those initial weeks were chaotic. We were all frustrated and terrified by what appeared on our screens—scary red errors seemed to pop up every second, and we felt like we were making no progress at all. However, Tessa remained by our side. She dedicated the beginning of every class to addressing the typical errors we encountered, guiding us toward smart tactics and helpful online resources.\nBy the end of the semester, our perception of RStudio had completely changed. Tessa helped us realize that it wasn’t some dark abyss from which no student returns; it was a powerful tool that made statistics easier to compute. We bid farewell to the daunting equations we were accustomed to and began computing statistical techniques at a rapid pace. To this day, every one of my classmates continues to use RStudio with excitement and without fear. This software is truly your best friend if you use it wisely.\n\n\n\nAsking for Help\nPackages\nReading Data\nTidying Data\nBuilding Functions\nData Visualization\n\n\n\n\nMost experts mention asking for help at the end of their tutorials, but I believe it deserves to be first. Learning R involves seeking a lot of assistance, and there are countless online resources available to answer your questions. Below are some of my favorite resources:\n\nThe Help Center in RStudio: Located in the bottom right pane of your window, the Help Center provides detailed explanations of each command in R. You can learn about the purpose of each function, as well as their sub-commands and properties. You can also type any command with the ? symbol in front of it and it will direct you to the same window (e.g., ?plot()).\nThe R Project Website: Provides comprehensive documentation and manuals.\nRStudio Community: An active forum where you can ask questions.\nR-Bloggers: A fantastic resource for tutorials, tips, and tricks from the R community.\nYouTube: Useful tutorials for visual learners.\nReddit: A community where everyone deals with problems using R. Visit Reddit to view a wide range of questions and answers from people around the globe.\nGenerative AI: Appropriate in certain circumstances. Use with caution, as AI can occasionally produce incorrect solutions to your code.\n\nBy leveraging these resources, you can make your learning journey with R more efficient and less daunting.\n\n\n\nR packages are collections of functions, data, and documentation that extend the capabilities of base R, making it easier and more efficient to perform a wide range of tasks.\n\nSimplify Complex Tasks\n\nPackages can simplify complex tasks into manageable functions. For instance, data visualization with ggplot2 or data manipulation with dplyr breaks down intricate processes into straightforward commands.\n\nEnhance Data Analysis\n\nPackages like tidyverse offer tools for data import, tidying, transformation, visualization, and modeling. These integrated tools follow a consistent philosophy, making data analysis more intuitive.\n\nImprove Data Visualization\n\nPackages such as ggplot2, plotly, and shiny help create advanced and interactive data visualizations. This makes it easier to explore and communicate data insights.\n\nStreamline Workflow\n\nPackages like knitr and rmarkdown allow for dynamic report generation, integrating code and its output in a single document. This streamlines the workflow from data analysis to reporting.\n\nAccess Vast Amounts of Data\n\nPackages like dplyr and tidyr help in efficiently handling and processing large datasets. This is particularly useful for big data applications where performance is crucial.\n\n\n\n\n\n\n\nYou can find your data either using the file.choose() function or by clicking on Files in the bottom right window. Make sure to save your new data in your local Environment in the top right window.\n\n\n\nThe most common format for data files is CSV (Comma-Separated Values). The readr package from the tidyverse provides functions to read CSV files efficiently.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Read CSV file\ndata &lt;- read.csv(\"exercise_data.csv\")\n\n\n\n\n\nlibrary(readxl)\n\n# Read the first sheet\ndata &lt;- read_excel(\"/Users/owencallahan/Downloads/Fitness.xlsx\")\n\n# Read a specific sheet by name\ndata_sheet &lt;- read_excel(\"/Users/owencallahan/Downloads/Fitness.xlsx\", sheet = \"Sheet1\")\n\n# Read a specific sheet by index\ndata_index &lt;- read_excel(\"/Users/owencallahan/Downloads/Fitness.xlsx\", sheet = 1)\n\n\n\n\n\ndata &lt;- c(1,2,3,4)\nwrite.csv(data, \"my_data.csv\")\n\n\n\n\n\nTidying data is an essential part of data analysis in R, making your data easier to work with and analyze. We’ll use the tidyverse suite of packages, which includes dplyr, tidyr, and readr among others. Below is a step-by-step tutorial with code examples to demonstrate how to tidy data in R.\n\n\nPivoting longer:\n\nwide_data &lt;- tibble(\n  id = 1:3,\n  age = c(25, 30, 35),\n  height = c(175, 180, 165),\n  weight = c(70, 80, 65)\n)\n\n# Convert to long format\nlong_data &lt;- wide_data %&gt;%\n  pivot_longer(cols = c(age, height, weight), names_to = \"measure\", values_to = \"value\")\n\nprint(long_data)\n\n# A tibble: 9 × 3\n     id measure value\n  &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1 age        25\n2     1 height    175\n3     1 weight     70\n4     2 age        30\n5     2 height    180\n6     2 weight     80\n7     3 age        35\n8     3 height    165\n9     3 weight     65\n\n\nPivoting wider:\n\n# Convert back to wide format\nwide_again &lt;- long_data %&gt;%\n  pivot_wider(names_from = measure, values_from = value)\n\nprint(wide_again)\n\n# A tibble: 3 × 4\n     id   age height weight\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1    25    175     70\n2     2    30    180     80\n3     3    35    165     65\n\n\nSeparate columns:\n\ndata &lt;- tibble(\n  name = c(\"John_Doe\", \"Jane_Smith\", \"Alice_Johnson\")\n)\n\n# Separate into first and last names\nseparated_data &lt;- data %&gt;%\n  separate(name, into = c(\"first_name\", \"last_name\"), sep = \"_\")\n\nprint(separated_data)\n\n# A tibble: 3 × 2\n  first_name last_name\n  &lt;chr&gt;      &lt;chr&gt;    \n1 John       Doe      \n2 Jane       Smith    \n3 Alice      Johnson  \n\n\nUnite columns:\n\nunited_data &lt;- separated_data %&gt;%\n  unite(\"full_name\", first_name, last_name, sep = \" \")\n\n\n\n\nSelecting Columns\n\nselected_data &lt;- mtcars %&gt;%\n  select(cyl, gear)\n\nhead(selected_data)\n\n                  cyl gear\nMazda RX4           6    4\nMazda RX4 Wag       6    4\nDatsun 710          4    4\nHornet 4 Drive      6    3\nHornet Sportabout   8    3\nValiant             6    3\n\n\nFiltering Rows\n\nfiltered_data &lt;- mtcars %&gt;%\n  filter(gear &lt; 4)\n\nhead(filtered_data)\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 450SE        16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL        17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n\n\nMutating Columns\n\nmutated_data &lt;- mtcars %&gt;%\n  mutate(speed.scale = qsec/wt)\n\nhead(mutated_data %&gt;%\n        select(speed.scale))\n\n                  speed.scale\nMazda RX4            6.282443\nMazda RX4 Wag        5.920000\nDatsun 710           8.021552\nHornet 4 Drive       6.046656\nHornet Sportabout    4.947674\nValiant              5.843931\n\n\nSummarizing Data\n\nsummary_data &lt;- mtcars %&gt;%\n  group_by(gear) %&gt;%\n  summarize(\n    average_wt = mean(wt),\n    count = n()\n  )\n\nprint(summary_data)\n\n# A tibble: 3 × 3\n   gear average_wt count\n  &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1     3       3.89    15\n2     4       2.62    12\n3     5       2.63     5\n\n\n\n\n\n\n# Identify missing values\nmissing_data &lt;- mtcars %&gt;%\n  filter(is.na(wt))\n\n# Remove rows with missing values\ncleaned_data &lt;- data %&gt;%\n  drop_na()\n\n# Replace missing values with a specific value\nfilled_data &lt;- data %&gt;%\n  replace_na(list(height = 170, weight = 70))\n\n\n\n\n\ndata1 &lt;- tibble(\n  id = 1:3,\n  name = c(\"John\", \"Jane\", \"Alice\")\n)\n\ndata2 &lt;- tibble(\n  id = 1:3,\n  score = c(85, 90, 78)\n)\n\n# Inner join\njoined_data &lt;- data1 %&gt;%\n  inner_join(data2, by = \"id\")\n\nprint(joined_data)\n\n# A tibble: 3 × 3\n     id name  score\n  &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 John     85\n2     2 Jane     90\n3     3 Alice    78\n\n\n\n\n\n\nWhen I learned how to create my own functions, I felt like the creative side of R expanded beyond my expectations. I could tailor a command to address exactly what I needed to perform on a given set of data.\n\n\nThe function() command is a fundamental building block in R, enabling users to create their own functions. This is crucial for both simplifying repetitive tasks and organizing code in a clean, efficient manner. Here’s what the function() command can do for new learners using R:\n\nEncapsulate Repetitive Tasks\n\nCreating functions allows you to encapsulate repetitive code into reusable blocks. This reduces redundancy and makes your scripts more concise and readable. For example, if you frequently perform the same data transformation, you can write a function for it and call it whenever needed.\n\nOrganize Code\n\nFunctions help in organizing code logically. By breaking down complex procedures into smaller, manageable functions, you make your code more modular and easier to debug and maintain.\n\nImprove Readability\n\nUsing functions can significantly enhance the readability of your code. Descriptive function names and clear parameter definitions help others (and your future self) understand the purpose and usage of the code more quickly.\n\nParameterization\n\nFunctions allow you to use parameters to make your code more flexible. Instead of hard-coding values, you can pass different arguments to your functions, making them adaptable to various inputs and scenarios.\n\nEnhance Reproducibility\n\nFunctions contribute to reproducibility in your analyses. By encapsulating specific tasks, you ensure that the same operations can be repeated with different data or settings, leading to consistent results.\n\nPromote Code Reuse\n\nOnce you write a function, you can reuse it across different projects. This saves time and effort, as you don’t need to rewrite the same code for similar tasks.\n\n\nExample of Using function() in R\n\n# Define a function to calculate the mean of a numeric vector\ncalculate_mean &lt;- function(numbers) {\n  mean_value &lt;- mean(numbers)\n  return(mean_value)\n}\n\n# Use the function with a numeric vector\nsample_data &lt;- c(4, 8, 15, 16, 23, 42)\naverage &lt;- calculate_mean(sample_data)\nprint(average)\n\n[1] 18\n\n\n\n\n\nCorrelation measures the strength and direction of the relationship between two variables.\n\n# Correlation matrix\ncor_matrix &lt;- cor(mtcars)\nprint(cor_matrix)\n\n            mpg        cyl       disp         hp        drat         wt\nmpg   1.0000000 -0.8521620 -0.8475514 -0.7761684  0.68117191 -0.8676594\ncyl  -0.8521620  1.0000000  0.9020329  0.8324475 -0.69993811  0.7824958\ndisp -0.8475514  0.9020329  1.0000000  0.7909486 -0.71021393  0.8879799\nhp   -0.7761684  0.8324475  0.7909486  1.0000000 -0.44875912  0.6587479\ndrat  0.6811719 -0.6999381 -0.7102139 -0.4487591  1.00000000 -0.7124406\nwt   -0.8676594  0.7824958  0.8879799  0.6587479 -0.71244065  1.0000000\nqsec  0.4186840 -0.5912421 -0.4336979 -0.7082234  0.09120476 -0.1747159\nvs    0.6640389 -0.8108118 -0.7104159 -0.7230967  0.44027846 -0.5549157\nam    0.5998324 -0.5226070 -0.5912270 -0.2432043  0.71271113 -0.6924953\ngear  0.4802848 -0.4926866 -0.5555692 -0.1257043  0.69961013 -0.5832870\ncarb -0.5509251  0.5269883  0.3949769  0.7498125 -0.09078980  0.4276059\n            qsec         vs          am       gear        carb\nmpg   0.41868403  0.6640389  0.59983243  0.4802848 -0.55092507\ncyl  -0.59124207 -0.8108118 -0.52260705 -0.4926866  0.52698829\ndisp -0.43369788 -0.7104159 -0.59122704 -0.5555692  0.39497686\nhp   -0.70822339 -0.7230967 -0.24320426 -0.1257043  0.74981247\ndrat  0.09120476  0.4402785  0.71271113  0.6996101 -0.09078980\nwt   -0.17471588 -0.5549157 -0.69249526 -0.5832870  0.42760594\nqsec  1.00000000  0.7445354 -0.22986086 -0.2126822 -0.65624923\nvs    0.74453544  1.0000000  0.16834512  0.2060233 -0.56960714\nam   -0.22986086  0.1683451  1.00000000  0.7940588  0.05753435\ngear -0.21268223  0.2060233  0.79405876  1.0000000  0.27407284\ncarb -0.65624923 -0.5696071  0.05753435  0.2740728  1.00000000\n\n# Correlation between two variables\ncor(mtcars$mpg, mtcars$hp)\n\n[1] -0.7761684\n\n\n\n\n\nLinear regression models the relationship between a dependent variable and one or more independent variables.\n\n# Simple linear regression\nmodel &lt;- lm(mpg ~ hp, data = mtcars)\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7121 -2.1122 -0.8854  1.5819  8.2360 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 30.09886    1.63392  18.421  &lt; 2e-16 ***\nhp          -0.06823    0.01012  -6.742 1.79e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 30 degrees of freedom\nMultiple R-squared:  0.6024,    Adjusted R-squared:  0.5892 \nF-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n\n# Multiple linear regression\nmodel2 &lt;- lm(mpg ~ hp + wt, data = mtcars)\nsummary(model2)\n\n\nCall:\nlm(formula = mpg ~ hp + wt, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\n\n\n\n\nANOVA tests the difference in means among groups.\n\n# One-way ANOVA\nanova_model &lt;- aov(mpg ~ as.factor(cyl), data = mtcars)\nsummary(anova_model)\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nas.factor(cyl)  2  824.8   412.4    39.7 4.98e-09 ***\nResiduals      29  301.3    10.4                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Two-way ANOVA\nanova_model2 &lt;- aov(mpg ~ as.factor(cyl) + as.factor(gear), data = mtcars)\nsummary(anova_model2)\n\n                Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nas.factor(cyl)   2  824.8   412.4   38.00 1.41e-08 ***\nas.factor(gear)  2    8.3     4.1    0.38    0.687    \nResiduals       27  293.0    10.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nT-tests compare the means of two groups.\n\n# One-sample t-test\nt.test(mtcars$mpg, mu = 20)\n\n\n    One Sample t-test\n\ndata:  mtcars$mpg\nt = 0.08506, df = 31, p-value = 0.9328\nalternative hypothesis: true mean is not equal to 20\n95 percent confidence interval:\n 17.91768 22.26357\nsample estimates:\nmean of x \n 20.09062 \n\n# Two-sample t-test\nt.test(mpg ~ as.factor(am), data = mtcars)\n\n\n    Welch Two Sample t-test\n\ndata:  mpg by as.factor(am)\nt = -3.7671, df = 18.332, p-value = 0.001374\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -11.280194  -3.209684\nsample estimates:\nmean in group 0 mean in group 1 \n       17.14737        24.39231 \n\n\n\n\n\nChi-square tests are used for categorical data to test relationships between variables.\n\n# Create a contingency table\ntable_data &lt;- table(mtcars$cyl, mtcars$gear)\n\n# Chi-square test\nchisq.test(table_data)\n\nWarning in chisq.test(table_data): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  table_data\nX-squared = 18.036, df = 4, p-value = 0.001214\n\n\n\n\n\nPCA reduces the dimensionality of the data while preserving as much variance as possible. An excellent tutorial on PCA can be found by clicking this link: https://www.youtube.com/watch?\n\n# Perform PCA\npca_result &lt;- prcomp(mtcars, scale. = TRUE)\n\n# Summary of PCA\nsummary(pca_result)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.5707 1.6280 0.79196 0.51923 0.47271 0.46000 0.3678\nProportion of Variance 0.6008 0.2409 0.05702 0.02451 0.02031 0.01924 0.0123\nCumulative Proportion  0.6008 0.8417 0.89873 0.92324 0.94356 0.96279 0.9751\n                           PC8    PC9    PC10   PC11\nStandard deviation     0.35057 0.2776 0.22811 0.1485\nProportion of Variance 0.01117 0.0070 0.00473 0.0020\nCumulative Proportion  0.98626 0.9933 0.99800 1.0000\n\n\n\n\n\nK-means clustering groups data into k clusters.\n\n# Perform K-means clustering on the mtcars dataset\nset.seed(123)\nkmeans_result &lt;- kmeans(mtcars[, c(\"mpg\", \"hp\")], centers = 3)\n\n# Add cluster results to the original mtcars data\nmtcars$cluster &lt;- as.factor(kmeans_result$cluster)\n\n# Print the first few rows to check the results\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb cluster\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4       3\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4       3\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1       3\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1       3\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2       2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1       3\n\n\n\n\n\nLogistic regression models the probability of a binary outcome.\n\n# Logistic regression\nlogit_model &lt;- glm(am ~ hp + wt, data = mtcars, family = binomial)\nsummary(logit_model)\n\n\nCall:\nglm(formula = am ~ hp + wt, family = binomial, data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) 18.86630    7.44356   2.535  0.01126 * \nhp           0.03626    0.01773   2.044  0.04091 * \nwt          -8.08348    3.06868  -2.634  0.00843 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 10.059  on 29  degrees of freedom\nAIC: 16.059\n\nNumber of Fisher Scoring iterations: 8\n\n\n\n\n\n\n# Decomposition\n\n# Sample time series data\nts_data &lt;- ts(AirPassengers, frequency = 12)\n\n# Decompose the time series\ndecomposed &lt;- decompose(ts_data)\nplot(decomposed)\n\n\n\n\n\n\n\n# Forecasting\n\n# Install and load the forecast package\nlibrary(forecast)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n# Fit an ARIMA model\nfit &lt;- auto.arima(ts_data)\nforecasted &lt;- forecast(fit, h = 12)\nplot(forecasted)\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s explore one of my favorite, and also one of the most essential packages in R, ggplot2.\n\n\nFirst, you need to install and load the ggplot2 package:\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\ninstall.packages(\"ggplot2\")\n\n\nThe downloaded binary packages are in\n    /var/folders/k3/v39j6g_x4bv7mv_xq03986d00000gn/T//Rtmpgb6tV5/downloaded_packages\n\nlibrary(ggplot2)\n\n\n\n\nggplot2 follows the grammar of graphics, which means you build plots layer by layer. The essential components are:\n\nData: The dataset you’re plotting.\nAesthetics (aes): The mapping of variables to visual properties like x and y coordinates, colors, sizes, etc.\nGeometries (geom): The type of plot you want to create (e.g., points, lines, bars).\nFacets: Subplots based on the values of one or more variables.\nScales: Control how data values are mapped to visual properties.\nCoordinate Systems: Control the coordinate space.\nThemes: Control the appearance of the plot.\n\n\n\n\nScatter Plot\n\n# Load example data\ndata(mtcars)\n\n# Create a scatter plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\n\n\n\n\nLine Plot\n\n# Create a line plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_line() +\n  labs(title = \"Line Plot of MPG vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\n\n\n\n\nBar Plot\n\n# Create a bar plot\nggplot(data = mtcars, aes(x = factor(cyl))) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Cylinder Counts\",\n       x = \"Number of Cylinders\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\nHistogram\n\n# Create a histogram\nggplot(data = mtcars, aes(x = mpg)) +\n  geom_histogram(binwidth = 2) +\n  labs(title = \"Histogram of MPG\",\n       x = \"Miles Per Gallon (MPG)\",\n       y = \"Frequency\")\n\n\n\n\n\n\n\n\nBox Plot\n\n# Create a box plot\nggplot(data = mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot() +\n  labs(title = \"Box Plot of MPG by Cylinder Count\",\n       x = \"Number of Cylinders\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\n\n\n\n\n\n\n\nAdding Colors\n\n# Scatter plot with color\nggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight by Cylinder Count\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\",\n       color = \"Cylinders\")\n\n\n\n\n\n\n\n\nAdding Size\n\n# Scatter plot with color and size\nggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl), size = hp)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight by Cylinder Count\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\",\n       color = \"Cylinders\",\n       size = \"Horsepower\")\n\n\n\n\n\n\n\n\nFaceting\n\n# Faceted scatter plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  facet_wrap(~ cyl) +\n  labs(title = \"Scatter Plot of MPG vs Weight by Cylinder Count\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\n\n\n\n\nThemes\n\n# Scatter plot with theme\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\") +\n  theme_minimal()"
  },
  {
    "objectID": "begin.html#my-learning-journey",
    "href": "begin.html#my-learning-journey",
    "title": "Fear Not!",
    "section": "",
    "text": "Asking for Help\nPackages\nReading Data\nTidying Data\nBuilding Functions\nData Visualization\n\n\n\n\nMost experts mention asking for help at the end of their tutorials, but I believe it deserves to be first. Learning R involves seeking a lot of assistance, and there are countless online resources available to answer your questions. Below are some of my favorite resources:\n\nThe Help Center in RStudio: Located in the bottom right pane of your window, the Help Center provides detailed explanations of each command in R. You can learn about the purpose of each function, as well as their sub-commands and properties. You can also type any command with the ? symbol in front of it and it will direct you to the same window (e.g., ?plot()).\nThe R Project Website: Provides comprehensive documentation and manuals.\nRStudio Community: An active forum where you can ask questions.\nR-Bloggers: A fantastic resource for tutorials, tips, and tricks from the R community.\nYouTube: Useful tutorials for visual learners.\nReddit: A community where everyone deals with problems using R. Visit Reddit to view a wide range of questions and answers from people around the globe.\nGenerative AI: Appropriate in certain circumstances. Use with caution, as AI can occasionally produce incorrect solutions to your code.\n\nBy leveraging these resources, you can make your learning journey with R more efficient and less daunting.\n\n\n\nR packages are collections of functions, data, and documentation that extend the capabilities of base R, making it easier and more efficient to perform a wide range of tasks.\n\nSimplify Complex Tasks\n\nPackages can simplify complex tasks into manageable functions. For instance, data visualization with ggplot2 or data manipulation with dplyr breaks down intricate processes into straightforward commands.\n\nEnhance Data Analysis\n\nPackages like tidyverse offer tools for data import, tidying, transformation, visualization, and modeling. These integrated tools follow a consistent philosophy, making data analysis more intuitive.\n\nImprove Data Visualization\n\nPackages such as ggplot2, plotly, and shiny help create advanced and interactive data visualizations. This makes it easier to explore and communicate data insights.\n\nStreamline Workflow\n\nPackages like knitr and rmarkdown allow for dynamic report generation, integrating code and its output in a single document. This streamlines the workflow from data analysis to reporting.\n\nAccess Vast Amounts of Data\n\nPackages like dplyr and tidyr help in efficiently handling and processing large datasets. This is particularly useful for big data applications where performance is crucial.\n\n\n\n\n\n\n\nYou can find your data either using the file.choose() function or by clicking on Files in the bottom right window. Make sure to save your new data in your local Environment in the top right window.\n\n\n\nThe most common format for data files is CSV (Comma-Separated Values). The readr package from the tidyverse provides functions to read CSV files efficiently.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Read CSV file\ndata &lt;- read.csv(\"exercise_data.csv\")\n\n\n\n\n\nlibrary(readxl)\n\n# Read the first sheet\ndata &lt;- read_excel(\"/Users/owencallahan/Downloads/Fitness.xlsx\")\n\n# Read a specific sheet by name\ndata_sheet &lt;- read_excel(\"/Users/owencallahan/Downloads/Fitness.xlsx\", sheet = \"Sheet1\")\n\n# Read a specific sheet by index\ndata_index &lt;- read_excel(\"/Users/owencallahan/Downloads/Fitness.xlsx\", sheet = 1)\n\n\n\n\n\ndata &lt;- c(1,2,3,4)\nwrite.csv(data, \"my_data.csv\")\n\n\n\n\n\nTidying data is an essential part of data analysis in R, making your data easier to work with and analyze. We’ll use the tidyverse suite of packages, which includes dplyr, tidyr, and readr among others. Below is a step-by-step tutorial with code examples to demonstrate how to tidy data in R.\n\n\nPivoting longer:\n\nwide_data &lt;- tibble(\n  id = 1:3,\n  age = c(25, 30, 35),\n  height = c(175, 180, 165),\n  weight = c(70, 80, 65)\n)\n\n# Convert to long format\nlong_data &lt;- wide_data %&gt;%\n  pivot_longer(cols = c(age, height, weight), names_to = \"measure\", values_to = \"value\")\n\nprint(long_data)\n\n# A tibble: 9 × 3\n     id measure value\n  &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1 age        25\n2     1 height    175\n3     1 weight     70\n4     2 age        30\n5     2 height    180\n6     2 weight     80\n7     3 age        35\n8     3 height    165\n9     3 weight     65\n\n\nPivoting wider:\n\n# Convert back to wide format\nwide_again &lt;- long_data %&gt;%\n  pivot_wider(names_from = measure, values_from = value)\n\nprint(wide_again)\n\n# A tibble: 3 × 4\n     id   age height weight\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1    25    175     70\n2     2    30    180     80\n3     3    35    165     65\n\n\nSeparate columns:\n\ndata &lt;- tibble(\n  name = c(\"John_Doe\", \"Jane_Smith\", \"Alice_Johnson\")\n)\n\n# Separate into first and last names\nseparated_data &lt;- data %&gt;%\n  separate(name, into = c(\"first_name\", \"last_name\"), sep = \"_\")\n\nprint(separated_data)\n\n# A tibble: 3 × 2\n  first_name last_name\n  &lt;chr&gt;      &lt;chr&gt;    \n1 John       Doe      \n2 Jane       Smith    \n3 Alice      Johnson  \n\n\nUnite columns:\n\nunited_data &lt;- separated_data %&gt;%\n  unite(\"full_name\", first_name, last_name, sep = \" \")\n\n\n\n\nSelecting Columns\n\nselected_data &lt;- mtcars %&gt;%\n  select(cyl, gear)\n\nhead(selected_data)\n\n                  cyl gear\nMazda RX4           6    4\nMazda RX4 Wag       6    4\nDatsun 710          4    4\nHornet 4 Drive      6    3\nHornet Sportabout   8    3\nValiant             6    3\n\n\nFiltering Rows\n\nfiltered_data &lt;- mtcars %&gt;%\n  filter(gear &lt; 4)\n\nhead(filtered_data)\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 450SE        16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL        17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n\n\nMutating Columns\n\nmutated_data &lt;- mtcars %&gt;%\n  mutate(speed.scale = qsec/wt)\n\nhead(mutated_data %&gt;%\n        select(speed.scale))\n\n                  speed.scale\nMazda RX4            6.282443\nMazda RX4 Wag        5.920000\nDatsun 710           8.021552\nHornet 4 Drive       6.046656\nHornet Sportabout    4.947674\nValiant              5.843931\n\n\nSummarizing Data\n\nsummary_data &lt;- mtcars %&gt;%\n  group_by(gear) %&gt;%\n  summarize(\n    average_wt = mean(wt),\n    count = n()\n  )\n\nprint(summary_data)\n\n# A tibble: 3 × 3\n   gear average_wt count\n  &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1     3       3.89    15\n2     4       2.62    12\n3     5       2.63     5\n\n\n\n\n\n\n# Identify missing values\nmissing_data &lt;- mtcars %&gt;%\n  filter(is.na(wt))\n\n# Remove rows with missing values\ncleaned_data &lt;- data %&gt;%\n  drop_na()\n\n# Replace missing values with a specific value\nfilled_data &lt;- data %&gt;%\n  replace_na(list(height = 170, weight = 70))\n\n\n\n\n\ndata1 &lt;- tibble(\n  id = 1:3,\n  name = c(\"John\", \"Jane\", \"Alice\")\n)\n\ndata2 &lt;- tibble(\n  id = 1:3,\n  score = c(85, 90, 78)\n)\n\n# Inner join\njoined_data &lt;- data1 %&gt;%\n  inner_join(data2, by = \"id\")\n\nprint(joined_data)\n\n# A tibble: 3 × 3\n     id name  score\n  &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 John     85\n2     2 Jane     90\n3     3 Alice    78\n\n\n\n\n\n\nWhen I learned how to create my own functions, I felt like the creative side of R expanded beyond my expectations. I could tailor a command to address exactly what I needed to perform on a given set of data.\nThe function() command is a fundamental building block in R, enabling users to create their own functions. This is crucial for both simplifying repetitive tasks and organizing code in a clean, efficient manner. Here’s what the function() command can do for new learners using R:\n\nEncapsulate Repetitive Tasks\n\nCreating functions allows you to encapsulate repetitive code into reusable blocks. This reduces redundancy and makes your scripts more concise and readable. For example, if you frequently perform the same data transformation, you can write a function for it and call it whenever needed.\n\nOrganize Code\n\nFunctions help in organizing code logically. By breaking down complex procedures into smaller, manageable functions, you make your code more modular and easier to debug and maintain.\n\nImprove Readability\n\nUsing functions can significantly enhance the readability of your code. Descriptive function names and clear parameter definitions help others (and your future self) understand the purpose and usage of the code more quickly.\n\nParameterization\n\nFunctions allow you to use parameters to make your code more flexible. Instead of hard-coding values, you can pass different arguments to your functions, making them adaptable to various inputs and scenarios.\n\nEnhance Reproducibility\n\nFunctions contribute to reproducibility in your analyses. By encapsulating specific tasks, you ensure that the same operations can be repeated with different data or settings, leading to consistent results.\n\nPromote Code Reuse\n\nOnce you write a function, you can reuse it across different projects. This saves time and effort, as you don’t need to rewrite the same code for similar tasks.\n\n\nExample of Using function() in R\n\n# Define a function to calculate the mean of a numeric vector\ncalculate_mean &lt;- function(numbers) {\n  mean_value &lt;- mean(numbers)\n  return(mean_value)\n}\n\n# Use the function with a numeric vector\nsample_data &lt;- c(4, 8, 15, 16, 23, 42)\naverage &lt;- calculate_mean(sample_data)\nprint(average)\n\n[1] 18\n\n\n\n\n\nLet’s explore one of my favorite, and also one of the most essential packages in R, ggplot2.\n\n\nFirst, you need to install and load the ggplot2 package:\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\ninstall.packages(\"ggplot2\")\n\n\nThe downloaded binary packages are in\n    /var/folders/k3/v39j6g_x4bv7mv_xq03986d00000gn/T//RtmpAP3t0f/downloaded_packages\n\nlibrary(ggplot2)\n\n\n\n\nggplot2 follows the grammar of graphics, which means you build plots layer by layer. The essential components are:\n\nData: The dataset you’re plotting.\nAesthetics (aes): The mapping of variables to visual properties like x and y coordinates, colors, sizes, etc.\nGeometries (geom): The type of plot you want to create (e.g., points, lines, bars).\nFacets: Subplots based on the values of one or more variables.\nScales: Control how data values are mapped to visual properties.\nCoordinate Systems: Control the coordinate space.\nThemes: Control the appearance of the plot.\n\n\n\n\nScatter Plot\n\n# Load example data\ndata(mtcars)\n\n# Create a scatter plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\n\n\n\n\nLine Plot\n\n# Create a line plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_line() +\n  labs(title = \"Line Plot of MPG vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\n\n\n\n\nBar Plot\n\n# Create a bar plot\nggplot(data = mtcars, aes(x = factor(cyl))) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Cylinder Counts\",\n       x = \"Number of Cylinders\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\nHistogram\n\n# Create a histogram\nggplot(data = mtcars, aes(x = mpg)) +\n  geom_histogram(binwidth = 2) +\n  labs(title = \"Histogram of MPG\",\n       x = \"Miles Per Gallon (MPG)\",\n       y = \"Frequency\")\n\n\n\n\n\n\n\n\nBox Plot\n\n# Create a box plot\nggplot(data = mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot() +\n  labs(title = \"Box Plot of MPG by Cylinder Count\",\n       x = \"Number of Cylinders\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\n\n\n\n\n\n\n\nAdding Colors\n\n# Scatter plot with color\nggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight by Cylinder Count\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\",\n       color = \"Cylinders\")\n\n\n\n\n\n\n\n\nAdding Size\n\n# Scatter plot with color and size\nggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl), size = hp)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight by Cylinder Count\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\",\n       color = \"Cylinders\",\n       size = \"Horsepower\")\n\n\n\n\n\n\n\n\nFaceting\n\n# Faceted scatter plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  facet_wrap(~ cyl) +\n  labs(title = \"Scatter Plot of MPG vs Weight by Cylinder Count\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\n\n\n\n\nThemes\n\n# Scatter plot with theme\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\") +\n  theme_minimal()"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Welcome to My Website",
    "section": "About Me",
    "text": "About Me\nHi there! I’m Owen, a passionate data enthusiast, R programmer, and lifelong learner. Here you’ll find information about my background, skills, and education. Whether you’re a potential employer, colleague, or collaborator, I’m excited to share my journey with you."
  },
  {
    "objectID": "index.html#dive-into-my-r-projects",
    "href": "index.html#dive-into-my-r-projects",
    "title": "Welcome to My Website",
    "section": "Dive into My R Projects",
    "text": "Dive into My R Projects\nExplore my portfolio of R projects, where I showcase my data analysis, visualization, and programming skills."
  },
  {
    "objectID": "index.html#learn-r",
    "href": "index.html#learn-r",
    "title": "Welcome to My Website",
    "section": "Learn R",
    "text": "Learn R\nIf you’re new to R or if you are a part of my tutoring program, you’ve come to the right place. Here, I provide beginner-friendly tutorials and resources to help you navigate the basics of R programming."
  },
  {
    "objectID": "index.html#connect-with-me",
    "href": "index.html#connect-with-me",
    "title": "Welcome to My Website",
    "section": "Connect with Me",
    "text": "Connect with Me\nInterested in collaborating on a project, discussing data science topics, or just saying hello? Don’t hesitate to reach out! Connect with me via email at owen11callahan@gmail.com"
  },
  {
    "objectID": "multilevel.html",
    "href": "multilevel.html",
    "title": "Project Description",
    "section": "",
    "text": "Project Description\nThis is a research project I developed during my Master’s Degree in Environmental Psychology. It includes a hierarchical model that explores the cross-level dynamics within communities in the Netherlands.\nI am still motivated about executing this project in a practical setting because it addresses contemporary challenges in ethnically heterogeneous neighborhoods and provides an unique approach to understanding social cohesion. It assesses the individual-level, cognitive processes in neighborhood behavior through spatial measurement, which can offer alternative evidence for policymakers’ views toward contextual level action (e.g., developing economically vital cities, demolition, and housing renovation) (Van Kempen & Bolt, 2009). By bridging the gap between individual-level processes and neighborhood dynamics, this research has the potential to inform policy interventions and community development initiatives.\n\nOutcome Variable\n\nboundary: combined value of perceived neighborhood scale and perceived trust in neighbors\n\nDemographics:\n\nage: Normally distributed continuous variable.\neducation: Dichotomous variable generated using rbinom with a probability of 0.6 for university education.\nincome: Ordinal variable sampled from 1 to 5.\nethnicity: Categorical variable sampled from four categories “A”, “B”, “C”, and “D”.\nlength_res: Normally distributed continuous variable for length of residency.\n\nPhysical Disorder:\n\nphysical_disorder: Ordinal variable sampled from 1 to 5.\n\nPerceived Collective Efficacy:\n\nperceived_collective_efficacy: Normally distributed continuous variable.\n\nIntergroup Leader:\n\nleadership: Dichotomous variable generated using rbinom.\n\nPersonalized Features:\n\nnames, art, playgrounds: Dichotomous variables generated using rbinom.\n\n\n\nStep 1: Load Mapping data\n\n# Load the libraries\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(geojsonio)\n\nRegistered S3 method overwritten by 'geojsonsf':\n  method        from   \n  print.geojson geojson\n\n\n\nAttaching package: 'geojsonio'\n\n\nThe following object is masked from 'package:base':\n\n    pretty\n\nlibrary(sp)\n\n# Read the GeoJSON file\nmap_data &lt;- geojson_read(\"map_data.geojson\", what = \"sp\")\n\n# Convert to sf object for easier handling\nmap_sf &lt;- st_as_sf(map_data)\n\n# Calculate the area of each drawn boundary\nmap_sf &lt;- map_sf %&gt;%\n  mutate(area = st_area(geometry))\n\n# Convert area to numeric\nmap_sf$area &lt;- as.numeric(map_sf$area)\n\n# Plot a sample of the boundaries\nggplot(data = map_sf) +\n  geom_sf() +\n  theme_minimal() +\n  labs(title = \"Neighborhood Boundaries\",\n       caption = \"Generated for demonstration purposes\")\n\n\n\n\n\n\n\n\n\n\nStep 2: Load Survey Data\n\n# Load necessary libraries\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\ninstall.packages(\"lme4\")\n\n\nThe downloaded binary packages are in\n    /var/folders/k3/v39j6g_x4bv7mv_xq03986d00000gn/T//Rtmpnle3ow/downloaded_packages\n\ninstall.packages(\"car\")\n\n\nThe downloaded binary packages are in\n    /var/folders/k3/v39j6g_x4bv7mv_xq03986d00000gn/T//Rtmpnle3ow/downloaded_packages\n\ninstall.packages(\"ggplot2\")\n\n\nThe downloaded binary packages are in\n    /var/folders/k3/v39j6g_x4bv7mv_xq03986d00000gn/T//Rtmpnle3ow/downloaded_packages\n\nlibrary(lme4)\n\nLoading required package: Matrix\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nlibrary(ggplot2)\n\n# Load the dataset\ndata &lt;- read.csv(\"multilevel_model_data.csv\")\n\n# Merge datasets\n# Create the `id` column\ndata$id &lt;- 1:nrow(data)\ndata &lt;- data %&gt;%\n  left_join(map_sf %&gt;% select(id, area), by = \"id\")\n\n\n\nStep 3: Exploratory Data Analysis\nBefore I compare the models, let’s take a look at the full model and understand out data a bit better.\n\n# Fit the multilevel model\nmultilevel_model &lt;- lmer(boundary ~ age + education + income + ethnicity + length_res + physical_disorder + perceived_collective_efficacy + facilities + names + art + playgrounds + leadership + (1 | neighborhood_id), data = data)\n\n# Print summary of the model\nsummary(multilevel_model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: boundary ~ age + education + income + ethnicity + length_res +  \n    physical_disorder + perceived_collective_efficacy + facilities +  \n    names + art + playgrounds + leadership + (1 | neighborhood_id)\n   Data: data\n\nREML criterion at convergence: 12877.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.9966 -0.7063  0.0071  0.6710  3.2348 \n\nRandom effects:\n Groups          Name        Variance Std.Dev.\n neighborhood_id (Intercept)  0.5319  0.7293  \n Residual                    91.6096  9.5713  \nNumber of obs: 1750, groups:  neighborhood_id, 25\n\nFixed effects:\n                               Estimate Std. Error t value\n(Intercept)                   49.019346   1.575249  31.118\nage                           -0.009148   0.015198  -0.602\neducation                     -0.211335   0.469636  -0.450\nincome                        -0.425076   0.164823  -2.579\nethnicityB                     0.541790   0.645876   0.839\nethnicityMe                    0.439613   0.651204   0.675\nethnicityW                     0.359890   0.650369   0.553\nlength_res                    -0.052653   0.076229  -0.691\nphysical_disorder              0.063064   0.162170   0.389\nperceived_collective_efficacy  0.366557   0.232243   1.578\nfacilities                     0.163935   0.155372   1.055\nnames                         -0.091896   0.600375  -0.153\nart                            0.821966   0.597757   1.375\nplaygrounds                   -0.077756   0.566788  -0.137\nleadership                     0.746840   0.585630   1.275\n\n\n\nCorrelation matrix not shown by default, as p = 15 &gt; 12.\nUse print(x, correlation=TRUE)  or\n    vcov(x)        if you need it\n\n# Check normality of residuals\nplot(multilevel_model)\n\n\n\n\n\n\n\n# Check homoscedasticity\nplot(residuals(multilevel_model) ~ fitted(multilevel_model))\n\n\n\n\n\n\n\n# Check linearity (if applicable)\n# Example for a continuous predictor age\nplot(data$age, residuals(multilevel_model))\n\n\n\n\n\n\n\n# Extract fixed effects estimates\nfixed_effects &lt;- fixef(multilevel_model)\n\n# Interpretation of fixed effects\nprint(fixed_effects)\n\n                  (Intercept)                           age \n                 49.019346235                  -0.009148289 \n                    education                        income \n                 -0.211335096                  -0.425075682 \n                   ethnicityB                   ethnicityMe \n                  0.541790280                   0.439613297 \n                   ethnicityW                    length_res \n                  0.359890186                  -0.052653388 \n            physical_disorder perceived_collective_efficacy \n                  0.063063774                   0.366557166 \n                   facilities                         names \n                  0.163934559                  -0.091895788 \n                          art                   playgrounds \n                  0.821965502                  -0.077756138 \n                   leadership \n                  0.746839797 \n\n\n\n\nStep 4: Comparing Alternative Models:\n\n# Model without Low-estimate predictors:\nalt_model1 &lt;- lmer(boundary ~ education + income + ethnicity + perceived_collective_efficacy + facilities + art + leadership + (1 | neighborhood_id), data = data)\n\n# Individual-focused Model:\nalt_model2 &lt;- lmer(boundary ~ age + education + income + perceived_collective_efficacy + (1 | neighborhood_id), data = data)\n\n# Interaction Effects:\nalt_model3 &lt;- lmer(boundary ~ age * perceived_collective_efficacy + education + income + length_res + physical_disorder + facilities + names + art + playgrounds + leadership + (1 | neighborhood_id), data = data)\n\n# Random Slopes Model:\nalt_model4 &lt;- lmer(boundary ~ age + education + income + perceived_collective_efficacy + (age + education + income + perceived_collective_efficacy | neighborhood_id), data = data)\n\nboundary (singular) fit: see help('isSingular')\n\n# Cross-Level Interactions:\nalt_model5 &lt;- lmer(boundary ~ age + education + income + perceived_collective_efficacy + facilities + names + art + playgrounds + leadership + (age + education + income + perceived_collective_efficacy | neighborhood_id), data = data)\n\nboundary (singular) fit: see help('isSingular')\n\n# Non-linear Effects:\ndata$age_squared &lt;- data$age^2\nalt_model6 &lt;- lmer(boundary ~ age + age_squared + education + income + perceived_collective_efficacy + (1 | neighborhood_id), data = data)\n\nWarning: Some predictor variables are on very different scales: consider\nrescaling\n\n# Nested Models:\nalt_model7 &lt;- lmer(boundary ~ age + education + income + perceived_collective_efficacy + (1 | neighborhood_id) + (1 + age | neighborhood_id), data = data)\n\nboundary (singular) fit: see help('isSingular')\n\n# Compare models using AIC\nAIC_values &lt;- AIC(multilevel_model, alt_model1, alt_model2, alt_model3, alt_model4, alt_model5, alt_model6, alt_model7)\nBIC_values &lt;- BIC(multilevel_model, alt_model1, alt_model2, alt_model3, alt_model4, alt_model5, alt_model6, alt_model7)\nprint(AIC_values)\n\n                 df      AIC\nmultilevel_model 17 12911.40\nalt_model1       12 12892.09\nalt_model2        7 12895.86\nalt_model3       15 12916.94\nalt_model4       21 12919.90\nalt_model5       26 12924.87\nalt_model6        8 12907.87\nalt_model7       10 12901.39\n\nprint(BIC_values)\n\n                 df      BIC\nmultilevel_model 17 13004.35\nalt_model1       12 12957.70\nalt_model2        7 12934.13\nalt_model3       15 12998.95\nalt_model4       21 13034.71\nalt_model5       26 13067.02\nalt_model6        8 12951.61\nalt_model7       10 12956.07\n\n\nAfter removing variables (age, length_res, names, playgrounds, physical_disorder) , my first adjusted model has one of the lower AIC/BIC, while also aligning with my theoretical framework and hypotheses.\n\n\nStep 5: Post-estimation Analyses:\n\ninfluencePlot(alt_model1, id.n = 5)\n\nWarning in plot.window(...): \"id.n\" is not a graphical parameter\n\n\nWarning in plot.xy(xy, type, ...): \"id.n\" is not a graphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"id.n\" is not a\ngraphical parameter\nWarning in axis(side = side, at = at, labels = labels, ...): \"id.n\" is not a\ngraphical parameter\n\n\nWarning in box(...): \"id.n\" is not a graphical parameter\n\n\nWarning in title(...): \"id.n\" is not a graphical parameter\n\n\nWarning in plot.xy(xy.coords(x, y), type = type, ...): \"id.n\" is not a\ngraphical parameter\n\n\n\n\n\n\n\n\n\n        StudRes         Hat        CookD\n325   3.2599741 0.008857092 0.0094969282\n832  -2.7199424 0.010389742 0.0077671191\n961   3.0225287 0.006943756 0.0063879495\n1411 -0.8990109 0.016199316 0.0013308205\n1606 -0.6344288 0.014523900 0.0005932024\n\n# Calculate variance inflation factors (VIF)\nvif(alt_model1)\n\n                                  GVIF Df GVIF^(1/(2*Df))\neducation                     1.003845  1        1.001921\nincome                        1.002528  1        1.001263\nethnicity                     1.007411  3        1.001231\nperceived_collective_efficacy 1.004720  1        1.002357\nfacilities                    1.075506  1        1.037066\nart                           1.105281  1        1.051324\nleadership                    1.045332  1        1.022415\n\n\n\n\nStep 6: Reporting:\n\n# Extract coefficients\nmodel_coefs &lt;- coef(summary(alt_model1))\nmodel_coefs\n\n                                Estimate Std. Error    t value\n(Intercept)                   48.5442001  1.2528175 38.7480213\neducation                     -0.1962912  0.4685969 -0.4188913\nincome                        -0.4273233  0.1645159 -2.5974585\nethnicityB                     0.5186504  0.6449240  0.8042039\nethnicityMe                    0.4096027  0.6499668  0.6301902\nethnicityW                     0.3533965  0.6496055  0.5440171\nperceived_collective_efficacy  0.3572640  0.2314167  1.5438123\nfacilities                     0.1658361  0.1436879  1.1541409\nart                            0.8218169  0.5516168  1.4898330\nleadership                     0.7168449  0.5260312  1.3627422\n\n\n\n\nReferences\nBektaş, Y., & Taşan-Kok, T. (2020). Love thy neighbor? Remnants of the social-mix policy in the Kolenkit neighborhood, Amsterdam. Journal of Housing and the Built Environment, 35, 743-761.\nCampbell, E., Henly, J. R., Elliott, D. S., & Irwin, K. (2009). Subjective constructions of neighborhood boundaries: lessons from a qualitative study of four neighborhoods. Journal of Urban Affairs, 31(4), 461-490.\nChan, J., To, H. P., & Chan, E. (2006). Reconsidering social cohesion: Developing a definition and analytical framework for empirical research. Social Indicators Research, 75, 273-302.\nCoulton, C. J., Jennings, M. Z., & Chan, T. (2013). How big is my neighborhood? Individual and contextual effects on perceptions of neighborhood scale. American Journal of Community Psychology, 51(1-2), 140-150.\nEasterbrook, M. J., & Vignoles, V. L. (2015). When friendship formation goes down the toilet: Design features of shared accommodation influence interpersonal bonds and well‐being. British Journal of Social Psychology, 54(1), 125-139.\nJordan, B. (1996). A theory of poverty & social exclusion.\nKleinhans, R. (2004). Social implications of housing diversification in urban renewal: A review of recent literature. Journal of Housing and the Built Environment, 19, 367-390.\nKloos, B., Hill, J., Thomas, E., Wandersman, A., Elias, M. J., & Dalton, J. H. (2012). Community psychology. Belmont, CA: Cengage Learning.\nLowe, S. S. (2000). Creating community: Art for community development. Journal of Contemporary Ethnography, 29(3), 357-386.\nLynch, S. M., & Bartlett, B. (2019). Bayesian statistics in sociology: Past, present, and future. Annual Review of Sociology, 45, 47-68.\nPeng, J., Strijker, D., & Wu, Q. (2020). Place identity: How far have we come in exploring its meanings?. Frontiers in Psychology, 11, 503569.\nPretty, G. H., Chipuer, H. M., & Bramston, P. (2003). Sense of place amongst adolescents and adults in two rural Australian towns: The discriminating features of place attachment, sense of community and place dependence in relation to place identity. Journal of environmental psychology, 23(3), 273-287.\nPutnam, R. D. (2000). Bowling alone: The collapse and revival of American community. Simon and Schuster.\nPutnam, R. D. (2007). E pluribus unum: Diversity and community in the twenty‐first century the 2006 Johan Skytte Prize Lecture. Scandinavian Political Studies, 30(2), 137-174.\nStevenson, C., Easterbrook, M., Harkin, L., McNamara, N., Kellezi, B., & Shuttleworth, I. (2019). Neighborhood identity helps residents cope with residential diversification: Contact in increasingly mixed neighborhoods of Northern Ireland. Political Psychology, 40(2), 277-295.\nStolle, D., & Harell, A. (2013). Social capital and ethno-racial diversity: Learning to trust in an immigrant society. Political Studies, 61(1), 42-66.\nTaylor, R. B., Gottfredson, S. D., & Brower, S. (1984). Neighborhood naming as an index of attachment to place. Population and Environment, 7, 103-125.\nTurner, R. N., Hewstone, M., Voci, A., & Vonofakou, C. (2008). A test of the extended intergroup contact hypothesis: The mediating role of intergroup anxiety, perceived ingroup and outgroup norms, and inclusion of the outgroup in the self. Journal of Personality and Social Psychology, 95(4), 843.\nVan Kempen, R., & Bolt, G. (2009). Social cohesion, social mix, and urban policies in the Netherlands. Journal of Housing and the Built Environment, 24, 457-475.\nVölker, B., & Flap, H. (2007). Sixteen million neighbors: A multilevel study of the role of neighbors in the personal networks of the Dutch. Urban Affairs Review, 43(2), 256-284."
  },
  {
    "objectID": "hyperloop.html",
    "href": "hyperloop.html",
    "title": "Measuring Perceptions of the Hyperloop",
    "section": "",
    "text": "Project Description\nMy primary research objective at Hardt Hyperloop was to investigate user perceptions of a simulated hyperloop trip. The company aimed to determine the appropriate dimensions of the hyperloop capsule before investing extensively in materials. However, since the company had not yet developed a hyperloop accessible to the public, predicting how people would feel inside the capsule was challenging. To address this, our team designed a real-life hyperloop experience—a wooden mock-up capsule equipped with seating and accompanied by a VR headset to illustrate the remaining characteristics of the hyperloop interior.\nWe recruited residents in Groningen to participate in the VR simulation and complete questionnaires. Four main constructs—technology acceptance, perceived safety, perceived comfort, and claustrophobia—were identified for latent class analysis. This model will be instrumental in classifying users in future simulated hyperloop studies.\n\nStep 1: Install and load necessary packages\ninstall.packages(\"poLCA\")\ninstall.packages(\"ggplot2\")\nlibrary(poLCA)\nlibrary(ggplot2)\n\n\nStep 2: Prepare your data\nThe dataframe named survey_data contains columns: tech_acceptance, perceived_safety, perceived_comfort, and claustrophobia. Unfortunately, due to privacy laws, I cannot share the data on this platform. I can only provide my code for analyzing the survey data collected at Hardt Hyperloop.\n\n\nStep 3: Specify the LCA model\nsurvey_data &lt;- cbind(tech_acceptance, perceived_safety, perceived_comfort, claustrophobia) ~ 1\n\n\nStep 4: Run the LCA for different numbers of classes and compare models\n# Fit the models with different numbers of classes\nset.seed(123) # For reproducibility\nlca_models &lt;- list()\naic_values &lt;- numeric()\nbic_values &lt;- numeric()\n\nfor (nclass in 1:4) { \n  lca_model &lt;- poLCA(formula, data = survey_data, nclass = nclass, maxiter = 1000, na.rm = TRUE)\n  lca_models[[nclass]] &lt;- lca_model\n  aic_values[nclass] &lt;- lca_model$aic\n  bic_values[nclass] &lt;- lca_model$bic\n}\n\n# Compare models using AIC and BIC\ncomparison &lt;- data.frame(\n  Classes = 1:5,\n  AIC = aic_values,\n  BIC = bic_values\n)\n\nprint(comparison)\n\n\nStep 5: Plot AIC and BIC for model comparison\nggplot(comparison, aes(x = Classes)) +\n  geom_line(aes(y = AIC, color = \"AIC\")) +\n  geom_line(aes(y = BIC, color = \"BIC\")) +\n  geom_point(aes(y = AIC, color = \"AIC\")) +\n  geom_point(aes(y = BIC, color = \"BIC\")) +\n  labs(title = \"Model Comparison\", y = \"Criterion Value\", x = \"Number of Classes\") +\n  scale_color_manual(name = \"Criteria\", values = c(\"AIC\" = \"blue\", \"BIC\" = \"red\")) +\n  theme_minimal()\n\n\nStep 6: Select the best model based on AIC/BIC and interpret parameters\n# Select the best model (assuming the lowest BIC is preferred)\nbest_nclass &lt;- which.min(bic_values)\nbest_model &lt;- lca_models[[best_nclass]]\n\n# Summary of the best model\nsummary(best_model)\n\n# Print class probabilities\nprint(best_model$P)\n\n# Print item-response probabilities\nprint(best_model$probs)\n\n\nStep 7: Graph the item-response probabilities for the best model\n# Create a data frame for plotting\nitem_probs &lt;- data.frame(\n  Class = rep(1:best_nclass, each = length(best_model$probs[[1]])),\n  Variable = rep(names(best_model$probs), each = best_nclass),\n  Level = rep(1:length(best_model$probs[[1]]), best_nclass),\n  Probability = unlist(lapply(best_model$probs, function(x) unlist(x)))\n)\n\n# Plot item-response probabilities\nggplot(item_probs, aes(x = Level, y = Probability, fill = factor(Class))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_wrap(~ Variable, scales = \"free_y\") +\n  labs(title = \"Item-Response Probabilities by Class\", x = \"Response Level\", y = \"Probability\") +\n  scale_fill_brewer(palette = \"Set1\", name = \"Class\") +\n  theme_minimal()"
  },
  {
    "objectID": "interest.html",
    "href": "interest.html",
    "title": "My Portfolio",
    "section": "",
    "text": "Community-Based Psychological Research\nModel Development\nTeaching Statistics in the Social Sciences\nSports Psychology and Coaching\n\n\n\nHumans are evolutionarely grounded for community-based lifestyles. Social intimacy has shown to improve mental and physical health. Yet, individualized living has sent us toward isolated living.\nIncreasing diverity has played a role in recent trends of isolation.\nPlacing more funding into personalizing neighborhoods that would build a new group identity for which people can embody.\n\n\n\n\nInspiring teachers are like summer camp counselors. Realistically, they might seem worlds apart, but they share a crucial trait: they are both excellent storytellers. Inspiring teachers unravel complex topics through stories, enabling their students to form emotional connections with the lessons. They focus on more than just imparting knowledge; they empower students to apply imaginative thinking in the classroom. Inspiring teachers build a nurturing space where students feel encouraged to explore and question.\nThe storytelling approach has been a cornerstone of my teaching philosophy, helping me inspire many of my peers in college. After developing this skill as a summer camp counselor for seven years, I continued to weave stories into my lessons as a statistics tutor. For instance, I would take a subject like hypothesis testing and create vivid scenarios, expressing the researchers’ thoughts and dilemmas with their datasets.\nThis approach might seem unconventional, but it has proven highly effective. By using characters and real-life examples, I helped my colleagues transform the often dry principles of statistics into vivid, engaging narratives. It added color to the traditional, monotonous format of lecturing about functions and distributions. This method particularly resonated with literature-driven students in the social sciences, allowing them to forge deeper connections with statistical concepts.\nMy experiences have led me to believe that storytelling is not just a tool but a bridge that connects abstract concepts with tangible understanding. I am eager to bring stories that elucidate key statistical concepts and to inspire every student I meet with an enthusiastic drive. By doing so, I hope to create a lasting impact on their academic and professional journeys."
  },
  {
    "objectID": "interest.html#my-academic-and-personal-interests",
    "href": "interest.html#my-academic-and-personal-interests",
    "title": "My Portfolio",
    "section": "",
    "text": "Community-Based Psychological Research\nModel Development\nTeaching Statistics in the Social Sciences\nSports Psychology and Coaching\n\n\n\nHumans are evolutionarely grounded for community-based lifestyles. Social intimacy has shown to improve mental and physical health. Yet, individualized living has sent us toward isolated living.\nIncreasing diverity has played a role in recent trends of isolation.\nPlacing more funding into personalizing neighborhoods that would build a new group identity for which people can embody.\n\n\n\n\nInspiring teachers are like summer camp counselors. Realistically, they might seem worlds apart, but they share a crucial trait: they are both excellent storytellers. Inspiring teachers unravel complex topics through stories, enabling their students to form emotional connections with the lessons. They focus on more than just imparting knowledge; they empower students to apply imaginative thinking in the classroom. Inspiring teachers build a nurturing space where students feel encouraged to explore and question.\nThe storytelling approach has been a cornerstone of my teaching philosophy, helping me inspire many of my peers in college. After developing this skill as a summer camp counselor for seven years, I continued to weave stories into my lessons as a statistics tutor. For instance, I would take a subject like hypothesis testing and create vivid scenarios, expressing the researchers’ thoughts and dilemmas with their datasets.\nThis approach might seem unconventional, but it has proven highly effective. By using characters and real-life examples, I helped my colleagues transform the often dry principles of statistics into vivid, engaging narratives. It added color to the traditional, monotonous format of lecturing about functions and distributions. This method particularly resonated with literature-driven students in the social sciences, allowing them to forge deeper connections with statistical concepts.\nMy experiences have led me to believe that storytelling is not just a tool but a bridge that connects abstract concepts with tangible understanding. I am eager to bring stories that elucidate key statistical concepts and to inspire every student I meet with an enthusiastic drive. By doing so, I hope to create a lasting impact on their academic and professional journeys."
  },
  {
    "objectID": "bayes.html",
    "href": "bayes.html",
    "title": "Project Description",
    "section": "",
    "text": "Project Description\nBayesian analysis is a statistical approach that interprets probability as a measure of belief or confidence in an event occurring. It allows us to update our beliefs about parameters of interest based on evidence (data) as it becomes available. This is a field of Statistics that I am constantly trying to learn more about.\nConsidering my love for exercise and health, I found a dataset on physical activity and heart rate to use for a Bayesian analysis. I am interested in exploring the relationship between exercise and heart rate, which calls for a linear regression approach. To perform a Bayesian linear regression, I will use the brms package in R. This package provides an interface to fit complex Bayesian models using the probabilistic programming language Stan. Stan’s efficient Hamiltonian Monte Carlo (HMC) algorithms can sample from the posterior distribution of the model parameters.\nI could not have learned some of these steps by myself. An amazing resource to check out is bayesrulesbook.com; it will teach you so much about the Bayesian philosophy and techniques. If you curious about where I learned some of the steps below, check out Chapter 6!\n\nStep 1: Install and load necessary packages\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\nlibrary(brms)\n\nLoading required package: Rcpp\n\n\nLoading 'brms' package (version 2.21.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\n\nAttaching package: 'brms'\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\nlibrary(bayesplot)\n\nThis is bayesplot version 1.11.1\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\n\n\nAttaching package: 'bayesplot'\n\n\nThe following object is masked from 'package:brms':\n\n    rhat\n\n\n\ndata&lt;-read.csv(\"exercise_data.csv\")\nhead(data)\n\n  physical_activity heart_rate age   weight gender\n1                55   66.32434  22 66.45458   Male\n2                69   69.98812  28 69.26444 Female\n3                46   82.85591  64 58.31349   Male\n4                61   53.97753  37 63.65252 Female\n5                73   74.57795  19 69.71158   Male\n6                63   70.95191  79 76.70696 Female\n\n\n\n\nStep 2: Create the Bayesian Linear Regression\n\n# We can model heart rate as a function of physical activity, age, weight, and gender\nmodel &lt;- brm(\n  heart_rate ~ physical_activity + age + weight + gender,\n  data = data,\n  family = gaussian(), # Assuming heart rate follows a Gaussian normal distribution\n  prior = c(set_prior(\"normal(0, 10)\", class = \"b\")) # set a normal prior of M = 0 and SD = 10\n)\n\nCompiling Stan program...\n\n\nTrying to compile a simple C file\n\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 15.0.0 (clang-1500.1.0.2.5)’\nusing SDK: ‘MacOSX14.2.sdk’\nclang -arch x86_64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/x86_64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from &lt;built-in&gt;:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n#include &lt;cmath&gt;\n         ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3.4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.34 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.042 seconds (Warm-up)\nChain 1:                0.024 seconds (Sampling)\nChain 1:                0.066 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.6e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.041 seconds (Warm-up)\nChain 2:                0.025 seconds (Sampling)\nChain 2:                0.066 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.6e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.04 seconds (Warm-up)\nChain 3:                0.023 seconds (Sampling)\nChain 3:                0.063 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.039 seconds (Warm-up)\nChain 4:                0.025 seconds (Sampling)\nChain 4:                0.064 seconds (Total)\nChain 4: \n\n# Print a summary of the fitted model, we can assess our estimates and our credible intervals\nsummary(model)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: heart_rate ~ physical_activity + age + weight + gender \n   Data: data (Number of observations: 145) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            75.59      9.84    56.35    95.14 1.00     4448     3175\nphysical_activity    -0.06      0.12    -0.29     0.17 1.00     5154     2993\nage                   0.05      0.05    -0.05     0.15 1.00     4980     3133\nweight                0.01      0.10    -0.19     0.20 1.00     4800     3049\ngenderMale           -0.86      1.77    -4.32     2.70 1.00     5899     2983\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    10.70      0.64     9.52    12.06 1.00     5083     3079\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nImportant Note: Credible intervals crossing through 0 in Bayesian analysis indicate uncertainty or lack of evidence for a nonzero effect. They prompt careful consideration of the implications for your research question and the need for further investigation or interpretation in light of these findings.\n\n\nStep 3: Data Visualization\n\nTrace Plots: Displays the sampled values of the parameters over iterations of the Markov Chain Monte Carlo (MCMC) sampling.\n\nInterpretation: Look for a “fuzzy caterpillar” appearance where the chains mix well and cover the parameter space evenly. This suggests good mixing and convergence. If the chains show trends or don’t mix well, it indicates poor convergence and potential issues with the sampling process.\n\n\n\n# Trace Plots\ntrace_plot &lt;- mcmc_trace(model, size = .1)\nprint(trace_plot)\n\n\n\n\n\n\n\n\n\nDensity Plots: Shows the posterior distributions of the parameters.\n\nInterpretation: These should be smooth and unimodal. Multiple peaks might indicate issues with convergence or that the model has not properly identified the posterior distribution.\n\n\n\n# Density Plots (Posterior distributions)\nmcmc_dens(model) + \n  yaxis_text(TRUE)\n\n\n\n\n\n\n\n\n\nDensity Overlay Plot: Compares the density of the observed data to the density of the data generated by the model.\n\nInterpretation: The observed data (usually shown in a solid line) should ideally lie within the range of the simulated data (shaded area or dashed lines). If the observed data fall outside this range, it may indicate that the model does not fit the data well.\n\n\n\n# Posterior Predictive Checks (Density Overlay)\npp_check_plot &lt;- pp_check(model, ndraws = 100)\nprint(pp_check_plot)"
  },
  {
    "objectID": "projects.html#project-2",
    "href": "projects.html#project-2",
    "title": "Projects",
    "section": "Project 2",
    "text": "Project 2"
  },
  {
    "objectID": "projects.html#project-3",
    "href": "projects.html#project-3",
    "title": "Projects",
    "section": "Project 3",
    "text": "Project 3"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "My Resume",
    "section": "Education",
    "text": "Education\n\nMSc Environmental Psychology, University of Groningen\n09/2021 - 02/2023\n\nUtilized theories from social, cognitive, and developmental psychology to assess the interplay between humans and the built and natural world\nDesigned and tested interventions in real-world applications\nLearned advanced multilevel statistical techniques to analyze and understand complex social and behavioral systems\n\n\n\nBS Psychology and Statistics Minor, Virginia Polytech Institute and State University\n09/2017 - 06/2021"
  },
  {
    "objectID": "about.html#professional-experience",
    "href": "about.html#professional-experience",
    "title": "My Resume",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nUX Researcher at Hardt Hyperloop, Groningen, NL\n04/2022 - 11/2022\nResearch lab focused on studying the user perceptions of a simulated hyperloop train, aimed at promoting sustainable transportation usage.\n\nPsychometrics: Designed questionnaires, interviewed subjects, and created latent-class models for experimental measures\nRegression: Designed multivariate multiple regression models\nPresentations: Presented item structure and regression graphics to company members, and was interviewed by HiveMobility\n\n\n\nStudent Researcher at the Center for Applied Behavioral Systems, Blacksburg, VA\n01/2018 - 05/2021\nA center for intervention and community-based research techniques.\n\nTeaching: Training undergraduate students in responsible research methods\nData Management: Reviewing and preparing data for publication using the Virginia Polytech Data Repository\nPsychometrics: Designed and tested surveys using Qualtrics and R\nCollaborative Projects: Conducted research projects with two of the doctoral students in the lab, including writing thorough literature reviews and co-authoring two papers\n\n\n\n\nPrivate Tutor in Statistics, Self Employed, Groningen, NL\n09/2021 - Present\nSupported 16 bachelor students and 8 master’s students in exam preparation and refining research methodology.\n\nSubject Knowledge: Instructed on topics in bachelor and master level statistics\nTeaching: Simplified coursework material into understandable lessons"
  },
  {
    "objectID": "about.html#skills-and-expertise",
    "href": "about.html#skills-and-expertise",
    "title": "My Resume",
    "section": "Skills and Expertise",
    "text": "Skills and Expertise\n\nSoftware\nR, Atlas, SPSS, Microsoft Office\n\n\nStatistics\nSet Theory and Logic, Methods of Regression, Item Response Theory, Latent Class Analysis, Hierarchical Modelling, Econometrics, Bayesian Statistics, Time Series Analysis, Contingency Table Analysis\n\n\nPsychology\nCognitive, Social, Developmental, Clinical, Behavioral, Humanistic, Psychometrics\n\n\nEnvironmental Science\nGeoscience, Geology, Meteorology, Climate Change, Ecosystems, Sustainable Systems,"
  },
  {
    "objectID": "about.html#certifications",
    "href": "about.html#certifications",
    "title": "My Resume",
    "section": "Certifications",
    "text": "Certifications\n\nR Programming (Coursera Specialization)\nJohns Hopkins Bloomberg School of Public Health\n04/2023 - 03/2024\n\nBuilding Data Visualization Tools\nBuilding R Packages\nAdvanced R Programming\nThe R Programming Environment\n\n\n\nMathematical Biostatistics (Coursera Specialization)\nJohns Hopkins Bloomberg School of Public Health\n09/2023 - 11/2023\n\nProbability Mass and Density Functions\nBayesian Statistics\nBootstrapping Algorithms\nGeometric Means"
  },
  {
    "objectID": "about.html#professional-presentations-awards-and-conferences",
    "href": "about.html#professional-presentations-awards-and-conferences",
    "title": "My Resume",
    "section": "Professional Presentations, Awards, and Conferences",
    "text": "Professional Presentations, Awards, and Conferences\nProfessional Presentations\n\nCallahan, O., Eick, A., Takemoto-Jennings, R., & Geller, E. S. (2019, February). Gratitude from student to professor: Evaluating the impact of thanking a professor. Poster presented at the Annual North Carolina Association for Behavior Analysis, Winston-Salem, NC.\nCallahan, O., Eick, A., Takemoto-Jennings, R., & Geller, E. S. (2019, March). The impact of showing gratitude in the classroom: Evaluating students’ change in their mood-states. Poster presented at the Mid-Atlantic Undergraduate Research Conference, Blacksburg, VA.\n\nAwards\n\nDean’s List\nBeyond Boundaries Scholar\nSumma Cum Laude\n\nConferences\n\nUniC Summit. 07/2021 Quebec, Canada\nCOP25 United Nations Climate Change Conference. 12/2019 Madrid, Spain\nMid-Atlantic Undergraduate Research Conference. 04/2019 Blacksburg, VA, USA\nNorth Carolina Applied Behavioral Association. 02/2019 Winston-Salem, NC, USA"
  },
  {
    "objectID": "about.html#contact-information",
    "href": "about.html#contact-information",
    "title": "My Resume",
    "section": "Contact Information",
    "text": "Contact Information\nEmail: owen11callahan@gmail.com"
  },
  {
    "objectID": "begin.html#learning-r",
    "href": "begin.html#learning-r",
    "title": "Fear Not!",
    "section": "",
    "text": "Asking for Help\nPackages\nReading Data\nTidying Data\nBuilding Functions\nData Visualization\n\n\n\n\nMost experts mention asking for help at the end of their tutorials, but I believe it deserves to be first. Learning R involves seeking a lot of assistance, and there are countless online resources available to answer your questions. Below are some of my favorite resources:\n\nThe Help Center in RStudio: Located in the bottom right pane of your window, the Help Center provides detailed explanations of each command in R. You can learn about the purpose of each function, as well as their sub-commands and properties. You can also type any command with the ? symbol in front of it and it will direct you to the same window (e.g., ?plot()).\nThe R Project Website: Provides comprehensive documentation and manuals.\nRStudio Community: An active forum where you can ask questions.\nR-Bloggers: A fantastic resource for tutorials, tips, and tricks from the R community.\nYouTube: Useful tutorials for visual learners.\nReddit: A community where everyone deals with problems using R. Visit Reddit to view a wide range of questions and answers from people around the globe.\nGenerative AI: Appropriate in certain circumstances. Use with caution, as AI can occasionally produce incorrect solutions to your code.\n\nBy leveraging these resources, you can make your learning journey with R more efficient and less daunting.\n\n\n\nR packages are collections of functions, data, and documentation that extend the capabilities of base R, making it easier and more efficient to perform a wide range of tasks.\n\nSimplify Complex Tasks\n\nPackages can simplify complex tasks into manageable functions. For instance, data visualization with ggplot2 or data manipulation with dplyr breaks down intricate processes into straightforward commands.\n\nEnhance Data Analysis\n\nPackages like tidyverse offer tools for data import, tidying, transformation, visualization, and modeling. These integrated tools follow a consistent philosophy, making data analysis more intuitive.\n\nImprove Data Visualization\n\nPackages such as ggplot2, plotly, and shiny help create advanced and interactive data visualizations. This makes it easier to explore and communicate data insights.\n\nStreamline Workflow\n\nPackages like knitr and rmarkdown allow for dynamic report generation, integrating code and its output in a single document. This streamlines the workflow from data analysis to reporting.\n\nAccess Vast Amounts of Data\n\nPackages like dplyr and tidyr help in efficiently handling and processing large datasets. This is particularly useful for big data applications where performance is crucial.\n\n\n\n\n\n\n\nYou can find your data either using the file.choose() function or by clicking on Files in the bottom right window. Make sure to save your new data in your local Environment in the top right window.\n\n\n\nThe most common format for data files is CSV (Comma-Separated Values). The readr package from the tidyverse provides functions to read CSV files efficiently.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Read CSV file\ndata &lt;- read.csv(\"exercise_data.csv\")\n\n\n\n\n\nlibrary(readxl)\n\n# Read the first sheet\ndata &lt;- read_excel(\"/Users/owencallahan/Downloads/Fitness.xlsx\")\n\n# Read a specific sheet by name\ndata_sheet &lt;- read_excel(\"/Users/owencallahan/Downloads/Fitness.xlsx\", sheet = \"Sheet1\")\n\n# Read a specific sheet by index\ndata_index &lt;- read_excel(\"/Users/owencallahan/Downloads/Fitness.xlsx\", sheet = 1)\n\n\n\n\n\ndata &lt;- c(1,2,3,4)\nwrite.csv(data, \"my_data.csv\")\n\n\n\n\n\nTidying data is an essential part of data analysis in R, making your data easier to work with and analyze. We’ll use the tidyverse suite of packages, which includes dplyr, tidyr, and readr among others. Below is a step-by-step tutorial with code examples to demonstrate how to tidy data in R.\n\n\nPivoting longer:\n\nwide_data &lt;- tibble(\n  id = 1:3,\n  age = c(25, 30, 35),\n  height = c(175, 180, 165),\n  weight = c(70, 80, 65)\n)\n\n# Convert to long format\nlong_data &lt;- wide_data %&gt;%\n  pivot_longer(cols = c(age, height, weight), names_to = \"measure\", values_to = \"value\")\n\nprint(long_data)\n\n# A tibble: 9 × 3\n     id measure value\n  &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1 age        25\n2     1 height    175\n3     1 weight     70\n4     2 age        30\n5     2 height    180\n6     2 weight     80\n7     3 age        35\n8     3 height    165\n9     3 weight     65\n\n\nPivoting wider:\n\n# Convert back to wide format\nwide_again &lt;- long_data %&gt;%\n  pivot_wider(names_from = measure, values_from = value)\n\nprint(wide_again)\n\n# A tibble: 3 × 4\n     id   age height weight\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1    25    175     70\n2     2    30    180     80\n3     3    35    165     65\n\n\nSeparate columns:\n\ndata &lt;- tibble(\n  name = c(\"John_Doe\", \"Jane_Smith\", \"Alice_Johnson\")\n)\n\n# Separate into first and last names\nseparated_data &lt;- data %&gt;%\n  separate(name, into = c(\"first_name\", \"last_name\"), sep = \"_\")\n\nprint(separated_data)\n\n# A tibble: 3 × 2\n  first_name last_name\n  &lt;chr&gt;      &lt;chr&gt;    \n1 John       Doe      \n2 Jane       Smith    \n3 Alice      Johnson  \n\n\nUnite columns:\n\nunited_data &lt;- separated_data %&gt;%\n  unite(\"full_name\", first_name, last_name, sep = \" \")\n\n\n\n\nSelecting Columns\n\nselected_data &lt;- mtcars %&gt;%\n  select(cyl, gear)\n\nhead(selected_data)\n\n                  cyl gear\nMazda RX4           6    4\nMazda RX4 Wag       6    4\nDatsun 710          4    4\nHornet 4 Drive      6    3\nHornet Sportabout   8    3\nValiant             6    3\n\n\nFiltering Rows\n\nfiltered_data &lt;- mtcars %&gt;%\n  filter(gear &lt; 4)\n\nhead(filtered_data)\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 450SE        16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL        17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n\n\nMutating Columns\n\nmutated_data &lt;- mtcars %&gt;%\n  mutate(speed.scale = qsec/wt)\n\nhead(mutated_data %&gt;%\n        select(speed.scale))\n\n                  speed.scale\nMazda RX4            6.282443\nMazda RX4 Wag        5.920000\nDatsun 710           8.021552\nHornet 4 Drive       6.046656\nHornet Sportabout    4.947674\nValiant              5.843931\n\n\nSummarizing Data\n\nsummary_data &lt;- mtcars %&gt;%\n  group_by(gear) %&gt;%\n  summarize(\n    average_wt = mean(wt),\n    count = n()\n  )\n\nprint(summary_data)\n\n# A tibble: 3 × 3\n   gear average_wt count\n  &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1     3       3.89    15\n2     4       2.62    12\n3     5       2.63     5\n\n\n\n\n\n\n# Identify missing values\nmissing_data &lt;- mtcars %&gt;%\n  filter(is.na(wt))\n\n# Remove rows with missing values\ncleaned_data &lt;- data %&gt;%\n  drop_na()\n\n# Replace missing values with a specific value\nfilled_data &lt;- data %&gt;%\n  replace_na(list(height = 170, weight = 70))\n\n\n\n\n\ndata1 &lt;- tibble(\n  id = 1:3,\n  name = c(\"John\", \"Jane\", \"Alice\")\n)\n\ndata2 &lt;- tibble(\n  id = 1:3,\n  score = c(85, 90, 78)\n)\n\n# Inner join\njoined_data &lt;- data1 %&gt;%\n  inner_join(data2, by = \"id\")\n\nprint(joined_data)\n\n# A tibble: 3 × 3\n     id name  score\n  &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 John     85\n2     2 Jane     90\n3     3 Alice    78\n\n\n\n\n\n\nWhen I learned how to create my own functions, I felt like the creative side of R expanded beyond my expectations. I could tailor a command to address exactly what I needed to perform on a given set of data.\n\n\nThe function() command is a fundamental building block in R, enabling users to create their own functions. This is crucial for both simplifying repetitive tasks and organizing code in a clean, efficient manner. Here’s what the function() command can do for new learners using R:\n\nEncapsulate Repetitive Tasks\n\nCreating functions allows you to encapsulate repetitive code into reusable blocks. This reduces redundancy and makes your scripts more concise and readable. For example, if you frequently perform the same data transformation, you can write a function for it and call it whenever needed.\n\nOrganize Code\n\nFunctions help in organizing code logically. By breaking down complex procedures into smaller, manageable functions, you make your code more modular and easier to debug and maintain.\n\nImprove Readability\n\nUsing functions can significantly enhance the readability of your code. Descriptive function names and clear parameter definitions help others (and your future self) understand the purpose and usage of the code more quickly.\n\nParameterization\n\nFunctions allow you to use parameters to make your code more flexible. Instead of hard-coding values, you can pass different arguments to your functions, making them adaptable to various inputs and scenarios.\n\nEnhance Reproducibility\n\nFunctions contribute to reproducibility in your analyses. By encapsulating specific tasks, you ensure that the same operations can be repeated with different data or settings, leading to consistent results.\n\nPromote Code Reuse\n\nOnce you write a function, you can reuse it across different projects. This saves time and effort, as you don’t need to rewrite the same code for similar tasks.\n\n\nExample of Using function() in R\n\n# Define a function to calculate the mean of a numeric vector\ncalculate_mean &lt;- function(numbers) {\n  mean_value &lt;- mean(numbers)\n  return(mean_value)\n}\n\n# Use the function with a numeric vector\nsample_data &lt;- c(4, 8, 15, 16, 23, 42)\naverage &lt;- calculate_mean(sample_data)\nprint(average)\n\n[1] 18\n\n\n\n\n\nCorrelation measures the strength and direction of the relationship between two variables.\n\n# Correlation matrix\ncor_matrix &lt;- cor(mtcars)\nprint(cor_matrix)\n\n            mpg        cyl       disp         hp        drat         wt\nmpg   1.0000000 -0.8521620 -0.8475514 -0.7761684  0.68117191 -0.8676594\ncyl  -0.8521620  1.0000000  0.9020329  0.8324475 -0.69993811  0.7824958\ndisp -0.8475514  0.9020329  1.0000000  0.7909486 -0.71021393  0.8879799\nhp   -0.7761684  0.8324475  0.7909486  1.0000000 -0.44875912  0.6587479\ndrat  0.6811719 -0.6999381 -0.7102139 -0.4487591  1.00000000 -0.7124406\nwt   -0.8676594  0.7824958  0.8879799  0.6587479 -0.71244065  1.0000000\nqsec  0.4186840 -0.5912421 -0.4336979 -0.7082234  0.09120476 -0.1747159\nvs    0.6640389 -0.8108118 -0.7104159 -0.7230967  0.44027846 -0.5549157\nam    0.5998324 -0.5226070 -0.5912270 -0.2432043  0.71271113 -0.6924953\ngear  0.4802848 -0.4926866 -0.5555692 -0.1257043  0.69961013 -0.5832870\ncarb -0.5509251  0.5269883  0.3949769  0.7498125 -0.09078980  0.4276059\n            qsec         vs          am       gear        carb\nmpg   0.41868403  0.6640389  0.59983243  0.4802848 -0.55092507\ncyl  -0.59124207 -0.8108118 -0.52260705 -0.4926866  0.52698829\ndisp -0.43369788 -0.7104159 -0.59122704 -0.5555692  0.39497686\nhp   -0.70822339 -0.7230967 -0.24320426 -0.1257043  0.74981247\ndrat  0.09120476  0.4402785  0.71271113  0.6996101 -0.09078980\nwt   -0.17471588 -0.5549157 -0.69249526 -0.5832870  0.42760594\nqsec  1.00000000  0.7445354 -0.22986086 -0.2126822 -0.65624923\nvs    0.74453544  1.0000000  0.16834512  0.2060233 -0.56960714\nam   -0.22986086  0.1683451  1.00000000  0.7940588  0.05753435\ngear -0.21268223  0.2060233  0.79405876  1.0000000  0.27407284\ncarb -0.65624923 -0.5696071  0.05753435  0.2740728  1.00000000\n\n# Correlation between two variables\ncor(mtcars$mpg, mtcars$hp)\n\n[1] -0.7761684\n\n\n\n\n\nLinear regression models the relationship between a dependent variable and one or more independent variables.\n\n# Simple linear regression\nmodel &lt;- lm(mpg ~ hp, data = mtcars)\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7121 -2.1122 -0.8854  1.5819  8.2360 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 30.09886    1.63392  18.421  &lt; 2e-16 ***\nhp          -0.06823    0.01012  -6.742 1.79e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 30 degrees of freedom\nMultiple R-squared:  0.6024,    Adjusted R-squared:  0.5892 \nF-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n\n# Multiple linear regression\nmodel2 &lt;- lm(mpg ~ hp + wt, data = mtcars)\nsummary(model2)\n\n\nCall:\nlm(formula = mpg ~ hp + wt, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\n\n\n\n\nANOVA tests the difference in means among groups.\n\n# One-way ANOVA\nanova_model &lt;- aov(mpg ~ as.factor(cyl), data = mtcars)\nsummary(anova_model)\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nas.factor(cyl)  2  824.8   412.4    39.7 4.98e-09 ***\nResiduals      29  301.3    10.4                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Two-way ANOVA\nanova_model2 &lt;- aov(mpg ~ as.factor(cyl) + as.factor(gear), data = mtcars)\nsummary(anova_model2)\n\n                Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nas.factor(cyl)   2  824.8   412.4   38.00 1.41e-08 ***\nas.factor(gear)  2    8.3     4.1    0.38    0.687    \nResiduals       27  293.0    10.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nT-tests compare the means of two groups.\n\n# One-sample t-test\nt.test(mtcars$mpg, mu = 20)\n\n\n    One Sample t-test\n\ndata:  mtcars$mpg\nt = 0.08506, df = 31, p-value = 0.9328\nalternative hypothesis: true mean is not equal to 20\n95 percent confidence interval:\n 17.91768 22.26357\nsample estimates:\nmean of x \n 20.09062 \n\n# Two-sample t-test\nt.test(mpg ~ as.factor(am), data = mtcars)\n\n\n    Welch Two Sample t-test\n\ndata:  mpg by as.factor(am)\nt = -3.7671, df = 18.332, p-value = 0.001374\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -11.280194  -3.209684\nsample estimates:\nmean in group 0 mean in group 1 \n       17.14737        24.39231 \n\n\n\n\n\nChi-square tests are used for categorical data to test relationships between variables.\n\n# Create a contingency table\ntable_data &lt;- table(mtcars$cyl, mtcars$gear)\n\n# Chi-square test\nchisq.test(table_data)\n\nWarning in chisq.test(table_data): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  table_data\nX-squared = 18.036, df = 4, p-value = 0.001214\n\n\n\n\n\nPCA reduces the dimensionality of the data while preserving as much variance as possible. An excellent tutorial on PCA can be found by clicking this link: https://www.youtube.com/watch?\n\n# Perform PCA\npca_result &lt;- prcomp(mtcars, scale. = TRUE)\n\n# Summary of PCA\nsummary(pca_result)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.5707 1.6280 0.79196 0.51923 0.47271 0.46000 0.3678\nProportion of Variance 0.6008 0.2409 0.05702 0.02451 0.02031 0.01924 0.0123\nCumulative Proportion  0.6008 0.8417 0.89873 0.92324 0.94356 0.96279 0.9751\n                           PC8    PC9    PC10   PC11\nStandard deviation     0.35057 0.2776 0.22811 0.1485\nProportion of Variance 0.01117 0.0070 0.00473 0.0020\nCumulative Proportion  0.98626 0.9933 0.99800 1.0000\n\n\n\n\n\nK-means clustering groups data into k clusters.\n\n# Perform K-means clustering on the mtcars dataset\nset.seed(123)\nkmeans_result &lt;- kmeans(mtcars[, c(\"mpg\", \"hp\")], centers = 3)\n\n# Add cluster results to the original mtcars data\nmtcars$cluster &lt;- as.factor(kmeans_result$cluster)\n\n# Print the first few rows to check the results\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb cluster\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4       3\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4       3\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1       3\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1       3\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2       2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1       3\n\n\n\n\n\nLogistic regression models the probability of a binary outcome.\n\n# Logistic regression\nlogit_model &lt;- glm(am ~ hp + wt, data = mtcars, family = binomial)\nsummary(logit_model)\n\n\nCall:\nglm(formula = am ~ hp + wt, family = binomial, data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) 18.86630    7.44356   2.535  0.01126 * \nhp           0.03626    0.01773   2.044  0.04091 * \nwt          -8.08348    3.06868  -2.634  0.00843 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 10.059  on 29  degrees of freedom\nAIC: 16.059\n\nNumber of Fisher Scoring iterations: 8\n\n\n\n\n\n\n# Decomposition\n\n# Sample time series data\nts_data &lt;- ts(AirPassengers, frequency = 12)\n\n# Decompose the time series\ndecomposed &lt;- decompose(ts_data)\nplot(decomposed)\n\n\n\n\n\n\n\n# Forecasting\n\n# Install and load the forecast package\nlibrary(forecast)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n# Fit an ARIMA model\nfit &lt;- auto.arima(ts_data)\nforecasted &lt;- forecast(fit, h = 12)\nplot(forecasted)\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s explore one of my favorite, and also one of the most essential packages in R, ggplot2.\n\n\nFirst, you need to install and load the ggplot2 package:\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\ninstall.packages(\"ggplot2\")\n\n\nThe downloaded binary packages are in\n    /var/folders/k3/v39j6g_x4bv7mv_xq03986d00000gn/T//Rtmpgb6tV5/downloaded_packages\n\nlibrary(ggplot2)\n\n\n\n\nggplot2 follows the grammar of graphics, which means you build plots layer by layer. The essential components are:\n\nData: The dataset you’re plotting.\nAesthetics (aes): The mapping of variables to visual properties like x and y coordinates, colors, sizes, etc.\nGeometries (geom): The type of plot you want to create (e.g., points, lines, bars).\nFacets: Subplots based on the values of one or more variables.\nScales: Control how data values are mapped to visual properties.\nCoordinate Systems: Control the coordinate space.\nThemes: Control the appearance of the plot.\n\n\n\n\nScatter Plot\n\n# Load example data\ndata(mtcars)\n\n# Create a scatter plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\n\n\n\n\nLine Plot\n\n# Create a line plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_line() +\n  labs(title = \"Line Plot of MPG vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\n\n\n\n\nBar Plot\n\n# Create a bar plot\nggplot(data = mtcars, aes(x = factor(cyl))) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Cylinder Counts\",\n       x = \"Number of Cylinders\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\nHistogram\n\n# Create a histogram\nggplot(data = mtcars, aes(x = mpg)) +\n  geom_histogram(binwidth = 2) +\n  labs(title = \"Histogram of MPG\",\n       x = \"Miles Per Gallon (MPG)\",\n       y = \"Frequency\")\n\n\n\n\n\n\n\n\nBox Plot\n\n# Create a box plot\nggplot(data = mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot() +\n  labs(title = \"Box Plot of MPG by Cylinder Count\",\n       x = \"Number of Cylinders\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\n\n\n\n\n\n\n\nAdding Colors\n\n# Scatter plot with color\nggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight by Cylinder Count\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\",\n       color = \"Cylinders\")\n\n\n\n\n\n\n\n\nAdding Size\n\n# Scatter plot with color and size\nggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl), size = hp)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight by Cylinder Count\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\",\n       color = \"Cylinders\",\n       size = \"Horsepower\")\n\n\n\n\n\n\n\n\nFaceting\n\n# Faceted scatter plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  facet_wrap(~ cyl) +\n  labs(title = \"Scatter Plot of MPG vs Weight by Cylinder Count\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\n\n\n\n\nThemes\n\n# Scatter plot with theme\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\") +\n  theme_minimal()"
  }
]