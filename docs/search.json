[
  {
    "objectID": "timeseries.html",
    "href": "timeseries.html",
    "title": "My Portfolio",
    "section": "",
    "text": "1. Install and Load Necessary Packages\ninstall.packages(\"fpp2\")\ninstall.packages(\"TTR\")\nlibrary(fpp2)\nlibrary(TTR)\ndata(\"elecdemand\")\n\n\n2. Plot the Time Series Data\nautoplot(elecdemand[, \"Demand\"], main = \"Electricity Demand in Victoria, Australia\",\n         xlab = \"Time\", ylab = \"Demand (MW)\", col = \"blue\")\n\n\n3. Experiment with Different Window Sizes\n# Calculate moving averages with different window sizes\nmoving_avg_24 &lt;- SMA(elecdemand[, \"Demand\"], n = 24)  # 12 hours\nmoving_avg_96 &lt;- SMA(elecdemand[, \"Demand\"], n = 96)  # 48 hours\nmoving_avg_192 &lt;- SMA(elecdemand[, \"Demand\"], n = 192)  # 96 hours\n\n# Plot original data and moving averages with different window sizes\nplot(elecdemand[, \"Demand\"], type = \"l\", col = \"blue\", main = \"Electricity Demand with Moving Averages\",\n     xlab = \"Time\", ylab = \"Demand (MW)\")\nlines(moving_avg_24, col = \"red\", lty = 2)\nlines(moving_avg_96, col = \"green\", lty = 3)\nlines(moving_avg_192, col = \"orange\", lty = 4)\nlegend(\"topright\", legend = c(\"Original Data\", \"24-period MA\", \"96-period MA\", \"192-period MA\"),\n       col = c(\"blue\", \"red\", \"green\", \"orange\"), lty = c(1, 2, 3, 4), lwd = 2)\n\n\n4. Create and Plot the Moving Average Model\n# Calculate the moving average with a window size of 48 (24 hours, since data is half-hourly)\nmoving_avg &lt;- SMA(elecdemand[, \"Demand\"], n = 48)\n\n# Plot the original time series and the moving average\nplot(elecdemand[, \"Demand\"], type = \"l\", col = \"blue\", main = \"Electricity Demand with Moving Average\",\n     xlab = \"Time\", ylab = \"Demand (MW)\")\nlines(moving_avg, col = \"red\")\nlegend(\"topright\", legend = c(\"Original Data\", \"48-period MA\"), col = c(\"blue\", \"red\"), lty = 1, lwd = 2)\n\n\n5. Evaluate Model Performance\n# Calculate Mean Absolute Error (MAE)\nmae &lt;- mean(abs(elecdemand[, \"Demand\"] - moving_avg), na.rm = TRUE)\n\n# Calculate Root Mean Squared Error (RMSE)\nrmse &lt;- sqrt(mean((elecdemand[, \"Demand\"] - moving_avg)^2, na.rm = TRUE))\n\nprint(paste(\"Mean Absolute Error (MAE):\", round(mae, 2)))\nprint(paste(\"Root Mean Squared Error (RMSE):\", round(rmse, 2)))"
  },
  {
    "objectID": "about.html#professional-experience",
    "href": "about.html#professional-experience",
    "title": "About Me",
    "section": "Professional Experience:",
    "text": "Professional Experience:\n\nJunior Researcher at Hardt Hyperloop, Groningen, NL\nResearch lab focused on studying the user perceptions of a simulated hyperloop train, aimed at promoting sustainable transportation usage.\n\nPsychometrics: Designed questionnaires, interviewed subjects, and created latent-class models for experimental measures\nRegression: Designed multivariate multiple regression models\nPresentations: Presented item structure and regression graphics to company members, and was interviewed by HiveMobility\n\n\n\nStudent Researcher at the Center for Applied Behavioral Systems, Blacksburg, VA\nA center for intervention and community-based research techniques.\n\nTeaching: Training undergraduate students in responsible research methods\nData Management: Reviewing and preparing data for publication using the Virginia Polytech Data Repository\nPsychometrics: Designed and tested surveys using Qualtrics and R\nCollaborative Projects: Conducted research projects with two of the doctoral students in the lab, including writing thorough literature reviews and co-authoring two papers\n\n\n\nPrivate Tutor in Statistics, Self Employed, Groningen, NL\nSupported 16 bachelor students and 8 master’s students in exam preparation and refining research methodology.\n\nSubject Knowledge: Instructed on topics in bachelor and master level statistics\nTeaching: Simplified coursework material into understandable lessons\n\n\n\nSocial and Climate Justice Researcher at Virginia Climate Action, Blacksburg, VA\nAn interdisciplinary team of researchers designed a climate action proposal for Virginia Polytech University, containing 15 transition pathways.\n\nData Analysis: filtered energy datasets using R and performed content analysis using Atlas\nTeaching: Created educational videos detailing the transition pathways for university students\nIndependent Projects: Computed lifecycle analysis that included the social justice costs and benefits of procured systems, as well as photovoltaic system decommissioning\nCollaborative Projects: Built new green spaces on campus for students and installed a new waste distribution system at campus dining halls"
  },
  {
    "objectID": "about.html#skills-and-expertise",
    "href": "about.html#skills-and-expertise",
    "title": "About Me",
    "section": "Skills and Expertise:",
    "text": "Skills and Expertise:\n\nSoftware\nR, Python, Atlas, SPSS, Microsoft Excel\n\n\nStatistics\nSet Theory and Logic, Regression, Multilevel Modelling, Psychometrics, Econometrics, Bayesian Statistics, Time Series\n\n\nPsychology\nCognitive, Social, Developmental, Clinical, Behavioral, Humanistic, Psychometrics\n\n\nEnvironmental Science\nGeoscience, Geology, Meteorology, Climate Change, Ecosystems, Sustainable Systems,"
  },
  {
    "objectID": "about.html#certifications",
    "href": "about.html#certifications",
    "title": "About Me",
    "section": "Certifications",
    "text": "Certifications\n\nR Programming (Coursera Specialization)\nJohns Hopkins Bloomberg School of Public Health\n04/2023 - 03/2024\nBuilding Data Visualization Tools\nBuilding R Packages\nAdvanced R Programming\nThe R Programming Environment\n\n\nMathematical Biostatistics (Coursera Specialization)\nJohns Hopkins Bloomberg School of Public Health\n09/2023 - 11/2023\nProbability Mass and Density Functions\nBayesian Statistics\nBootstrapping Algorithms\nGeometric Means"
  },
  {
    "objectID": "about.html#professional-presentations-awards-and-conferences",
    "href": "about.html#professional-presentations-awards-and-conferences",
    "title": "About Me",
    "section": "Professional Presentations, Awards, and Conferences:",
    "text": "Professional Presentations, Awards, and Conferences:\nProfessional Presentations\n\nCallahan, O., Eick, A., Takemoto-Jennings, R., & Geller, E. S. (2019, February). Gratitude from student to professor: Evaluating the impact of thanking a professor. Poster presented at the Annual North Carolina Association for Behavior Analysis, Winston-Salem, NC.\nCallahan, O., Eick, A., Takemoto-Jennings, R., & Geller, E. S. (2019, March). The impact of showing gratitude in the classroom: Evaluating students’ change in their mood-states. Poster presented at the Mid-Atlantic Undergraduate Research Conference, Blacksburg, VA.\n\nAwards\n\nDean’s List\nBeyond Boundaries Scholar\nSumma Cum Laude\n\nConferences\n\nUniC Summit. 07/2021 Quebec, Canada\nCOP25 United Nations Climate Change Conference. 12/2019 Madrid, Spain\nMid-Atlantic Undergraduate Research Conference. 04/2019 Blacksburg, VA, USA\nNorth Carolina Applied Behavioral Association. 02/2019 Winston-Salem, NC, USA"
  },
  {
    "objectID": "about.html#contact-information",
    "href": "about.html#contact-information",
    "title": "About Me",
    "section": "Contact Information:",
    "text": "Contact Information:\nEmail: owen11callahan@gmail.com"
  },
  {
    "objectID": "projects.html#project-2",
    "href": "projects.html#project-2",
    "title": "Projects",
    "section": "Project 2",
    "text": "Project 2"
  },
  {
    "objectID": "projects.html#project-3",
    "href": "projects.html#project-3",
    "title": "Projects",
    "section": "Project 3",
    "text": "Project 3"
  },
  {
    "objectID": "bayes.html",
    "href": "bayes.html",
    "title": "My Portfolio",
    "section": "",
    "text": "Step 1: Install and load necessary packages\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\ninstall.packages(\"bayesplot\")\n\n\nThe downloaded binary packages are in\n    /var/folders/k3/v39j6g_x4bv7mv_xq03986d00000gn/T//RtmptDPPAF/downloaded_packages\n\ninstall.packages(\"brms\")\n\n\nThe downloaded binary packages are in\n    /var/folders/k3/v39j6g_x4bv7mv_xq03986d00000gn/T//RtmptDPPAF/downloaded_packages\n\nlibrary(brms)\n\nLoading required package: Rcpp\n\n\nLoading 'brms' package (version 2.21.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\n\nAttaching package: 'brms'\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\nlibrary(bayesplot)\n\nThis is bayesplot version 1.11.1\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\n\n\nAttaching package: 'bayesplot'\n\n\nThe following object is masked from 'package:brms':\n\n    rhat\n\n\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Generate data for 100 individuals\nn &lt;- 145\n\n# Generate random values for physical activity level (in minutes)\nphysical_activity &lt;- rpois(n, lambda = 60)\n\n# Generate random values for heart rate (in beats per minute)\nheart_rate &lt;- rnorm(n, mean = 75, sd = 10)\n\n# Generate random values for age (in years)\nage &lt;- sample(18:80, n, replace = TRUE)\n\n# Generate random values for weight (in kilograms)\nweight &lt;- rnorm(n, mean = 70, sd = 10)\n\n# Generate random values for gender (1 = male, 2 = female)\ngender &lt;- sample(1:2, n, replace = TRUE)\n\n# Create a data frame\ndata &lt;- data.frame(\n  physical_activity = physical_activity,\n  heart_rate = heart_rate,\n  age = age,\n  weight = weight,\n  gender = factor(gender, labels = c(\"Male\", \"Female\"))\n)\n\n# View the first few rows of the dataset\nhead(data)\n\n  physical_activity heart_rate age   weight gender\n1                55   66.32434  22 66.45458   Male\n2                69   69.98812  28 69.26444 Female\n3                46   82.85591  64 58.31349   Male\n4                61   53.97753  37 63.65252 Female\n5                73   74.57795  19 69.71158   Male\n6                63   70.95191  79 76.70696 Female\n\n\n\n\nStep 2: Bayesian Linear Regression\nI am interested in assessing the relationship between exercise and heart rate.\n\n# In this example, we'll model heart rate as a function of physical activity, age, weight, and gender\nmodel &lt;- brm(\n  heart_rate ~ physical_activity + age + weight + gender,\n  data = data,\n  family = gaussian()  # Assuming heart rate follows a Gaussian (normal) distribution\n)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 3.1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.31 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.043 seconds (Warm-up)\nChain 1:                0.021 seconds (Sampling)\nChain 1:                0.064 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 6e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.04 seconds (Warm-up)\nChain 2:                0.025 seconds (Sampling)\nChain 2:                0.065 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 8e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.038 seconds (Warm-up)\nChain 3:                0.023 seconds (Sampling)\nChain 3:                0.061 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.4e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.039 seconds (Warm-up)\nChain 4:                0.023 seconds (Sampling)\nChain 4:                0.062 seconds (Total)\nChain 4: \n\n# Print a summary of the fitted model\nsummary(model)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: heart_rate ~ physical_activity + age + weight + gender \n   Data: data (Number of observations: 145) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            74.76      9.73    55.99    93.57 1.00     5020     3108\nphysical_activity    -0.06      0.12    -0.29     0.17 1.00     4815     3149\nage                   0.05      0.05    -0.04     0.14 1.00     4233     3039\nweight                0.01      0.10    -0.18     0.19 1.00     4778     3415\ngenderFemale          0.86      1.80    -2.72     4.37 1.00     5894     3139\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    10.69      0.66     9.53    12.07 1.00     4700     3075\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n# Plot the posterior distributions of model parameters\nplot(model)\n\n\n\n\n\n\n\nNow, lets compute our means and credible intervals.\n\n# Extract posterior summary\nposterior_summary &lt;- posterior_summary(model)\n\n# Extract posterior means from the summary\nposterior_means &lt;- fixef(model)\nposterior_means &lt;- posterior_means[, \"Estimate\"]\n\n# Specify prior means (assuming a normal prior with mean 0)\nprior_means &lt;- rep(0, length(posterior_means))\n\n# Compute credible intervals for posterior means\nposterior_quantiles &lt;- quantile(model$posterior, c(0.025, 0.975))\n\n# Assign the quantiles to posterior credible intervals\nposterior_ci &lt;- posterior_quantiles\n\n# Compute credible intervals for prior means\nprior_ci &lt;- c(NA, NA)  # Prior distribution centered at 0, so credible intervals are NA\n\n# Test for difference between posterior and prior means\ndifference_test &lt;- ifelse(posterior_ci[1] &gt; prior_ci[2] | posterior_ci[2] &lt; prior_ci[1],\n                          \"Significant difference\", \"No significant difference\")\n\n\n\nStep 3: Results Interpretation and Reporting\n\n# Extract summary of the posterior distribution\nposterior_summary &lt;- summary(model)\n\n# Extract posterior means and credible intervals\nposterior_summary_table &lt;- as.data.frame(posterior_summary$fixed)\n\n# Extract only the mean and credible intervals\nposterior_summary_table &lt;- posterior_summary_table[, c(\"Estimate\", \"Est.Error\")]\n\n# Rename the columns for clarity\ncolnames(posterior_summary_table) &lt;- c(\"Posterior_Mean\", \"Posterior_Std_Dev\")\n\n# Add parameter names\nposterior_summary_table$Parameter &lt;- rownames(posterior_summary_table)\n\n# Compute credible intervals\nposterior_summary_table$Posterior_CI_Lower &lt;- posterior_summary_table$Posterior_Mean - 1.96 * posterior_summary_table$Posterior_Std_Dev\nposterior_summary_table$Posterior_CI_Upper &lt;- posterior_summary_table$Posterior_Mean + 1.96 * posterior_summary_table$Posterior_Std_Dev\n\n# Specify prior means (assuming a normal prior with mean 0)\nprior_means &lt;- rep(0, nrow(posterior_summary_table))\n\n# Create a summary table\nsummary_table &lt;- data.frame(\n  Parameter = posterior_summary_table$Parameter,\n  Posterior_Mean = posterior_summary_table$Posterior_Mean,\n  Posterior_CI_Lower = posterior_summary_table$Posterior_CI_Lower,\n  Posterior_CI_Upper = posterior_summary_table$Posterior_CI_Upper, Difference_Test = ifelse(posterior_summary_table$Posterior_CI_Lower &gt; 0 | posterior_summary_table$Posterior_CI_Upper &lt; 0,\n                           \"Significant difference\", \"No significant difference\")\n)\nlibrary(knitr)\n\nWarning: package 'knitr' was built under R version 4.3.3\n\nkable(summary_table, format = \"markdown\")\n\n\n\n\n\n\n\n\n\n\n\nParameter\nPosterior_Mean\nPosterior_CI_Lower\nPosterior_CI_Upper\nDifference_Test\n\n\n\n\nIntercept\n74.7606824\n55.6925657\n93.8287990\nSignificant difference\n\n\nphysical_activity\n-0.0589222\n-0.2906713\n0.1728269\nNo significant difference\n\n\nage\n0.0492531\n-0.0437480\n0.1422542\nNo significant difference\n\n\nweight\n0.0090074\n-0.1782019\n0.1962167\nNo significant difference\n\n\ngenderFemale\n0.8638651\n-2.6675074\n4.3952376\nNo significant difference"
  },
  {
    "objectID": "interest.html",
    "href": "interest.html",
    "title": "My Portfolio",
    "section": "",
    "text": "Inspiring teachers are like summer camp counselors. Realistically, they might seem worlds apart, but they share a crucial trait: they are both excellent storytellers. Inspiring teachers unravel complex topics through stories, enabling their students to form emotional connections with the lessons. They focus on more than just imparting knowledge; they empower students to apply imaginative thinking in the classroom. Inspiring teachers build a nurturing space where students feel encouraged to explore and question.\nThe storytelling approach has been a cornerstone of my teaching philosophy, helping me inspire many of my peers in college. After developing this skill as a summer camp counselor for seven years, I continued to weave stories into my lessons as a statistics tutor. For instance, I would take a subject like hypothesis testing and create vivid scenarios, expressing the researchers’ thoughts and dilemmas with their datasets.\nThis approach might seem unconventional, but it has proven highly effective. By using characters and real-life examples, I helped my colleagues transform the often dry principles of statistics into vivid, engaging narratives. It added color to the traditional, monotonous format of lecturing about functions and distributions. This method particularly resonated with literature-driven students in the social sciences, allowing them to forge deeper connections with statistical concepts.\nMy experiences have led me to believe that storytelling is not just a tool but a bridge that connects abstract concepts with tangible understanding. I am eager to bring stories that elucidate key statistical concepts and to inspire every student I meet with an enthusiastic drive. By doing so, I hope to create a lasting impact on their academic and professional journeys."
  },
  {
    "objectID": "interest.html#professional-goals",
    "href": "interest.html#professional-goals",
    "title": "My Portfolio",
    "section": "",
    "text": "Inspiring teachers are like summer camp counselors. Realistically, they might seem worlds apart, but they share a crucial trait: they are both excellent storytellers. Inspiring teachers unravel complex topics through stories, enabling their students to form emotional connections with the lessons. They focus on more than just imparting knowledge; they empower students to apply imaginative thinking in the classroom. Inspiring teachers build a nurturing space where students feel encouraged to explore and question.\nThe storytelling approach has been a cornerstone of my teaching philosophy, helping me inspire many of my peers in college. After developing this skill as a summer camp counselor for seven years, I continued to weave stories into my lessons as a statistics tutor. For instance, I would take a subject like hypothesis testing and create vivid scenarios, expressing the researchers’ thoughts and dilemmas with their datasets.\nThis approach might seem unconventional, but it has proven highly effective. By using characters and real-life examples, I helped my colleagues transform the often dry principles of statistics into vivid, engaging narratives. It added color to the traditional, monotonous format of lecturing about functions and distributions. This method particularly resonated with literature-driven students in the social sciences, allowing them to forge deeper connections with statistical concepts.\nMy experiences have led me to believe that storytelling is not just a tool but a bridge that connects abstract concepts with tangible understanding. I am eager to bring stories that elucidate key statistical concepts and to inspire every student I meet with an enthusiastic drive. By doing so, I hope to create a lasting impact on their academic and professional journeys."
  },
  {
    "objectID": "hyperloop.html",
    "href": "hyperloop.html",
    "title": "Measuring Perceptions of the Hyperloop",
    "section": "",
    "text": "Project Description\nMy primary research objective at Hardt Hyperloop was to investigate user perceptions of a simulated hyperloop trip. The company aimed to determine the appropriate dimensions of the hyperloop capsule before investing extensively in materials. However, since the company had not yet developed a hyperloop accessible to the public, predicting how people would feel inside the capsule was challenging. To address this, our team designed a real-life hyperloop experience—a wooden mock-up capsule equipped with seating and accompanied by a VR headset to illustrate the remaining characteristics of the hyperloop interior.\nWe recruited residents in Groningen to participate in the VR simulation and complete questionnaires. Four main constructs—technology acceptance, perceived safety, perceived comfort, and claustrophobia—were identified for latent class analysis. This model will be instrumental in classifying users in future simulated hyperloop studies.\n\nStep 1: Install and load necessary packages\ninstall.packages(\"poLCA\")\ninstall.packages(\"ggplot2\")\nlibrary(poLCA)\nlibrary(ggplot2)\n\n\nStep 2: Prepare your data\nThe dataframe named survey_data contains columns: tech_acceptance, perceived_safety, perceived_comfort, and claustrophobia. Unfortunately, due to privacy laws, I cannot share the data on this platform. I can only provide my code for analyzing the survey data collected at Hardt Hyperloop.\n\n\nStep 3: Specify the LCA model\nformula &lt;- cbind(tech_acceptance, perceived_safety, perceived_comfort, claustrophobia) ~ 1\n\n\nStep 4: Run the LCA for different numbers of classes and compare models\n# Fit the models with different numbers of classes\nset.seed(123) # For reproducibility\nlca_models &lt;- list()\naic_values &lt;- numeric()\nbic_values &lt;- numeric()\n\nfor (nclass in 1:5) { # Adjust range as needed\n  lca_model &lt;- poLCA(formula, data = survey_data, nclass = nclass, maxiter = 1000, na.rm = TRUE)\n  lca_models[[nclass]] &lt;- lca_model\n  aic_values[nclass] &lt;- lca_model$aic\n  bic_values[nclass] &lt;- lca_model$bic\n}\n\n# Compare models using AIC and BIC\ncomparison &lt;- data.frame(\n  Classes = 1:5,\n  AIC = aic_values,\n  BIC = bic_values\n)\n\nprint(comparison)\n\n\nStep 5: Plot AIC and BIC for model comparison\nggplot(comparison, aes(x = Classes)) +\n  geom_line(aes(y = AIC, color = \"AIC\")) +\n  geom_line(aes(y = BIC, color = \"BIC\")) +\n  geom_point(aes(y = AIC, color = \"AIC\")) +\n  geom_point(aes(y = BIC, color = \"BIC\")) +\n  labs(title = \"Model Comparison\", y = \"Criterion Value\", x = \"Number of Classes\") +\n  scale_color_manual(name = \"Criteria\", values = c(\"AIC\" = \"blue\", \"BIC\" = \"red\")) +\n  theme_minimal()\n\n\nStep 6: Select the best model based on AIC/BIC and interpret parameters\n# Select the best model (assuming the lowest BIC is preferred)\nbest_nclass &lt;- which.min(bic_values)\nbest_model &lt;- lca_models[[best_nclass]]\n\n# Summary of the best model\nsummary(best_model)\n\n# Print class probabilities\nprint(best_model$P)\n\n# Print item-response probabilities\nprint(best_model$probs)\n\n\nStep 7: Graph the item-response probabilities for the best model\n# Create a data frame for plotting\nitem_probs &lt;- data.frame(\n  Class = rep(1:best_nclass, each = length(best_model$probs[[1]])),\n  Variable = rep(names(best_model$probs), each = best_nclass),\n  Level = rep(1:length(best_model$probs[[1]]), best_nclass),\n  Probability = unlist(lapply(best_model$probs, function(x) unlist(x)))\n)\n\n# Plot item-response probabilities\nggplot(item_probs, aes(x = Level, y = Probability, fill = factor(Class))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_wrap(~ Variable, scales = \"free_y\") +\n  labs(title = \"Item-Response Probabilities by Class\", x = \"Response Level\", y = \"Probability\") +\n  scale_fill_brewer(palette = \"Set1\", name = \"Class\") +\n  theme_minimal()"
  },
  {
    "objectID": "multilevel.html",
    "href": "multilevel.html",
    "title": "Project Description",
    "section": "",
    "text": "Project Description\nThis is a research proposal I developed during my Master’s Degree in Environmental Psychology. It includes an introduction and the necessary calculations for designing a hierarchical model that explores the cross-level dynamics within communities in the Netherlands.\nI am still motivated about executing this project because it addresses contemporary challenges in ethnically heterogeneous neighborhoods and provides an unique approach to understanding social cohesion. It assesses the individual-level, cognitive processes in neighborhood behavior through spatial measurement, which can offer alternative evidence for policymakers’ views toward contextual level action (e.g., developing economically vital cities, demolition, and housing renovation) (Van Kempen & Bolt, 2009). By bridging the gap between individual-level processes and neighborhood dynamics, this research has the potential to inform policy interventions and community development initiatives.\nTitle: How Individual Level Processes Undermine Social Cohesion in Ethnically Heterogeneous Neighborhoods\nAbstract\nCurrent research on ethnically heterogeneous neighborhoods lacks individual-level variables, limiting evaluations of social cohesion. Building upon Bektaş and Taşan-Kok (2020), this proposal integrates psychological constructs like place identity and intergroup anxiety to enhance the current framework. By utilizing cognitive mapping techniques, I aim to provide a more comprehensive understanding of social cohesion at the intersection of individual and neighborhood factors. Through hierarchical modeling, this study seeks to assess cross-level interactions and variability across ethnically heterogeneous neighborhoods in the Netherlands.\n\nIntroduction\nModern individualized lifestyles have forgotten the special nature of an intimate society (Kloos et al., 2012). Between 1970 and 2000, close-knit neighborhoods were more common, and they enriched our social lives with friendly routine discourse and spontaneous activities (Putnam, 2000). Residents were generally happier, safer, and healthier (Jordan, 1996). More importantly, they had place identities; a cognitive overlap with the self and the community (Turner et al., 2008). Strong place identities led people to treat neighbors like the self by embodying their problems and successes, which helped create a sense of social cohesion (Stevenson et al., 2019; Turner et al., 2008). However, in recent decades, there has been a noticeable decline in neighborhood engagement, contributing to increased social isolation among individuals (Kloos et al., 2012; Putnam, 2000). This trend is particularly pronounced in the Netherlands, where personal networks consist of fewer neighbors (Völker & Flap, 2007).\nSocial cohesion, often influenced by residents’ ethnic similarities (Van Kempen & Bolt, 2009), facilitates the development of strong place identities (Putnam, 2000; Jordan, 1996). But in ethnically heterogeneous neighborhoods, individual differences create strenuous interactions that can lead to social dissolution (Stevenson et al., 2019), which is defined as the opposite of social cohesion (Chan et al., 2006). Social dissolution was found in Northern Ireland, where residents in religiously desegregated neighborhoods had trouble defining their place identities (Stevenson et al., 2019). With no sense of belonging, they struggle to form strong relationships with each other. More recently, Bektaş and Taşan-Kok (2020) found similar trends in the Kolenkit neighborhood located in Amsterdam, and residents placed most of the blame on the cultural differences. With increased migration on the rise in the Netherlands (Van Kempen & Bolt, 2009), the inevitable reality of ethnically heterogeneous neighborhoods should reveal the criticality of place identity research.\nPolicy interventions often overlook the individual-level processes (e.g., place identity), relying on social mixing policies that fail to address underlying issues (Bektaş & Taşan-Kok, 2020; Van Kempen & Bolt, 2009). In 2007, the Minister of Housing, Districts, and Integration prompted the integration of different social and ethnic groups into neighborhoods with confidence that designing innovative public spaces or identifying the perfect distribution of ethnicities per district would promote social cohesion (Van Kempen & Bolt, 2009). However, direct intergroup contact does not always promote social cohesion, it can sometimes damage relationships even more by creating intergroup anxiety (Turner et al., 2008). Concerns raised by researchers in the Netherlands highlight the need for more nuanced approaches (Kleinhans 2004; Stolle & Harell, 2013; Van Kempen & Bolt, 2009; Völker & Flap, 2007). For instance, Bektaş and Taşan-Kok (2020) found that the social mixing policies only led to superficial interactions that had little impact on social cohesion. Similarly, Stevenson et al. (2019) found that close proximity was insufficient to foster positive intergroup relationships, emphasizing the importance of developing place identity.\nMy PhD proposal aims to complement the research conducted by Bektaş and Taşan-Kok (2020) and Stevenson et al. (2019) by studying the individual-level processes that undermine social cohesion in ethnically heterogeneous neighborhoods in the Netherlands. The problem at hand is the presence of unmeasured individual-level variables contributing to the social dissolution of these neighborhoods, thereby rendering contemporary evaluations of social cohesion inaccurate. This research seeks to address the following questions: (1) What reliable measures of social cohesion can capture the unexplained variability at the individual level? (2) What is the current state of residents’ place identity in ethnically heterogeneous neighborhoods? (3) How can multilevel evidence about individual and contextual variables shed light on social cohesion and intergroup anxiety? (4) What impact does neighborhood leadership have on reducing intergroup anxiety? (5) How do personalized neighborhood features influence place identity?\n\nTheoretical Approach\nThe principles of intergroup contact offer insights into the challenges faced by ethnically heterogeneous neighborhoods in maintaining social cohesion. Introducing an outgroup minority to an ingroup native community can trigger intergroup anxiety—a response rooted in negative expectations of rejection or discrimination during interactions (Turner et al., 2008). This anxiety can result in social isolation and narrowed attention (Van Kempen & Bolt, 2009). While direct interactions do not invariably lead to positive attitudes, intergroup anxiety can be mitigated by factors like pre-established friendships. Pre-established friendships foster inclusivity and reduce uncertainty for outgroup members, facilitating positive contact (Turner et al., 2008). In the context of this proposal, the presence of inclusive members or leaders within the ingroup community, and showing interest in the outgroup culture, may mitigate intergroup anxiety.\nStrong place identities emerge when residents feel a sense of belonging, aided by emotional ties to neighborhood features (Peng et al., 2020; Stevenson et al., 2019). Measuring place identity has multiple avenues, and some suggest that one or two-item scales are adequate (Peng et al., 2020). In community-based research, small-item scales may be a preferable method that would lead to more responses (Pretty et al., 2003).\nMeasuring social cohesion poses challenges, as direct inquiries about neighbors may only capture part of the picture (Bektaş & Taşan-Kok, 2020). Cognitive mapping offers a promising avenue to understand residents’ spatial perceptions (Campbell et al., 2009). Coulton et al. (2013) applied this strategy when studying the effect a neighborhood has on individuals in America, where residents drew boundaries around their perceived neighborhood on a blank map of their district, reflecting disagreements between them. In regards to intergroup anxiety, strong neighborhood ties can be assessed by studying mutual trust (Bektaş & Taşan-Kok, 2020; Stolle & Harell, 2013; Turner et al., 2008). For example, measuring residents’ perceived boundary of trust in regards to theft may reveal previously unexplained variability in intergroup anxiety.\n\nIndividual Predictors\nAt the individual level, disparities in education, length of residency in the neighborhood, and income have been linked to social dissolution (Coulton et al., 2013; Stevenson et al., 2019; Van Kempen & Bolt, 2009), potentially affecting perceived boundaries. Additionally, Coulton et al. (2013) advocate for assessing individual perceptions of collective efficacy—a measure of confidence in neighborhood action (Sampson et al., 1997). Collective efficacy encompasses the belief that others will contribute, influencing one’s engagement in community activities (Kloos et al., 2012).\nContextual Predictors\nAt the contextual level, neighborhood characteristics, which help develop place identity, are also associated with positive intergroup attitudes (Stevenson et al., 2019). People generally prefer to live in orderly areas that contain operational facilities (Coulton et al., 2013). Personalizing neighborhood features such as community names (Stevenson et al., 2019; Taylor et al., 1984), local art (Lowe, 2000), or playgrounds (Kloos et al., 2012) can help people develop place identities toward their neighborhood. As mentioned, intergroup friendships and/or leadership can impact an outgroup member’s sense of belonging. Inclusive leadership, as a contextual level predictor, may shape intergroup attitudes. However, it is critical to note that individual-level processes can still influence these features. For example, public spaces can be effective because they provide an area for close interactions (Easterbrook et al., 2015), but this can be less effective in intergroup settings (Völker & Flap, 2007). Exploring cross-level interactions would help clear up some of the contradictions in the literature.\nProcedure\nResidents will be asked to draw their perceived neighborhood scale (social cohesion) and perceived trust in neighbors (intergroup anxiety) on a blank map of the district. This can be completed through a door-to-door survey or an online questionnaire. More specifically, they will draw a boundary on a map that encompasses their neighborhood boundaries and the perceived space in which they would feel safe leaving their personal belongings unattended for 24 hours, excluding individuals from outside their neighborhood who could potentially be involved in theft.\nGIS software can be used to estimate residuals when analyzing mapping data, as suggested by Coulton et al. (2013). Alternatively, R programming, supported by packages such as “ggmap,” “choroplethr,” and “choroplethrMaps,” can be utilized for this purpose. Given my extensive familiarity with R software, it will be the chosen tool for analysis.\n\n1. Exploratory Data Analysis (EDA):\n# Summary statistics for individual-level variables\nsummary(data[, c(\"education\", \"length_res\", \"income\", \"in_efficacy\")])\n\n# Summary statistics for neighborhood-level variables\nsummary(data[, c(\"facilities\", \"names\", \"art\", \"playgrounds\", \"leadership\")])\n\n# Check for missing values\ncolSums(is.na(data))\n\n# Explore relationships of individual efficacy using visualizations\nhist(data$in_efficacy, main = \"Distribution of Individual Efficacy\", xlab = \"in_efficacy\")\nboxplot(inter_anx ~ gender, data = data, main = \"Behavior Score by Gender\")\n\n\n2. Model Specification and Fitting:\n# Define the multilevel model with both fixed and random effects\nmultilevel_model &lt;- lmer(boundary ~ education + length_res + income + in_efficacy + facilities + names + art + playgrounds + leadership + (1 | neighborhood_id), data = data)\n\n# Print summary of the model\nsummary(multilevel_model)\n\n\n3. Model Diagnostics:\n# Check for multicollinearity among predictors\ncor(data[, c(\"education\", \"length_res\", \"income\", \"in_efficacy\")])\n\n# Plot residuals to check for homoscedasticity and normality\nplot(residuals(multilevel_model), main = \"Residuals vs. Fitted Values\")\n\n# Check for influential cases or outliers\nplot(influence(multilevel_model)$hat, main = \"Influence Plot\")\n\n\n4. Interpretation and Inference:\n# Extract fixed effects estimates\nfixed_effects &lt;- fixef(multilevel_model)\n\n# Interpretation of fixed effects\nprint(fixed_effects)\n\n\n5. Post-estimation Analyses:\n# Conduct sensitivity analysis by excluding influential cases\nsensitivity_model &lt;- lmer(boundary ~ education + length_res + income + in_efficacy + facilities + names + art + playgrounds + leadership + (1 | neighborhood_id), data = data[-influence(multilevel_model)$which, ])\n\n# Compare results with the original model\nanova(multilevel_model, sensitivity_model)\n\n\n6. Reporting and Visualization:\n# Visualize the estimated coefficients\ndotplot(ranef(multilevel_model), main = \"Random Effects\")\n\n# Generate a forest plot for fixed effects\ncoefplot(multilevel_model, main = \"Fixed Effects Coefficients\")\n\n\nReferences\nBektaş, Y., & Taşan-Kok, T. (2020). Love thy neighbor? Remnants of the social-mix policy in the Kolenkit neighborhood, Amsterdam. Journal of Housing and the Built Environment, 35, 743-761.\nCampbell, E., Henly, J. R., Elliott, D. S., & Irwin, K. (2009). Subjective constructions of neighborhood boundaries: lessons from a qualitative study of four neighborhoods. Journal of Urban Affairs, 31(4), 461-490.\nChan, J., To, H. P., & Chan, E. (2006). Reconsidering social cohesion: Developing a definition and analytical framework for empirical research. Social Indicators Research, 75, 273-302.\nCoulton, C. J., Jennings, M. Z., & Chan, T. (2013). How big is my neighborhood? Individual and contextual effects on perceptions of neighborhood scale. American Journal of Community Psychology, 51(1-2), 140-150.\nEasterbrook, M. J., & Vignoles, V. L. (2015). When friendship formation goes down the toilet: Design features of shared accommodation influence interpersonal bonds and well‐being. British Journal of Social Psychology, 54(1), 125-139.\nJordan, B. (1996). A theory of poverty & social exclusion.\nKleinhans, R. (2004). Social implications of housing diversification in urban renewal: A review of recent literature. Journal of Housing and the Built Environment, 19, 367-390.\nKloos, B., Hill, J., Thomas, E., Wandersman, A., Elias, M. J., & Dalton, J. H. (2012). Community psychology. Belmont, CA: Cengage Learning.\nLowe, S. S. (2000). Creating community: Art for community development. Journal of Contemporary Ethnography, 29(3), 357-386.\nLynch, S. M., & Bartlett, B. (2019). Bayesian statistics in sociology: Past, present, and future. Annual Review of Sociology, 45, 47-68.\nPeng, J., Strijker, D., & Wu, Q. (2020). Place identity: How far have we come in exploring its meanings?. Frontiers in Psychology, 11, 503569.\nPretty, G. H., Chipuer, H. M., & Bramston, P. (2003). Sense of place amongst adolescents and adults in two rural Australian towns: The discriminating features of place attachment, sense of community and place dependence in relation to place identity. Journal of environmental psychology, 23(3), 273-287.\nPutnam, R. D. (2000). Bowling alone: The collapse and revival of American community. Simon and Schuster.\nPutnam, R. D. (2007). E pluribus unum: Diversity and community in the twenty‐first century the 2006 Johan Skytte Prize Lecture. Scandinavian Political Studies, 30(2), 137-174.\nStevenson, C., Easterbrook, M., Harkin, L., McNamara, N., Kellezi, B., & Shuttleworth, I. (2019). Neighborhood identity helps residents cope with residential diversification: Contact in increasingly mixed neighborhoods of Northern Ireland. Political Psychology, 40(2), 277-295.\nStolle, D., & Harell, A. (2013). Social capital and ethno-racial diversity: Learning to trust in an immigrant society. Political Studies, 61(1), 42-66.\nTaylor, R. B., Gottfredson, S. D., & Brower, S. (1984). Neighborhood naming as an index of attachment to place. Population and Environment, 7, 103-125.\nTurner, R. N., Hewstone, M., Voci, A., & Vonofakou, C. (2008). A test of the extended intergroup contact hypothesis: The mediating role of intergroup anxiety, perceived ingroup and outgroup norms, and inclusion of the outgroup in the self. Journal of Personality and Social Psychology, 95(4), 843.\nVan Kempen, R., & Bolt, G. (2009). Social cohesion, social mix, and urban policies in the Netherlands. Journal of Housing and the Built Environment, 24, 457-475.\nVölker, B., & Flap, H. (2007). Sixteen million neighbors: A multilevel study of the role of neighbors in the personal networks of the Dutch. Urban Affairs Review, 43(2), 256-284."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Welcome to My Website",
    "section": "About Me",
    "text": "About Me\nHi there! I’m Owen, a passionate data enthusiast, R programmer, and lifelong learner. Here you’ll find information about my background, skills, and education. Whether you’re a potential employer, colleague, or collaborator, I’m excited to share my journey with you."
  },
  {
    "objectID": "index.html#discover-my-interests",
    "href": "index.html#discover-my-interests",
    "title": "Welcome to My Website",
    "section": "Discover My Interests",
    "text": "Discover My Interests\nBeyond data and programming, I’m passionate about teaching and activism. From nature and sports to culinary arts and history, there’s more to me than just data! Get to know the person behind the code."
  },
  {
    "objectID": "index.html#dive-into-my-r-projects",
    "href": "index.html#dive-into-my-r-projects",
    "title": "Welcome to My Website",
    "section": "Dive into My R Projects",
    "text": "Dive into My R Projects\nExplore my portfolio of R projects, where I showcase my data analysis, visualization, and programming skills. From data wrangling and exploratory analysis to data storytelling, each project tells a unique story."
  },
  {
    "objectID": "index.html#connect-with-me",
    "href": "index.html#connect-with-me",
    "title": "Welcome to My Website",
    "section": "Connect with Me",
    "text": "Connect with Me\nInterested in collaborating on a project, discussing data science topics, or just saying hello? Don’t hesitate to reach out! Connect with me via email at owen11callahan@gmail.com"
  },
  {
    "objectID": "begin.html",
    "href": "begin.html",
    "title": "Fear Not!",
    "section": "",
    "text": "If you’re new to RStudio, you’ve come to the right place. Beyond my advanced projects, I’ve created this beginner-friendly page to share some basic computations. Whether you’re an employer or a colleague visiting, I’m delighted to have your interest.\nWe all have our first “sense of horror” moment when we first encounter RStudio. For me, it was during my first year in a biostatistics class with my teacher, Tessa. She had us working on projects using RStudio during class, and those initial weeks were chaotic. We were all frustrated and terrified by what appeared on our screens—scary red errors seemed to pop up every second, and we felt like we were making no progress at all. However, Tessa remained by our side. She dedicated the beginning of every class to addressing the typical errors we encountered, guiding us toward smart tactics and helpful online resources.\nBy the end of the semester, our perception of RStudio had completely changed. Tessa helped us realize that it wasn’t some dark abyss from which no student returns; it was a powerful tool that made statistics easier to compute. We bid farewell to the daunting equations we were accustomed to and began computing statistical techniques at a rapid pace. To this day, every one of my classmates continues to use RStudio with excitement and without fear. This software is truly your best friend if you use it wisely.\n\n\n\nAsking for Help\nPackages\nFunctions\nData Visualization\n\n\n\nMost experts mention asking for help at the end of their tutorials, but I believe it deserves to be first. Learning R involves seeking assistance a lot. There are countless online resources available to answer your questions. Below are some of my favorite resources:\n\nThe Help Center in RStudio: Located in the bottom right pane of your window, the Help Center provides detailed explanations of each command in R. You can learn about the purpose of each function, as well as their sub-commands and properties.\nYouTube: Useful tutorials for visual learners.\nReddit: A community where everyone deals with problems using R. Visit Reddit to view a wide range of questions and answers from people around the globe.\nGenerative AI: Appropriate in certain circumstances. Use with caution, as AI can occasionally produce incorrect solutions to your code.\n\nBy leveraging these resources, you can make your learning journey with R more efficient and less daunting.\n\n\n\nR packages are collections of functions, data, and documentation that extend the capabilities of base R, making it easier and more efficient to perform a wide range of tasks. Here’s a brief overview of what R packages can do for new learners:\n\n\nPackages can simplify complex tasks into manageable functions. For instance, data visualization with ggplot2 or data manipulation with dplyr breaks down intricate processes into straightforward commands.\n\n\n\nPackages like tidyverse offer tools for data import, tidying, transformation, visualization, and modeling. These integrated tools follow a consistent philosophy, making data analysis more intuitive.\n\n\n\nThere are packages for almost every field and application, from bioinformatics (Bioconductor) to finance (quantmod). These packages provide specialized functions tailored to specific tasks, saving time and effort.\n\n\n\nPackages such as ggplot2, plotly, and shiny help create advanced and interactive data visualizations. This makes it easier to explore and communicate data insights.\n\n\n\nPackages like knitr and rmarkdown allow for dynamic report generation, integrating code and its output in a single document. This streamlines the workflow from data analysis to reporting.\n\n\n\nPackages like dplyr and tidyr help in efficiently handling and processing large datasets. This is particularly useful for big data applications where performance is crucial.\n\n\n\nUsing packages ensures that your code is reproducible. The functions are well-documented and tested, reducing the chances of errors and improving the reliability of your analysis.\n\n\n\nThe R community is robust, with packages often accompanied by comprehensive documentation, tutorials, and vignettes. This makes it easier for learners to understand and implement new techniques.\n\n\n\n\nWhen I learned how to create my own functions, I felt like the creative side of R expanded beyond my expectations. I could tailor a command to address exactly what I needed to perform on a given set of data.\nThe function() command is a fundamental building block in R, enabling users to create their own functions. This is crucial for both simplifying repetitive tasks and organizing code in a clean, efficient manner. Here’s what the function() command can do for new learners using R:\n\n\nCreating functions allows you to encapsulate repetitive code into reusable blocks. This reduces redundancy and makes your scripts more concise and readable. For example, if you frequently perform the same data transformation, you can write a function for it and call it whenever needed.\n\n\n\nFunctions help in organizing code logically. By breaking down complex procedures into smaller, manageable functions, you make your code more modular and easier to debug and maintain.\n\n\n\nUsing functions can significantly enhance the readability of your code. Descriptive function names and clear parameter definitions help others (and your future self) understand the purpose and usage of the code more quickly.\n\n\n\nFunctions allow you to use parameters to make your code more flexible. Instead of hard-coding values, you can pass different arguments to your functions, making them adaptable to various inputs and scenarios.\n\n\n\nFunctions contribute to reproducibility in your analyses. By encapsulating specific tasks, you ensure that the same operations can be repeated with different data or settings, leading to consistent results.\n\n\n\nOnce you write a function, you can reuse it across different projects. This saves time and effort, as you don’t need to rewrite the same code for similar tasks.\n\n\n\nWriting functions encourages good programming practices, such as documenting code and testing individual components. This can lead to more robust and error-free scripts.\nExample of Using function() in R\n\n# Define a function to calculate the mean of a numeric vector\ncalculate_mean &lt;- function(numbers) {\n  if(!is.numeric(numbers)) {\n    stop(\"Input must be a numeric vector\")\n  }\n  mean_value &lt;- mean(numbers)\n  return(mean_value)\n}\n\n# Use the function with a numeric vector\nsample_data &lt;- c(4, 8, 15, 16, 23, 42)\naverage &lt;- calculate_mean(sample_data)\nprint(average)\n\n[1] 18\n\n\n\n\n\n\nLet’s explore one of my favorite, and also one of the most essential packages in R, ggplot2.\n\n\nFirst, you need to install and load the ggplot2 package:\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\ninstall.packages(\"ggplot2\")\n\n\nThe downloaded binary packages are in\n    /var/folders/k3/v39j6g_x4bv7mv_xq03986d00000gn/T//RtmpItlqed/downloaded_packages\n\nlibrary(ggplot2)\n\n\n\n\nggplot2 follows the grammar of graphics, which means you build plots layer by layer. The essential components are:\n\nData: The dataset you’re plotting.\nAesthetics (aes): The mapping of variables to visual properties like x and y coordinates, colors, sizes, etc.\nGeometries (geom): The type of plot you want to create (e.g., points, lines, bars).\nFacets: Subplots based on the values of one or more variables.\nScales: Control how data values are mapped to visual properties.\nCoordinate Systems: Control the coordinate space.\nThemes: Control the appearance of the plot.3. Basic Plot Types\n\nScatter Plot\n\n# Load example data\ndata(mtcars)\n\n# Create a scatter plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\nLine Plot\n\n# Create a line plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_line() +\n  labs(title = \"Line Plot of MPG vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\nBar Plot\n\n# Create a bar plot\nggplot(data = mtcars, aes(x = factor(cyl))) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Cylinder Counts\",\n       x = \"Number of Cylinders\",\n       y = \"Count\")\n\n\n\n\nHistogram\n\n# Create a histogram\nggplot(data = mtcars, aes(x = mpg)) +\n  geom_histogram(binwidth = 2) +\n  labs(title = \"Histogram of MPG\",\n       x = \"Miles Per Gallon (MPG)\",\n       y = \"Frequency\")\n\n\n\n\nBox Plot\n\n# Create a box plot\nggplot(data = mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot() +\n  labs(title = \"Box Plot of MPG by Cylinder Count\",\n       x = \"Number of Cylinders\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\n\n\n\nAdding Colors\n\n# Scatter plot with color\nggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight by Cylinder Count\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\",\n       color = \"Cylinders\")\n\n\n\n\nAdding Size\n\n# Scatter plot with color and size\nggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl), size = hp)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight by Cylinder Count\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\",\n       color = \"Cylinders\",\n       size = \"Horsepower\")\n\n\n\n\nFaceting\n\n# Faceted scatter plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  facet_wrap(~ cyl) +\n  labs(title = \"Scatter Plot of MPG vs Weight by Cylinder Count\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\nThemes\n\n# Scatter plot with theme\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\") +\n  theme_minimal()"
  },
  {
    "objectID": "begin.html#my-learning-journey",
    "href": "begin.html#my-learning-journey",
    "title": "Fear Not!",
    "section": "",
    "text": "Asking for Help\nPackages\nFunctions\nData Visualization\n\n\n\nMost experts mention asking for help at the end of their tutorials, but I believe it deserves to be first. Learning R involves seeking assistance a lot. There are countless online resources available to answer your questions. Below are some of my favorite resources:\n\nThe Help Center in RStudio: Located in the bottom right pane of your window, the Help Center provides detailed explanations of each command in R. You can learn about the purpose of each function, as well as their sub-commands and properties.\nYouTube: Useful tutorials for visual learners.\nReddit: A community where everyone deals with problems using R. Visit Reddit to view a wide range of questions and answers from people around the globe.\nGenerative AI: Appropriate in certain circumstances. Use with caution, as AI can occasionally produce incorrect solutions to your code.\n\nBy leveraging these resources, you can make your learning journey with R more efficient and less daunting.\n\n\n\nR packages are collections of functions, data, and documentation that extend the capabilities of base R, making it easier and more efficient to perform a wide range of tasks. Here’s a brief overview of what R packages can do for new learners:\n\n\nPackages can simplify complex tasks into manageable functions. For instance, data visualization with ggplot2 or data manipulation with dplyr breaks down intricate processes into straightforward commands.\n\n\n\nPackages like tidyverse offer tools for data import, tidying, transformation, visualization, and modeling. These integrated tools follow a consistent philosophy, making data analysis more intuitive.\n\n\n\nThere are packages for almost every field and application, from bioinformatics (Bioconductor) to finance (quantmod). These packages provide specialized functions tailored to specific tasks, saving time and effort.\n\n\n\nPackages such as ggplot2, plotly, and shiny help create advanced and interactive data visualizations. This makes it easier to explore and communicate data insights.\n\n\n\nPackages like knitr and rmarkdown allow for dynamic report generation, integrating code and its output in a single document. This streamlines the workflow from data analysis to reporting.\n\n\n\nPackages like dplyr and tidyr help in efficiently handling and processing large datasets. This is particularly useful for big data applications where performance is crucial.\n\n\n\nUsing packages ensures that your code is reproducible. The functions are well-documented and tested, reducing the chances of errors and improving the reliability of your analysis.\n\n\n\nThe R community is robust, with packages often accompanied by comprehensive documentation, tutorials, and vignettes. This makes it easier for learners to understand and implement new techniques.\n\n\n\n\nWhen I learned how to create my own functions, I felt like the creative side of R expanded beyond my expectations. I could tailor a command to address exactly what I needed to perform on a given set of data.\nThe function() command is a fundamental building block in R, enabling users to create their own functions. This is crucial for both simplifying repetitive tasks and organizing code in a clean, efficient manner. Here’s what the function() command can do for new learners using R:\n\n\nCreating functions allows you to encapsulate repetitive code into reusable blocks. This reduces redundancy and makes your scripts more concise and readable. For example, if you frequently perform the same data transformation, you can write a function for it and call it whenever needed.\n\n\n\nFunctions help in organizing code logically. By breaking down complex procedures into smaller, manageable functions, you make your code more modular and easier to debug and maintain.\n\n\n\nUsing functions can significantly enhance the readability of your code. Descriptive function names and clear parameter definitions help others (and your future self) understand the purpose and usage of the code more quickly.\n\n\n\nFunctions allow you to use parameters to make your code more flexible. Instead of hard-coding values, you can pass different arguments to your functions, making them adaptable to various inputs and scenarios.\n\n\n\nFunctions contribute to reproducibility in your analyses. By encapsulating specific tasks, you ensure that the same operations can be repeated with different data or settings, leading to consistent results.\n\n\n\nOnce you write a function, you can reuse it across different projects. This saves time and effort, as you don’t need to rewrite the same code for similar tasks.\n\n\n\nWriting functions encourages good programming practices, such as documenting code and testing individual components. This can lead to more robust and error-free scripts.\nExample of Using function() in R\n\n# Define a function to calculate the mean of a numeric vector\ncalculate_mean &lt;- function(numbers) {\n  if(!is.numeric(numbers)) {\n    stop(\"Input must be a numeric vector\")\n  }\n  mean_value &lt;- mean(numbers)\n  return(mean_value)\n}\n\n# Use the function with a numeric vector\nsample_data &lt;- c(4, 8, 15, 16, 23, 42)\naverage &lt;- calculate_mean(sample_data)\nprint(average)\n\n[1] 18\n\n\n\n\n\n\nLet’s explore one of my favorite, and also one of the most essential packages in R, ggplot2.\n\n\nFirst, you need to install and load the ggplot2 package:\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\ninstall.packages(\"ggplot2\")\n\n\nThe downloaded binary packages are in\n    /var/folders/k3/v39j6g_x4bv7mv_xq03986d00000gn/T//RtmpItlqed/downloaded_packages\n\nlibrary(ggplot2)\n\n\n\n\nggplot2 follows the grammar of graphics, which means you build plots layer by layer. The essential components are:\n\nData: The dataset you’re plotting.\nAesthetics (aes): The mapping of variables to visual properties like x and y coordinates, colors, sizes, etc.\nGeometries (geom): The type of plot you want to create (e.g., points, lines, bars).\nFacets: Subplots based on the values of one or more variables.\nScales: Control how data values are mapped to visual properties.\nCoordinate Systems: Control the coordinate space.\nThemes: Control the appearance of the plot.3. Basic Plot Types\n\nScatter Plot\n\n# Load example data\ndata(mtcars)\n\n# Create a scatter plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\nLine Plot\n\n# Create a line plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_line() +\n  labs(title = \"Line Plot of MPG vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\nBar Plot\n\n# Create a bar plot\nggplot(data = mtcars, aes(x = factor(cyl))) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Cylinder Counts\",\n       x = \"Number of Cylinders\",\n       y = \"Count\")\n\n\n\n\nHistogram\n\n# Create a histogram\nggplot(data = mtcars, aes(x = mpg)) +\n  geom_histogram(binwidth = 2) +\n  labs(title = \"Histogram of MPG\",\n       x = \"Miles Per Gallon (MPG)\",\n       y = \"Frequency\")\n\n\n\n\nBox Plot\n\n# Create a box plot\nggplot(data = mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot() +\n  labs(title = \"Box Plot of MPG by Cylinder Count\",\n       x = \"Number of Cylinders\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\n\n\n\nAdding Colors\n\n# Scatter plot with color\nggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight by Cylinder Count\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\",\n       color = \"Cylinders\")\n\n\n\n\nAdding Size\n\n# Scatter plot with color and size\nggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl), size = hp)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight by Cylinder Count\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\",\n       color = \"Cylinders\",\n       size = \"Horsepower\")\n\n\n\n\nFaceting\n\n# Faceted scatter plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  facet_wrap(~ cyl) +\n  labs(title = \"Scatter Plot of MPG vs Weight by Cylinder Count\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\nThemes\n\n# Scatter plot with theme\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\") +\n  theme_minimal()"
  }
]