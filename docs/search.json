[
  {
    "objectID": "modelsel.html",
    "href": "modelsel.html",
    "title": "Interpretations of Anova",
    "section": "",
    "text": "1. ANOVA (Analysis of Variance)\nANOVA is a statistical method used to analyze the differences among group means in a sample. It tests whether there are any significant differences between the means of two or more groups.\n\nKey Points:\n\nPurpose: To determine if there are any significant differences between the means of two or more groups.\nTypes: One-way ANOVA (for one independent variable), Two-way ANOVA (for two independent variables).\nAssumptions: Normally distributed data, homogeneity of variances (tested using Levene’s test), independence of observations.\nR Packages: stats, car, afex\n\n\n# Load required packages\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\nlibrary(stats)  # Base R package for ANOVA\nlibrary(car)    # For ANOVA with Type III sums of squares\n\nLoading required package: carData\n\n# One-way ANOVA\nmodel &lt;- aov(mpg ~ cyl, data = mtcars)\nsummary(model)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncyl          1  817.7   817.7   79.56 6.11e-10 ***\nResiduals   30  308.3    10.3                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Two-way ANOVA (Factorial design)\nmodel &lt;- aov(mpg ~ cyl + vs + cyl:vs, data = mtcars)\nsummary(model)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncyl          1  817.7   817.7  77.035 1.58e-09 ***\nvs           1    2.4     2.4   0.224    0.640    \ncyl:vs       1    8.7     8.7   0.823    0.372    \nResiduals   28  297.2    10.6                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nNotes:\n\nFactorial designs: involve manipulating multiple independent variables (IVs) across different levels to observe their combined effects on dependent variables (DVs).\nRM ANOVA: is used when measurements are taken from the same subjects over multiple time points or conditions.\n\n\n\n\n2. ANCOVA (Analysis of Covariance)\nANCOVA extends ANOVA by including one or more continuous variables (covariates) in addition to the categorical independent variable(s). It adjusts group means based on these covariates.\n\nKey Points:\n\nPurpose: To compare group means while statistically controlling for the effects of one or more covariates.\nAssumptions: Linearity between covariates and dependent variable, homogeneity of regression slopes.\nR Packages: car, lmtest\n\n\nmodel &lt;- lm(mpg ~ wt + factor(am), data = mtcars)\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ wt + factor(am), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5295 -2.3619 -0.1317  1.4025  6.8782 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.32155    3.05464  12.218 5.84e-13 ***\nwt          -5.35281    0.78824  -6.791 1.87e-07 ***\nfactor(am)1 -0.02362    1.54565  -0.015    0.988    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.098 on 29 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7358 \nF-statistic: 44.17 on 2 and 29 DF,  p-value: 1.579e-09\n\n\n\n\nNotes:\n\nCovariate (CV): Acts to adjust group means based on pretest or unexplained exogenous variables. Must be continuous and have a linear relationship with the dependent variable (DV).\nReasons to use ANCOVA: Provides more power and adjusts for confounding variables.\nReasons not to use ANCOVA: Badly chosen variables may obscure real differences. Also, do not use ANCOVA if your covariate is related to group membership!!\n\n\n\n\n3. MANOVA (Multivariate Analysis of Variance)\nMANOVA is used when there are multiple dependent variables (DVs) and one or more independent variables (IVs). It tests the simultaneous effect of multiple IVs on multiple DVs.\n\nKey Points:\n\nPurpose: To test the joint effect of multiple independent variables on multiple dependent variables.\nAssumptions: Multivariate normality, homogeneity of covariance matrices.\nR Packages: MANOVA, car, multcomp\n\n\ndata(\"iris\")\nmodel &lt;- manova(cbind(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) ~ Species, data = iris)\nsummary(model)\n\n           Df Pillai approx F num Df den Df    Pr(&gt;F)    \nSpecies     2 1.1919   53.466      8    290 &lt; 2.2e-16 ***\nResiduals 147                                            \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nNotes:\n\nReasons to use MANOVA: Tests interaction of multiple DVs and IVs, protects against type 1 error.\nReasons not to use MANOVA: Techniques do not answer all questions; still need separate ANOVAs for significance testing.\n\n\n\nRoy-Bargmann Stepdown:\nRoy-Bargmann stepdown procedure prioritizes dependent variables (DVs) in a sequence of ANOVAs and ANCOVAs to understand which variables explain group differences most significantly.\n\nThe highest priority DV is tested in a univariate ANOVA.\nThe next most important DVs are tested in an ANCOVA with the high priority DV acting as a CV.\nSometimes the lower order DV still has an effect but not in the stepdown analysis, this tells us that yes, the DV explains some of the variance, but has no unique variability with the treatment after adjusting with the higher order DV and it is not needed.\n\n# dv1 and dv2 are dependent variables of interest\n\n# Univariate ANOVA for dv1\nmodel1 &lt;- aov(dv1 ~ group, data = mydata)\n\n# ANCOVA for dv2 with dv1 as covariate\nmodel2 &lt;- lm(dv2 ~ dv1 + group, data = mydata)\n\nNotes\n\nTo find out which DV explains group differences in means, test them with separate ANOVAs.\nTo find out which groups caused an effect, use post-hoc comparisons\n\n\n\n\n\n4. Repeated Measures ANOVA\nRM ANOVA is used when measurements are taken from the same subjects over multiple time points or conditions.\n\nKey Points:\n\nWhen k = 2: Use paired t-test or ANCOVA\nWhen k &gt; 2: Use RM ANOVA or Profile Analysis\n\nmodel &lt;- aov(score ~ time + group + time:group + Error(subject/time), data = mydata)\n\n\nSphericity Violation and Corrections\n\nSphericity violation occurs when the variances of the differences between all possible pairs of within-subject conditions are not equal.\nViolation leads to inflated type 1 error\nCorrective measures such as Mauchly’s test and Greenhouse-Geisser correction (especially for small n) can be applied.\n\n# Sphericity test and correction\nmodel &lt;- aov(score ~ condition + Error(subject/condition), data = mydata)\nsummary(model)\n\n# Check for sphericity violation\nsummary(aov(score ~ condition + Error(subject/condition), data = mydata))\n\n# Greenhouse-Geisser correction\nsummary(aov(score ~ condition + Error(subject/condition), data = mydata, Greenhouse-Geisser = TRUE))"
  },
  {
    "objectID": "active.html",
    "href": "active.html",
    "title": "Climate Activism",
    "section": "",
    "text": "Climate change is the most essential topic to discuss right now since it affects every aspect of our life, from the environment and economy to health and society. We must act soon since the situation is grave, and I believe that everyone can contribute to the cause. As an activist, I wish to contribute to the solution by advocating for policies and techniques that promote sustainability and environmental justice.\n\n\nMy Journey\nI was just twelve years old when I began my path as a climate activist. The first time I learnt about global warming, it sparked a passion in me that has only gotten stronger with time. I grew captivated with its science and promised myself to learn more about its effects. All of my school writings concentrated around this critical issue.\n\nOne of my first projects was to create a floating windmill, which I entered in a competition at the local university. This not only sparked an unwavering interest in renewable energy, but also demonstrated how ingenuity and creativity can be used to solve environmental challenges. This endeavor was the catalyst for my full-fledged involvement in climate advocacy.\n\n\n\nVirginia Tech Climate Action Committee\nAs a member of the Virginia Tech Climate Action Committee, I was able to collaborate directly with university administration, teachers, and students to create and execute ways to reduce the campus’s greenhouse gas emissions. Our endeavors included energy-efficient construction projects, environmentally friendly transit choices, and significant educational campaigns. This program refined my policy-making and collaborative problem-solving abilities while also demonstrating the crucial role that academic institutions play in tackling climate change.\n\n\n\nRepresenting at COP25\nBeing a student delegate at COP25 marked a pivitol point in my advocacy. Attending the United Nations Climate Change Conference gave me a global perspective on the climate situation and the many methods that countries are adopting. Meeting with officials, scientists, and other activists from around the world was both motivating and eye-opening. It strengthened my confidence in the value of international collaboration and the critical necessity for collective action to combat climate change.\n\n\n\nEnvironmental Psychology and Hardt Hyperloop\nContinuing my study, I earned a master’s degree in Environmental Psychology, where I used my talents and experiences to better understand the human aspects of environmental concerns. My research at Hardt Hyperloop was focused on sustainable transportation solutions, namely novel approaches to cut carbon emissions in the transportation industry. This study was both hard and enjoyable, and it reinforced my confidence in the necessity of incorporating psychological insights into environmental efforts to promote sustainable habits and practices.\n\n\nThe Scientist Rebellion\nJoining the Scientist Rebellion was a bold step that demonstrated my dedication to climate justice. This movement, made up of scientists fighting for bold changes to solve the climate crisis, allowed me to combine my scientific training with activism. We hoped that by holding peaceful protests and public rallies, we might put pressure on governments and companies to pursue more aggressive climate action. This experience helped me better appreciate civil disobedience as a tactic for social change, as well as the crucial role that scientists play in the climate movement.\n\n\nExpanding Activism\nMy advocacy has also taken me beyond climate change and into the Free Palestine cause. The connection between environmental justice and human rights has pushed me to broaden my involvement. Advocacy for Palestine has expanded my knowledge of global justice by demonstrating that genuine sustainability requires social equality and human dignity. This larger viewpoint has reminded me that the battle for a fair and sustainable society is multidimensional and interdependent.\n\n\nReflecting on My Journey\nMy engagement in these many aspects of climate advocacy has greatly influenced who I am now. Each event taught me vital insights about the intricacies of the climate catastrophe and the varied solutions required to address it. Creating the VTCJ Foundation showed me the value of community and grassroots initiatives. My work with the Virginia Tech Climate Action Committee emphasized the significance of institutional transformation. Representing at COP25 gave a global perspective, while engaging in Scientist Rebellion highlighted the importance of bold, direct action.\nLooking ahead, I am more determined than ever to continue working with groups dedicated to climate change. I want to use my experiences to inspire and motivate others, creating a culture of sustainability and resilience. Working together, we can build a future in which renewable energy is the norm, ecosystems are preserved, and all communities live in harmony with the environment.\n\nThank you for showing interest in my climate advocacy. I hope that my path motivates you to take action in your own manner. Together, we can make a difference."
  },
  {
    "objectID": "rconcept.html",
    "href": "rconcept.html",
    "title": "Essential Statistical Concepts",
    "section": "",
    "text": "1. Power\nStatistical power is the probability that a test will correctly reject a false null hypothesis (H0). It is influenced by the significance level (alpha), sample size, and effect size.\n\nType I error (α): The probability of rejecting H0 when it is true.\n\n\\(α=P(rejecting H0∣H0 is true)\\)\nFixed by the user (common values: 0.05, 0.01).\n\nType II error (β): The probability of not rejecting H0 when it is false.\n\n\\(β=P(not rejecting H0∣H0 is false)\\)\nNot fixed, depends on α, sample size, and effect size.\n\nPower (1-β): The probability of correctly rejecting H0 when it is false.\n\n\\(1−β=P(rejecting H0∣H0 is false)\\)\n\n\n\nImproving Power:\n\nIncrease sample size.\nIncrease effect size.\nIncrease alpha level.\nDecrease error variance by blocking or using covariates (CV).\n\nlibrary(pwr)\n\n# Power analysis\npwr.t.test(d = 0.5, power = 0.8, sig.level = 0.05, type = \"two.sample\")\n\n\n\n2. Effect Size\nEffect size measures the strength of the relationship between variables. It is crucial for understanding the practical significance of research findings.\n\nTypes: Cohen’s d, Pearson’s r, eta squared (η²), partial eta squared.\nUse in ANOVA: η² and partial η² measure the proportion of variance explained by an independent variable.\n\nlibrary(effsize)\n\n# Calculate Cohen's d for two groups\ncohen.d(mtcars$mpg[mtcars$cyl == 4], mtcars$mpg[mtcars$cyl == 6])\n\nWhen to Use η²\n\nSimple ANOVA: Use η² when you are conducting a simple one-way ANOVA or when your model includes only one independent variable.\nTotal Variance: Use η² if you are interested in understanding the proportion of total variance explained by an independent variable, including both the effect and error terms.\n\n\n\nWhen to Use Partial η²\n\nComplex Models: Use partial η² when you have multiple independent variables or a factorial design. Partial η² provides a clearer picture of the effect size for each variable, controlling for the variance explained by other variables in the model.\nControl for Other Variables: Use partial η² when you want to control for other factors in the model, thus isolating the effect of a specific variable.\nMultiple Comparisons: Use partial η² in repeated measures or mixed-design ANOVA, where you want to account for the variability within subjects or other factors.\n\n\n\n\n3. Sample Size\nDetermining the appropriate sample size is essential to ensure sufficient power and reliable results. I highly recommend you calculate your appropriate sample size before collecting data (it will save you a lot of time and energy!)\n\nDepends on: Desired power, effect size, significance level.\nSample size calculators: pwr package in R.\n\n# Use same method from power analysis\npwr.t.test(d = 0.5, power = 0.8, sig.level = 0.05, type = \"two.sample\")\n\n\n4. Missing Values\n\nWhy Missing Data Occurs:\n\nData Collection Errors: Mistakes or malfunctions during data collection processes.\nNonresponse: Participants choose not to respond to certain questions.\nLoss of Data: Data may be lost due to technical issues or other unforeseen circumstances.\n\n\n\nTypes of Missing Data:\n\nMCAR (Missing Completely at Random): The probability of missingness is the same across all observations.\nMAR (Missing at Random): The probability of missingness is related to observed data but not the missing data itself.\nMNAR (Missing Not at Random): The probability of missingness is related to the missing data itself.\n\n\n\nImplications\n\nLoss of Information: Removing rows with missing data can lead to a significant reduction in the dataset size, potentially losing valuable information.\nBias: If the data is not missing completely at random, deleting rows with missing values can introduce bias into the analysis.\nReduced Statistical Power: Smaller sample sizes due to deletion of rows can lead to reduced power to detect effects.\n\n\n\nHow You Should Handle Missing Data\nListwise Deletion\nInvolves removing any row with missing data. It is simple but often not recommended unless data is MCAR.\ndata(\"airquality\")\n\nclean_data &lt;- na.omit(airquality)\nMean/Median Imputation\nReplaces missing values with the mean or median of the observed data. It is simple but can underestimate variability.\nairquality$Ozone[is.na(airquality$Ozone)] &lt;- mean(airquality$Ozone, na.rm = TRUE)\nMultiple Imputation\nCreates multiple datasets with imputed values and combines the results to account for the uncertainty of the missing data.\nlibrary(mice)\n\nimputed_data &lt;- mice(airquality, m = 5, method = 'pmm', seed = 500)\ncomplete_data &lt;- complete(imputed_data, 1)\nModel-Based Methods\nYou can use models to predict and fill in missing values. This will provide more accurate imputations by leveraging relationships between variables.\nlibrary(missForest)\n\n# Model-based imputation using random forests\nimputed_data_rf &lt;- missForest(airquality)\ncomplete_data_rf &lt;- imputed_data_rf$ximp\n\n\n\n5. Confounding Variables\nConfounding variables are extraneous variables that correlate with both the independent and dependent variables, potentially leading to biased results.\n\nBlocking: Group subjects into blocks based on confounding variables (e.g., sex, age). Must be discrete; otherwise, use ANCOVA for continuous variables.\nCovariate (CV): Adjust group means based on pretest or unexplained exogenous variables.\n\nlibrary(car)\n\n# Example: ANCOVA with a covariate\nmodel &lt;- lm(mpg ~ wt + factor(am), data = mtcars)\nAnova(model, type = \"III\")\n\n\n6. Fixed vs Random Effects\n\nFixed effects: Levels of independent variables (IVs) are selected by the researcher.\n\nex: Researcher selects specific treatment levels.\n\nRandom effects: Levels are randomly selected from the population to generalize findings.\n\nex: Randomly selects a sample of levels from the population.\n\n\nlibrary(lme4)\n\n# Mixed-effects model with fixed and random effects\nmodel &lt;- lmer(Reaction ~ Days + (1|Subject), data = sleepstudy)\nsummary(model)"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "My Resume",
    "section": "Education",
    "text": "Education\n\nMSc Environmental Psychology, University of Groningen\n09/2021 - 02/2023\n\nUtilized theories from social, cognitive, and developmental psychology to assess the interplay between humans and the built and natural world\nDesigned and tested interventions in real-world applications\nLearned advanced multilevel statistical techniques to analyze and understand complex social and behavioral systems\n\n\n\nBS Psychology and Statistics Minor, Virginia Polytech Institute and State University\n09/2017 - 06/2021"
  },
  {
    "objectID": "about.html#professional-experience",
    "href": "about.html#professional-experience",
    "title": "My Resume",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nUX Researcher at Hardt Hyperloop, Groningen, NL\n04/2022 - 11/2022\nResearch lab focused on studying the user perceptions of a simulated hyperloop train, aimed at promoting sustainable transportation usage.\n\nPsychometrics: Designed questionnaires, interviewed subjects, and created latent-class models for experimental measures\nRegression: Designed multivariate multiple regression models\nPresentations: Presented item structure and regression graphics to company members, and was interviewed by HiveMobility\n\n\n\nStudent Researcher at the Center for Applied Behavioral Systems, Blacksburg, VA\n01/2018 - 05/2021\nA center for intervention and community-based research techniques.\n\nTeaching: Training undergraduate students in responsible research methods\nData Management: Reviewing and preparing data for publication using the Virginia Polytech Data Repository\nPsychometrics: Designed and tested surveys using Qualtrics and R\nCollaborative Projects: Conducted research projects with two of the doctoral students in the lab, including writing thorough literature reviews and co-authoring two papers\n\n\n\n\nPrivate Tutor in Statistics, Self Employed, Groningen, NL\n09/2021 - Present\nSupported 16 bachelor students and 8 master’s students in exam preparation and refining research methodology.\n\nSubject Knowledge: Instructed on topics in bachelor and master level statistics\nTeaching: Simplified coursework material into understandable lessons"
  },
  {
    "objectID": "about.html#skills-and-expertise",
    "href": "about.html#skills-and-expertise",
    "title": "My Resume",
    "section": "Skills and Expertise",
    "text": "Skills and Expertise\n\nSoftware\nR, Atlas, SPSS, Microsoft Office\n\n\nStatistics\nSet Theory and Logic, Methods of Regression, Item Response Theory, Latent Class Analysis, Hierarchical Modelling, Econometrics, Bayesian Statistics, Time Series Analysis, Contingency Table Analysis\n\n\nPsychology\nCognitive, Social, Developmental, Clinical, Behavioral, Humanistic, Psychometrics\n\n\nEnvironmental Science\nGeoscience, Geology, Meteorology, Climate Change, Ecosystems, Sustainable Systems,"
  },
  {
    "objectID": "about.html#certifications",
    "href": "about.html#certifications",
    "title": "My Resume",
    "section": "Certifications",
    "text": "Certifications\n\nR Programming (Coursera Specialization)\nJohns Hopkins Bloomberg School of Public Health\n04/2023 - 03/2024\n\nBuilding Data Visualization Tools\nBuilding R Packages\nAdvanced R Programming\nThe R Programming Environment\n\n\n\nMathematical Biostatistics (Coursera Specialization)\nJohns Hopkins Bloomberg School of Public Health\n09/2023 - 11/2023\n\nProbability Mass and Density Functions\nBayesian Statistics\nBootstrapping Algorithms\nGeometric Means"
  },
  {
    "objectID": "about.html#professional-presentations-awards-and-conferences",
    "href": "about.html#professional-presentations-awards-and-conferences",
    "title": "My Resume",
    "section": "Professional Presentations, Awards, and Conferences",
    "text": "Professional Presentations, Awards, and Conferences\nProfessional Presentations\n\nCallahan, O., Eick, A., Takemoto-Jennings, R., & Geller, E. S. (2019, February). Gratitude from student to professor: Evaluating the impact of thanking a professor. Poster presented at the Annual North Carolina Association for Behavior Analysis, Winston-Salem, NC.\nCallahan, O., Eick, A., Takemoto-Jennings, R., & Geller, E. S. (2019, March). The impact of showing gratitude in the classroom: Evaluating students’ change in their mood-states. Poster presented at the Mid-Atlantic Undergraduate Research Conference, Blacksburg, VA.\n\nAwards\n\nDean’s List\nBeyond Boundaries Scholar\nSumma Cum Laude\n\nConferences\n\nUniC Summit. 07/2021 Quebec, Canada\nCOP25 United Nations Climate Change Conference. 12/2019 Madrid, Spain\nMid-Atlantic Undergraduate Research Conference. 04/2019 Blacksburg, VA, USA\nNorth Carolina Applied Behavioral Association. 02/2019 Winston-Salem, NC, USA"
  },
  {
    "objectID": "about.html#contact-information",
    "href": "about.html#contact-information",
    "title": "My Resume",
    "section": "Contact Information",
    "text": "Contact Information\nEmail: owen11callahan@gmail.com"
  },
  {
    "objectID": "multilevel.html",
    "href": "multilevel.html",
    "title": "Project Description",
    "section": "",
    "text": "Project Description\nThis is a research project I developed during my Master’s Degree in Environmental Psychology. It includes a hierarchical model that explores the cross-level dynamics within communities in the Netherlands.\nI am still motivated about executing this project again in a different setting because it addresses contemporary challenges in ethnically heterogeneous neighborhoods and provides an unique approach to understanding social cohesion. It assesses the individual-level, cognitive processes in neighborhood behavior through spatial measurement, which can offer alternative evidence for policymakers’ views toward contextual level action (e.g., developing economically vital cities, demolition, and housing renovation) (Van Kempen & Bolt, 2009). By bridging the gap between individual-level processes and neighborhood dynamics, this research has the potential to inform policy interventions and community development initiatives.\n\nOutcome Variable\n\nboundary: combined value of perceived neighborhood scale and perceived trust in neighbors\n\nDemographics:\n\nage: Normally distributed continuous variable.\neducation: Dichotomous variable\nincome: Ordinal variable from 1 to 5.\nethnicity: Categorical variable sampled from four categories “A”, “B”, “C”, and “D”.\nlength_res: Normally distributed continuous variable for length of residency.\n\nPhysical Disorder:\n\nphysical_disorder: Ordinal variable from 1 to 5.\n\nPerceived Collective Efficacy:\n\nperceived_collective_efficacy: Normally distributed continuous variable.\n\nIntergroup Leader:\n\nleadership: Dichotomous variable.\n\nPersonalized Features:\n\nnames, art, playgrounds: Dichotomous variables\n\n\n\n1. Load Packages and Data\n\n# Load libraries\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(geojsonio)\n\nRegistered S3 method overwritten by 'geojsonsf':\n  method        from   \n  print.geojson geojson\n\n\n\nAttaching package: 'geojsonio'\n\n\nThe following object is masked from 'package:base':\n\n    pretty\n\nlibrary(sp)\nlibrary(lme4)\n\nLoading required package: Matrix\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nlibrary(ggplot2)\n\n# Read the GeoJSON mapping file\nmap_data &lt;- geojson_read(\"map_data.geojson\", what = \"sp\")\n\nmap_sf &lt;- st_as_sf(map_data)\n\n# Calculate the area of each drawn boundary\nmap_sf &lt;- map_sf %&gt;%\n  mutate(area = st_area(geometry))\nmap_sf$area &lt;- as.numeric(map_sf$area)\n\n\n# Load the survey dataset\ndata &lt;- read.csv(\"multilevel_model_data.csv\")\n\n# Merge datasets\ndata$id &lt;- 1:nrow(data)\ndata &lt;- data %&gt;%\n  left_join(map_sf %&gt;% select(id, area), by = \"id\")\n\n\n\n2. Exploratory Data Analysis\nBefore I compare the models, let’s take a look at the full model and understand our data a bit better.\n\n# Fit the multilevel model\nmultilevel_model &lt;- lmer(boundary ~ age + education + income + ethnicity + length_res + physical_disorder + perceived_collective_efficacy + facilities + names + art + playgrounds + leadership + (1 | neighborhood_id), data = data)\nsummary(multilevel_model)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: boundary ~ age + education + income + ethnicity + length_res +  \n    physical_disorder + perceived_collective_efficacy + facilities +  \n    names + art + playgrounds + leadership + (1 | neighborhood_id)\n   Data: data\n\nREML criterion at convergence: 12877.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.9966 -0.7063  0.0071  0.6710  3.2348 \n\nRandom effects:\n Groups          Name        Variance Std.Dev.\n neighborhood_id (Intercept)  0.5319  0.7293  \n Residual                    91.6096  9.5713  \nNumber of obs: 1750, groups:  neighborhood_id, 25\n\nFixed effects:\n                               Estimate Std. Error t value\n(Intercept)                   49.019346   1.575249  31.118\nage                           -0.009148   0.015198  -0.602\neducation                     -0.211335   0.469636  -0.450\nincome                        -0.425076   0.164823  -2.579\nethnicityB                     0.541790   0.645876   0.839\nethnicityMe                    0.439613   0.651204   0.675\nethnicityW                     0.359890   0.650369   0.553\nlength_res                    -0.052653   0.076229  -0.691\nphysical_disorder              0.063064   0.162170   0.389\nperceived_collective_efficacy  0.366557   0.232243   1.578\nfacilities                     0.163935   0.155372   1.055\nnames                         -0.091896   0.600375  -0.153\nart                            0.821966   0.597757   1.375\nplaygrounds                   -0.077756   0.566788  -0.137\nleadership                     0.746840   0.585630   1.275\n\n\n\nCorrelation matrix not shown by default, as p = 15 &gt; 12.\nUse print(x, correlation=TRUE)  or\n    vcov(x)        if you need it\n\n# Normality of residuals\nplot(multilevel_model)\n\n\n\n\n\n\n\n# Homoscedasticity\nplot(residuals(multilevel_model) ~ fitted(multilevel_model))\n\n\n\n\n\n\n\n# Fixed effects estimates\nfixed_effects &lt;- fixef(multilevel_model)\nprint(fixed_effects)\n\n                  (Intercept)                           age \n                 49.019346235                  -0.009148289 \n                    education                        income \n                 -0.211335096                  -0.425075682 \n                   ethnicityB                   ethnicityMe \n                  0.541790280                   0.439613297 \n                   ethnicityW                    length_res \n                  0.359890186                  -0.052653388 \n            physical_disorder perceived_collective_efficacy \n                  0.063063774                   0.366557166 \n                   facilities                         names \n                  0.163934559                  -0.091895788 \n                          art                   playgrounds \n                  0.821965502                  -0.077756138 \n                   leadership \n                  0.746839797 \n\n\n\n\n3. Model Selection\n\n# Model without Low-estimate predictors:\nalt_model1 &lt;- lmer(boundary ~ education + income + ethnicity + perceived_collective_efficacy + facilities + art + leadership + (1 | neighborhood_id), data = data)\n\n# Individual-focused Model:\nalt_model2 &lt;- lmer(boundary ~ age + education + income + perceived_collective_efficacy + (1 | neighborhood_id), data = data)\n\n# Interaction Effects:\nalt_model3 &lt;- lmer(boundary ~ age * perceived_collective_efficacy + education + income + length_res + physical_disorder + facilities + names + art + playgrounds + leadership + (1 | neighborhood_id), data = data)\n\n# Random Slopes Model:\nalt_model4 &lt;- lmer(boundary ~ age + education + income + perceived_collective_efficacy + (age + education + income + perceived_collective_efficacy | neighborhood_id), data = data)\n\nboundary (singular) fit: see help('isSingular')\n\n# Cross-Level Interactions:\nalt_model5 &lt;- lmer(boundary ~ age + education + income + perceived_collective_efficacy + facilities + names + art + playgrounds + leadership + (age + education + income + perceived_collective_efficacy | neighborhood_id), data = data)\n\nboundary (singular) fit: see help('isSingular')\n\n# Non-linear Effects:\ndata$age_squared &lt;- data$age^2\nalt_model6 &lt;- lmer(boundary ~ age + age_squared + education + income + perceived_collective_efficacy + (1 | neighborhood_id), data = data)\n\nWarning: Some predictor variables are on very different scales: consider\nrescaling\n\n# Nested Models:\nalt_model7 &lt;- lmer(boundary ~ age + education + income + perceived_collective_efficacy + (1 | neighborhood_id) + (1 + age | neighborhood_id), data = data)\n\nboundary (singular) fit: see help('isSingular')\n\n# Compare models using AIC\nAIC_values &lt;- AIC(multilevel_model, alt_model1, alt_model2, alt_model3, alt_model4, alt_model5, alt_model6, alt_model7)\nBIC_values &lt;- BIC(multilevel_model, alt_model1, alt_model2, alt_model3, alt_model4, alt_model5, alt_model6, alt_model7)\nprint(AIC_values)\n\n                 df      AIC\nmultilevel_model 17 12911.40\nalt_model1       12 12892.09\nalt_model2        7 12895.86\nalt_model3       15 12916.94\nalt_model4       21 12919.90\nalt_model5       26 12924.87\nalt_model6        8 12907.87\nalt_model7       10 12901.39\n\nprint(BIC_values)\n\n                 df      BIC\nmultilevel_model 17 13004.35\nalt_model1       12 12957.70\nalt_model2        7 12934.13\nalt_model3       15 12998.95\nalt_model4       21 13034.71\nalt_model5       26 13067.02\nalt_model6        8 12951.61\nalt_model7       10 12956.07\n\n\nAfter removing variables (age, length_res, names, playgrounds, physical_disorder) , my first adjusted model has one of the lower AIC/BIC, while also aligning with my theoretical framework and hypotheses.\n\n\n4. Results\n\n# Plot Cooks D estimates with influence plot\ninfluencePlot(alt_model1, id.n = 5)\n\nWarning in plot.window(...): \"id.n\" is not a graphical parameter\n\n\nWarning in plot.xy(xy, type, ...): \"id.n\" is not a graphical parameter\n\n\nWarning in axis(side = side, at = at, labels = labels, ...): \"id.n\" is not a\ngraphical parameter\nWarning in axis(side = side, at = at, labels = labels, ...): \"id.n\" is not a\ngraphical parameter\n\n\nWarning in box(...): \"id.n\" is not a graphical parameter\n\n\nWarning in title(...): \"id.n\" is not a graphical parameter\n\n\nWarning in plot.xy(xy.coords(x, y), type = type, ...): \"id.n\" is not a\ngraphical parameter\n\n\n\n\n\n\n\n\n\n        StudRes         Hat        CookD\n325   3.2599741 0.008857092 0.0094969282\n832  -2.7199424 0.010389742 0.0077671191\n961   3.0225287 0.006943756 0.0063879495\n1411 -0.8990109 0.016199316 0.0013308205\n1606 -0.6344288 0.014523900 0.0005932024\n\n# Calculate variance inflation factors\nvif(alt_model1)\n\n                                  GVIF Df GVIF^(1/(2*Df))\neducation                     1.003845  1        1.001921\nincome                        1.002528  1        1.001263\nethnicity                     1.007411  3        1.001231\nperceived_collective_efficacy 1.004720  1        1.002357\nfacilities                    1.075506  1        1.037066\nart                           1.105281  1        1.051324\nleadership                    1.045332  1        1.022415\n\n# Extract coefficients\nmodel_coefs &lt;- coef(summary(alt_model1))\nmodel_coefs\n\n                                Estimate Std. Error    t value\n(Intercept)                   48.5442001  1.2528175 38.7480213\neducation                     -0.1962912  0.4685969 -0.4188913\nincome                        -0.4273233  0.1645159 -2.5974585\nethnicityB                     0.5186504  0.6449240  0.8042039\nethnicityMe                    0.4096027  0.6499668  0.6301902\nethnicityW                     0.3533965  0.6496055  0.5440171\nperceived_collective_efficacy  0.3572640  0.2314167  1.5438123\nfacilities                     0.1658361  0.1436879  1.1541409\nart                            0.8218169  0.5516168  1.4898330\nleadership                     0.7168449  0.5260312  1.3627422"
  },
  {
    "objectID": "hyperloop.html",
    "href": "hyperloop.html",
    "title": "Measuring Perceptions of the Hyperloop",
    "section": "",
    "text": "Project Description\n\nMy primary research objective at Hardt Hyperloop was to investigate user perceptions of a simulated hyperloop trip. The company aimed to determine the appropriate dimensions of the hyperloop capsule before investing extensively in materials. However, since the company had not yet developed a hyperloop accessible to the public, predicting how people would feel inside the capsule was challenging. To address this, our team designed a real-life hyperloop experience—a wooden mock-up capsule equipped with seating and accompanied by a VR headset to illustrate the remaining characteristics of the hyperloop interior.\nWe recruited residents in Groningen to participate in the VR simulation and complete questionnaires. Four main constructs—technology acceptance, perceived safety, perceived comfort, and claustrophobia—were identified for latent class analysis. This model will be instrumental in classifying users in future simulated hyperloop studies.\nThe dataframe named survey_data contains columns: tech_acceptance, perceived_safety, perceived_comfort, and claustrophobia. Unfortunately, due to privacy laws, I cannot share the data on this platform. I can only provide my code for analyzing the survey data collected at Hardt Hyperloop.\nThis analysis includes the code for building reliability tests, an LCA model, item structures, and a MMR model.\n\n1. Install Packages\nlibrary(poLCA)\nlibrary(ggplot2)\nlibrary(reshape2)\n\n\n2. Testing Reliability\nsurvey_data &lt;- read.csv(\"survey_data.csv\")\n\n# Compute Cronbach's alpha for each construct\nalpha(survey_data[, c(\"tech_acceptance_item1\", \"tech_acceptance_item2\", \"tech_acceptance_item3\", \"tech_acceptance_item4\", \"tech_acceptance_item5\", \"tech_acceptance_item6\", \"tech_acceptance_item7\", \"tech_acceptance_item8\", \"tech_acceptance_item9\", \"tech_acceptance_item10\")], check.keys=TRUE)\nalpha(survey_data[, c(\"perceived_safety_item1\", \"perceived_safety_item2\", \"perceived_safety_item3\", \"perceived_safety_item4\", \"perceived_safety_item5\")], check.keys=TRUE)\nalpha(survey_data[, c(\"perceived_comfort_item1\", \"perceived_comfort_item2\", \"perceived_comfort_item3\", \"perceived_comfort_item4\", \"perceived_comfort_item5\")], check.keys=TRUE)\nalpha(survey_data[, c(\"claustrophobia_item1\", \"claustrophobia_item2\", \"claustrophobia_item3\", \"claustrophobia_item4\")], check.keys=TRUE)\n\n\n3. Latent-Class Analysis\nmy_classes &lt;- cbind(tech_acceptance, perceived_safety, perceived_comfort, claustrophobia) ~ 1\n\nsurvey_data$class &lt;- my_classes$predclass\nhead(survey_data)\n\nModel Selection\n\naic_values &lt;- numeric()\nbic_values &lt;- numeric()\nmodels &lt;- list()\n\n# Fit LCA models with 1 to 5 classes\nfor (k in 1:4) {\n  lca_model &lt;- poLCA(f, data = survey_data, nclass = k, maxiter = 1000, verbose = FALSE)\n  aic_values[k] &lt;- lca_model$aic\n  bic_values[k] &lt;- lca_model$bic\n  models[[k]] &lt;- lca_model\n}\n\n# Combine AIC and BIC values into a new data frame\nmodel_comparison &lt;- data.frame(\n  Classes = 1:5,\n  AIC = aic_values,\n  BIC = bic_values\n)\n\n# Simply look for the lowest AIC and BIC\nprint(model_comparison)\n\n\nAIC and BIC Plots\nmodel_comparison_melt &lt;- melt(model_comparison, id.vars = \"Classes\", variable.name = \"Metric\", value.name = \"Value\")\n\nggplot(model_comparison_melt, aes(x = Classes, y = Value, color = Metric)) +\n  geom_line() +\n  geom_point() +\n  theme_minimal() +\n  labs(title = \"Model Comparison using AIC and BIC\",\n       x = \"Number of Classes\",\n       y = \"Value\",\n       color = \"Metric\")\n\n\n\n4. Plotting Item Structures\nggplot(survey_data, aes(x = factor(class))) +\n  geom_bar(fill = \"blue\", alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"Latent Class Memberships\",\n       x = \"Class\",\n       y = \"Count\")\n\n# Class-specific item probabilities\nitem_probs &lt;- lca_model$probs\n\n# Convert to a new data frame for plotting\nitem_probs_df &lt;- do.call(rbind, lapply(seq_along(item_probs), function(i) {\n  item &lt;- names(item_probs)[i]\n  probs &lt;- item_probs[[i]]\n  data.frame(Class = rep(seq_along(probs), each = nrow(probs)),\n             Item = item,\n             Category = rep(1:nrow(probs), times = length(probs)),\n             Probability = as.vector(probs))\n}))\n\n# Plot item profiles\nggplot(item_probs_df, aes(x = Category, y = Probability, fill = factor(Class))) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  facet_wrap(~Item, scales = \"free_x\") +\n  theme_minimal() +\n  labs(title = \"Item Profiles for Latent Classes\",\n       x = \"Category\",\n       y = \"Probability\",\n       fill = \"Class\")\n\n\n5. Multivariate Multiple Regression\n# Multivariate multiple regression model\nmul_model &lt;- lm(cbind(tech_acceptance, perceived_safety, perceived_comfort, claustrophobia) ~ age + gender, data = survey_data)\n\n# Calculate VIF\nvif(mul_model)\n\nPlotting\n# Extract coefficients\ncoefficients &lt;- summary(mmr_model)$coefficients\n\n# Reshape data\ncoef_df &lt;- data.frame(Variable = rownames(coefficients[[1]]), coefficients)\ncoef_df &lt;- melt(coef_df, id.vars = \"Variable\", variable.name = \"Outcome\", value.name = \"Coefficient\")\n\n# Plot coefficents\nggplot(coef_df, aes(x = Variable, y = Coefficient, fill = Outcome)) +\n  geom_bar(stat = \"identity\", position = position_dodge()) +\n  theme_minimal() +\n  labs(title = \"Multivariate Multiple Regression Coefficients\",\n       x = \"Predictors\",\n       y = \"Coefficient\",\n       fill = \"Outcome Variables\") +\n  coord_flip()\n\n# Plot Residuals vs Fitted values for each dependent variable\n\n# Tech Acceptance\nplot(mmr_model$fitted.values[, 1], mmr_model$residuals[, 1], main = \"Residuals vs Fitted (Tech Acceptance)\", xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\n\n# Perceived Safety\nplot(mmr_model$fitted.values[, 2], mmr_model$residuals[, 2], main = \"Residuals vs Fitted (Perceived Safety)\", xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\n\n# Perceived Comfort\nplot(mmr_model$fitted.values[, 3], mmr_model$residuals[, 3], main = \"Residuals vs Fitted (Perceived Comfort)\", xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\n\n# Claustrophobia\nplot(mmr_model$fitted.values[, 4], mmr_model$residuals[, 4], main = \"Residuals vs Fitted (Claustrophobia)\", xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\")"
  },
  {
    "objectID": "interest.html",
    "href": "interest.html",
    "title": "My Portfolio",
    "section": "",
    "text": "Community-Based Psychological Research\nModel Development\nTeaching Statistics in the Social Sciences\nSports Psychology and Coaching\n\n\n\nHumans are evolutionarely grounded for community-based lifestyles. Social intimacy has shown to improve mental and physical health. Yet, individualized living has sent us toward isolated living.\nIncreasing diverity has played a role in recent trends of isolation.\nPlacing more funding into personalizing neighborhoods that would build a new group identity for which people can embody.\n\n\n\n\nInspiring teachers are like summer camp counselors. Realistically, they might seem worlds apart, but they share a crucial trait: they are both excellent storytellers. Inspiring teachers unravel complex topics through stories, enabling their students to form emotional connections with the lessons. They focus on more than just imparting knowledge; they empower students to apply imaginative thinking in the classroom. Inspiring teachers build a nurturing space where students feel encouraged to explore and question.\nThe storytelling approach has been a cornerstone of my teaching philosophy, helping me inspire many of my peers in college. After developing this skill as a summer camp counselor for seven years, I continued to weave stories into my lessons as a statistics tutor. For instance, I would take a subject like hypothesis testing and create vivid scenarios, expressing the researchers’ thoughts and dilemmas with their datasets.\nThis approach might seem unconventional, but it has proven highly effective. By using characters and real-life examples, I helped my colleagues transform the often dry principles of statistics into vivid, engaging narratives. It added color to the traditional, monotonous format of lecturing about functions and distributions. This method particularly resonated with literature-driven students in the social sciences, allowing them to forge deeper connections with statistical concepts.\nMy experiences have led me to believe that storytelling is not just a tool but a bridge that connects abstract concepts with tangible understanding. I am eager to bring stories that elucidate key statistical concepts and to inspire every student I meet with an enthusiastic drive. By doing so, I hope to create a lasting impact on their academic and professional journeys."
  },
  {
    "objectID": "interest.html#my-academic-and-personal-interests",
    "href": "interest.html#my-academic-and-personal-interests",
    "title": "My Portfolio",
    "section": "",
    "text": "Community-Based Psychological Research\nModel Development\nTeaching Statistics in the Social Sciences\nSports Psychology and Coaching\n\n\n\nHumans are evolutionarely grounded for community-based lifestyles. Social intimacy has shown to improve mental and physical health. Yet, individualized living has sent us toward isolated living.\nIncreasing diverity has played a role in recent trends of isolation.\nPlacing more funding into personalizing neighborhoods that would build a new group identity for which people can embody.\n\n\n\n\nInspiring teachers are like summer camp counselors. Realistically, they might seem worlds apart, but they share a crucial trait: they are both excellent storytellers. Inspiring teachers unravel complex topics through stories, enabling their students to form emotional connections with the lessons. They focus on more than just imparting knowledge; they empower students to apply imaginative thinking in the classroom. Inspiring teachers build a nurturing space where students feel encouraged to explore and question.\nThe storytelling approach has been a cornerstone of my teaching philosophy, helping me inspire many of my peers in college. After developing this skill as a summer camp counselor for seven years, I continued to weave stories into my lessons as a statistics tutor. For instance, I would take a subject like hypothesis testing and create vivid scenarios, expressing the researchers’ thoughts and dilemmas with their datasets.\nThis approach might seem unconventional, but it has proven highly effective. By using characters and real-life examples, I helped my colleagues transform the often dry principles of statistics into vivid, engaging narratives. It added color to the traditional, monotonous format of lecturing about functions and distributions. This method particularly resonated with literature-driven students in the social sciences, allowing them to forge deeper connections with statistical concepts.\nMy experiences have led me to believe that storytelling is not just a tool but a bridge that connects abstract concepts with tangible understanding. I am eager to bring stories that elucidate key statistical concepts and to inspire every student I meet with an enthusiastic drive. By doing so, I hope to create a lasting impact on their academic and professional journeys."
  },
  {
    "objectID": "bayes.html",
    "href": "bayes.html",
    "title": "Project Description",
    "section": "",
    "text": "Project Description\nConsidering my love for exercise and health, I found a dataset on physical activity and heart rate to use for a Bayesian analysis. I am interested in exploring the relationship between exercise and heart rate, which calls for a linear regression approach. To perform a Bayesian linear regression, I will use the brms package in R. This package provides an interface to fit complex Bayesian models using the probabilistic programming language Stan. Stan’s efficient Hamiltonian Monte Carlo algorithms can sample from the posterior distribution of the model parameters.\nI could not have learned some of these steps by myself. An amazing resource to check out is bayesrulesbook.com; it will teach you so much about the Bayesian philosophy and techniques. If you curious about where I learned some of the steps below, check out Chapter 6!\n\n1. Load Packages and Data\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\nlibrary(brms)\n\nLoading required package: Rcpp\n\n\nLoading 'brms' package (version 2.21.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\n\nAttaching package: 'brms'\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\nlibrary(bayesplot)\n\nThis is bayesplot version 1.11.1\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\n\n\nAttaching package: 'bayesplot'\n\n\nThe following object is masked from 'package:brms':\n\n    rhat\n\n\n\ndata&lt;-read.csv(\"exercise_data.csv\")\nhead(data)\n\n  physical_activity heart_rate age   weight gender\n1                55   66.32434  22 66.45458   Male\n2                69   69.98812  28 69.26444 Female\n3                46   82.85591  64 58.31349   Male\n4                61   53.97753  37 63.65252 Female\n5                73   74.57795  19 69.71158   Male\n6                63   70.95191  79 76.70696 Female\n\n\n\n\n2. Create the Bayesian Linear Regression\n\n# We can model heart rate as a function of physical activity, age, weight, and gender\nmodel &lt;- brm(\n  heart_rate ~ physical_activity + age + weight + gender,\n  data = data,\n  family = gaussian(),\n  prior = c(set_prior(\"normal(0, 10)\", class = \"b\")) # set a normal prior of M = 0 and SD = 10\n)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 6.3e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.63 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.045 seconds (Warm-up)\nChain 1:                0.024 seconds (Sampling)\nChain 1:                0.069 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.042 seconds (Warm-up)\nChain 2:                0.027 seconds (Sampling)\nChain 2:                0.069 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 8e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.049 seconds (Warm-up)\nChain 3:                0.029 seconds (Sampling)\nChain 3:                0.078 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 7e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.047 seconds (Warm-up)\nChain 4:                0.025 seconds (Sampling)\nChain 4:                0.072 seconds (Total)\nChain 4: \n\n# Print a summary of the fitted model, we can assess our estimates and our credible intervals\nsummary(model)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: heart_rate ~ physical_activity + age + weight + gender \n   Data: data (Number of observations: 145) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                  Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept            75.58      9.77    56.75    94.34 1.00     6425     3275\nphysical_activity    -0.06      0.12    -0.29     0.17 1.00     5619     3328\nage                   0.05      0.05    -0.04     0.14 1.00     5681     2843\nweight                0.01      0.10    -0.18     0.20 1.00     6231     3241\ngenderMale           -0.83      1.72    -4.14     2.51 1.00     5424     3195\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma    10.68      0.63     9.52    12.03 1.00     4866     2718\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nImportant Note: Credible intervals crossing through 0 in Bayesian analysis indicate uncertainty or lack of evidence for a nonzero effect. Also, a replication of this study may reveal different conclusions.\n\n\n3. Data Visualization\n\nTrace Plots: Displays the sampled values of the parameters over iterations of the Markov Chain Monte Carlo sampling.\n\nInterpretation: Look for a fuzzy caterpillar where the chains mix well and cover the parameter space evenly. This suggests good mixing and convergence.\n\n\n\n# Trace Plots\ntrace_plot &lt;- mcmc_trace(model, size = .1)\nprint(trace_plot)\n\n\n\n\n\n\n\n\n\nDensity Plots: Shows the posterior distributions of the parameters.\n\nInterpretation: These should be smooth and unimodal. Multiple peaks might indicate issues with convergence or that the model has not properly identified the posterior distribution.\n\n\n\n# Density Plots (Posterior distributions)\nmcmc_dens(model) + \n  yaxis_text(TRUE)\n\n\n\n\n\n\n\n\n\nDensity Overlay Plot: Compares the density of the observed data to the density of the data generated by the model.\n\nInterpretation: The observed data (solid line) should ideally lie within the range of the simulated data (light blue lines). If the observed data fall outside this range, it may indicate that the model does not fit the data well.\n\n\n\n# Posterior Predictive Checks\npp_check_plot &lt;- pp_check(model, ndraws = 100)\nprint(pp_check_plot)"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Welcome to My Website",
    "section": "About Me",
    "text": "About Me\nHi there! I’m Owen, a passionate data enthusiast, R programmer, and lifelong learner. Here you’ll find information about my background, skills, and education. Whether you’re a potential employer, colleague, or collaborator, I’m excited to share my journey with you."
  },
  {
    "objectID": "index.html#dive-into-my-r-projects",
    "href": "index.html#dive-into-my-r-projects",
    "title": "Welcome to My Website",
    "section": "Dive into My R Projects",
    "text": "Dive into My R Projects\nExplore my portfolio of R projects, where I showcase my data analysis, visualization, and programming skills."
  },
  {
    "objectID": "index.html#learn-r",
    "href": "index.html#learn-r",
    "title": "Welcome to My Website",
    "section": "Learn R",
    "text": "Learn R\nIf you’re part of my tutoring program or just new to R, you’ve come to the right place. Here, I offer beginner-friendly tutorials and resources to help you master the basics of R programming."
  },
  {
    "objectID": "index.html#connect-with-me",
    "href": "index.html#connect-with-me",
    "title": "Welcome to My Website",
    "section": "Connect with Me",
    "text": "Connect with Me\nInterested in collaborating on a project, discussing data science topics, or just saying hello? Don’t hesitate to reach out! Connect with me via email at owen11callahan@gmail.com"
  },
  {
    "objectID": "begin.html",
    "href": "begin.html",
    "title": "Fear Not!",
    "section": "",
    "text": "If you’re new to RStudio, you’ve come to the right place. Beyond my advanced projects, I’ve created this beginner-friendly page to share some basic computations. Whether you’re an employer or a colleague visiting, I’m delighted to have your interest.\nWe all have our first “sense of horror” moment when we first encounter RStudio. For me, it was during my first year in a biostatistics class with my teacher, Tessa. She had us working on projects using RStudio during class, and those initial weeks were chaotic. We were all frustrated and terrified by what appeared on our screens—scary red errors seemed to pop up every second, and we felt like we were making no progress at all. However, Tessa remained by our side. She dedicated the beginning of every class to addressing the typical errors we encountered, guiding us toward smart tactics and helpful online resources.\nBy the end of the semester, our perception of RStudio had completely changed. Tessa helped us realize that it wasn’t some dark abyss from which no student returns; it was a powerful tool that made statistics easier to compute. We bid farewell to the daunting equations we were accustomed to and began computing statistical techniques at a rapid pace. To this day, every one of my classmates continues to use RStudio with excitement. This software is truly your best friend if you use it wisely.\n\n\n\nAsking for Help\nPackages\nReading Data\nTidying Data\nFunctions\nData Visualization\n\n\n\n\nMost experts mention asking for help at the end of their tutorials, but I believe it deserves to be first. Learning R involves seeking a lot of assistance, and there are countless online resources available to answer your questions. Below are some of my favorite resources:\n\nThe Help Center in RStudio: Located in the bottom right pane of your window, the Help Center provides detailed explanations of each command in R. You can learn about the purpose of each function, as well as their sub-commands and properties. You can also type any command with the ? symbol in front of it and it will direct you to the same window (e.g., ?plot()).\nThe R Project Website: Provides comprehensive documentation and manuals.\nRStudio Community: An active forum where you can ask questions.\nR-Bloggers: A resource for tutorials, tips, and tricks from the R community.\nYouTube: Useful tutorials for visual learners.\nReddit: A community where everyone deals with problems using R. Visit Reddit to view a wide range of questions and answers from people around the globe.\nGenerative AI: An easy tool that can pick out the flaw in your code and guide you toward a solution. However, I recommend you use with caution, as AI can occasionally produce unreliable solutions to your code.\n\nBy leveraging these resources, you can make your learning journey with R more efficient and less daunting.\n\n\n\nR packages are collections of functions, data, and documentation that extend the capabilities of base R, making it easier and more efficient to perform a wide range of tasks.\n\nSimplify Complex Tasks\nImprove Data Analysis\nImprove Data Visualization\nStreamline Workflow\n\nHere are some of the most common or important packages in R used in the social sciences:\n\nggplot2: Customizable plots\ndplyr: Data manipulation\ntidyr: Cleaning data\nstringr: String manipulation\nlubridate: Dates and times\npsych: Psychometrics and psychological research\nlme4: Linear and generalized linear mixed-effects models\nafex: Factorial analysis\nbayesplotorbrms: Bayesian statistics\n\nThe tidyverse is a collection of R packages designed for data science. The core packages included in tidyverse are ggplot2, tidyr, readr, dplyr, andstringr. You can also download these packages separately.\n\n\n\n\n\nYou can find your data either using the file.choose() function or by clicking on Files in the bottom right window. Make sure to save your new data in your local Environment in the top right window.\n\n\n\nThe most common format for data files is CSV (Comma-Separated Values). The readr package from the tidyverse provides functions to read CSV files efficiently.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Read CSV file\ndata &lt;- read.csv(\"exercise_data.csv\")\n\n\n\n\n\nlibrary(readxl)\n\n# Read the first sheet\ndata &lt;- read_excel(\"/Users/owencallahan/Downloads/Fitness.xlsx\")\n\n# Read a specific sheet by name\ndata_sheet &lt;- read_excel(\"/Users/owencallahan/Downloads/Fitness.xlsx\", sheet = \"Sheet1\")\n\n# Read a specific sheet by index\ndata_index &lt;- read_excel(\"/Users/owencallahan/Downloads/Fitness.xlsx\", sheet = 1)\n\n\n\n\n\ndata &lt;- c(1,2,3,4)\nwrite.csv(data, \"my_data.csv\")\n\n\n\n\n\nTidying data is an essential part of data analysis in R, making your data easier to work with and analyze. We’ll use the tidyverse suite of packages, which includes dplyr, tidyr, and readr among others. Below is a step-by-step tutorial with code examples to demonstrate how to tidy data in R.\n\n\nPivoting longer:\n\nwide_data &lt;- tibble(\n  id = 1:3,\n  age = c(25, 30, 35),\n  height = c(175, 180, 165),\n  weight = c(70, 80, 65)\n)\n\n# Convert to long format\nlong_data &lt;- wide_data %&gt;%\n  pivot_longer(cols = c(age, height, weight), names_to = \"measure\", values_to = \"value\")\n\nprint(long_data)\n\n# A tibble: 9 × 3\n     id measure value\n  &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1 age        25\n2     1 height    175\n3     1 weight     70\n4     2 age        30\n5     2 height    180\n6     2 weight     80\n7     3 age        35\n8     3 height    165\n9     3 weight     65\n\n\nPivoting wider:\n\n# Convert back to wide format\nwide_again &lt;- long_data %&gt;%\n  pivot_wider(names_from = measure, values_from = value)\n\nprint(wide_again)\n\n# A tibble: 3 × 4\n     id   age height weight\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1    25    175     70\n2     2    30    180     80\n3     3    35    165     65\n\n\nSeparate columns:\n\ndata &lt;- tibble(\n  name = c(\"John_Doe\", \"Jane_Smith\", \"Alice_Johnson\")\n)\n\n# Separate into first and last names\nseparated_data &lt;- data %&gt;%\n  separate(name, into = c(\"first_name\", \"last_name\"), sep = \"_\")\n\nprint(separated_data)\n\n# A tibble: 3 × 2\n  first_name last_name\n  &lt;chr&gt;      &lt;chr&gt;    \n1 John       Doe      \n2 Jane       Smith    \n3 Alice      Johnson  \n\n\nUnite columns:\n\nunited_data &lt;- separated_data %&gt;%\n  unite(\"full_name\", first_name, last_name, sep = \" \")\n\n\n\n\nSelecting Columns\n\nselected_data &lt;- mtcars %&gt;%\n  select(cyl, gear)\n\nhead(selected_data)\n\n                  cyl gear\nMazda RX4           6    4\nMazda RX4 Wag       6    4\nDatsun 710          4    4\nHornet 4 Drive      6    3\nHornet Sportabout   8    3\nValiant             6    3\n\n\nFiltering Rows\n\nfiltered_data &lt;- mtcars %&gt;%\n  filter(gear &lt; 4)\n\nhead(filtered_data)\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 450SE        16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL        17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n\n\nMutating Columns\n\nmutated_data &lt;- mtcars %&gt;%\n  mutate(speed.scale = qsec/wt)\n\nhead(mutated_data %&gt;%\n        select(speed.scale))\n\n                  speed.scale\nMazda RX4            6.282443\nMazda RX4 Wag        5.920000\nDatsun 710           8.021552\nHornet 4 Drive       6.046656\nHornet Sportabout    4.947674\nValiant              5.843931\n\n\nSummarizing Data\n\nsummary_data &lt;- mtcars %&gt;%\n  group_by(gear) %&gt;%\n  summarize(\n    average_wt = mean(wt),\n    count = n()\n  )\n\nprint(summary_data)\n\n# A tibble: 3 × 3\n   gear average_wt count\n  &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1     3       3.89    15\n2     4       2.62    12\n3     5       2.63     5\n\n\n\n\n\n\n# Identify missing values\nmissing_data &lt;- mtcars %&gt;%\n  filter(is.na(wt))\n\n# Remove rows with missing values\ncleaned_data &lt;- data %&gt;%\n  drop_na()\n\n# Replace missing values with a specific value\nfilled_data &lt;- data %&gt;%\n  replace_na(list(height = 170, weight = 70))\n\n\n\n\n\ndata1 &lt;- tibble(\n  id = 1:3,\n  name = c(\"John\", \"Jane\", \"Alice\")\n)\n\ndata2 &lt;- tibble(\n  id = 1:3,\n  score = c(85, 90, 78)\n)\n\n# Inner join\njoined_data &lt;- data1 %&gt;%\n  inner_join(data2, by = \"id\")\n\nprint(joined_data)\n\n# A tibble: 3 × 3\n     id name  score\n  &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 John     85\n2     2 Jane     90\n3     3 Alice    78\n\n\n\n\n\n\nWhen I learned how to create my own functions, I felt like the creative side of R expanded beyond my expectations. I could tailor a command to address exactly what I needed to perform on a given set of data.\n\n\nThe function() command is a fundamental building block in R, enabling users to create their own functions. This is crucial for both simplifying repetitive tasks and organizing code in a clean, efficient manner. Here’s what the function() command can do for new learners using R:\n\nEncapsulate Repetitive Tasks\n\nCreating functions allows you to encapsulate repetitive code into reusable blocks. This reduces redundancy and makes your scripts more concise and readable. For example, if you frequently perform the same data transformation, you can write a function for it and call it whenever needed.\n\nOrganize Code\n\nFunctions help in organizing code logically. By breaking down complex procedures into smaller, manageable functions, you make your code more modular and easier to debug and maintain.\n\nImprove Readability\n\nUsing functions can significantly enhance the readability of your code. Descriptive function names and clear parameter definitions help others (and your future self) understand the purpose and usage of the code more quickly.\n\nParameterization\n\nFunctions allow you to use parameters to make your code more flexible. Instead of hard-coding values, you can pass different arguments to your functions, making them adaptable to various inputs and scenarios.\n\nEnhance Reproducibility\n\nFunctions contribute to reproducibility in your analyses. By encapsulating specific tasks, you ensure that the same operations can be repeated with different data or settings, leading to consistent results.\n\nPromote Code Reuse\n\nOnce you write a function, you can reuse it across different projects. This saves time and effort, as you don’t need to rewrite the same code for similar tasks.\n\n\nExample of Using function() in R\n\n# Define a function to calculate the mean of a numeric vector\ncalculate_mean &lt;- function(numbers) {\n  mean_value &lt;- mean(numbers)\n  return(mean_value)\n}\n\n# Use the function with a numeric vector\nsample_data &lt;- c(4, 8, 15, 16, 23, 42)\naverage &lt;- calculate_mean(sample_data)\nprint(average)\n\n[1] 18\n\n\n\n\n\nCorrelation measures the strength and direction of the relationship between two variables.\n\n# Correlation matrix\ncor_matrix &lt;- cor(mtcars)\nprint(cor_matrix)\n\n            mpg        cyl       disp         hp        drat         wt\nmpg   1.0000000 -0.8521620 -0.8475514 -0.7761684  0.68117191 -0.8676594\ncyl  -0.8521620  1.0000000  0.9020329  0.8324475 -0.69993811  0.7824958\ndisp -0.8475514  0.9020329  1.0000000  0.7909486 -0.71021393  0.8879799\nhp   -0.7761684  0.8324475  0.7909486  1.0000000 -0.44875912  0.6587479\ndrat  0.6811719 -0.6999381 -0.7102139 -0.4487591  1.00000000 -0.7124406\nwt   -0.8676594  0.7824958  0.8879799  0.6587479 -0.71244065  1.0000000\nqsec  0.4186840 -0.5912421 -0.4336979 -0.7082234  0.09120476 -0.1747159\nvs    0.6640389 -0.8108118 -0.7104159 -0.7230967  0.44027846 -0.5549157\nam    0.5998324 -0.5226070 -0.5912270 -0.2432043  0.71271113 -0.6924953\ngear  0.4802848 -0.4926866 -0.5555692 -0.1257043  0.69961013 -0.5832870\ncarb -0.5509251  0.5269883  0.3949769  0.7498125 -0.09078980  0.4276059\n            qsec         vs          am       gear        carb\nmpg   0.41868403  0.6640389  0.59983243  0.4802848 -0.55092507\ncyl  -0.59124207 -0.8108118 -0.52260705 -0.4926866  0.52698829\ndisp -0.43369788 -0.7104159 -0.59122704 -0.5555692  0.39497686\nhp   -0.70822339 -0.7230967 -0.24320426 -0.1257043  0.74981247\ndrat  0.09120476  0.4402785  0.71271113  0.6996101 -0.09078980\nwt   -0.17471588 -0.5549157 -0.69249526 -0.5832870  0.42760594\nqsec  1.00000000  0.7445354 -0.22986086 -0.2126822 -0.65624923\nvs    0.74453544  1.0000000  0.16834512  0.2060233 -0.56960714\nam   -0.22986086  0.1683451  1.00000000  0.7940588  0.05753435\ngear -0.21268223  0.2060233  0.79405876  1.0000000  0.27407284\ncarb -0.65624923 -0.5696071  0.05753435  0.2740728  1.00000000\n\n# Correlation between two variables\ncor(mtcars$mpg, mtcars$hp)\n\n[1] -0.7761684\n\n\n\n\n\nLinear regression models the relationship between a dependent variable and one or more independent variables.\n\n# Simple linear regression\nmodel &lt;- lm(mpg ~ hp, data = mtcars)\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7121 -2.1122 -0.8854  1.5819  8.2360 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 30.09886    1.63392  18.421  &lt; 2e-16 ***\nhp          -0.06823    0.01012  -6.742 1.79e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 30 degrees of freedom\nMultiple R-squared:  0.6024,    Adjusted R-squared:  0.5892 \nF-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n\n# Multiple linear regression\nmodel2 &lt;- lm(mpg ~ hp + wt, data = mtcars)\nsummary(model2)\n\n\nCall:\nlm(formula = mpg ~ hp + wt, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\n\n\n\n\nANOVA tests the difference in means among groups.\n\n# One-way ANOVA\nanova_model &lt;- aov(mpg ~ as.factor(cyl), data = mtcars)\nsummary(anova_model)\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nas.factor(cyl)  2  824.8   412.4    39.7 4.98e-09 ***\nResiduals      29  301.3    10.4                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Two-way ANOVA\nanova_model2 &lt;- aov(mpg ~ as.factor(cyl) + as.factor(gear), data = mtcars)\nsummary(anova_model2)\n\n                Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nas.factor(cyl)   2  824.8   412.4   38.00 1.41e-08 ***\nas.factor(gear)  2    8.3     4.1    0.38    0.687    \nResiduals       27  293.0    10.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nT-tests compare the means of two groups.\n\n# One-sample t-test\nt.test(mtcars$mpg, mu = 20)\n\n\n    One Sample t-test\n\ndata:  mtcars$mpg\nt = 0.08506, df = 31, p-value = 0.9328\nalternative hypothesis: true mean is not equal to 20\n95 percent confidence interval:\n 17.91768 22.26357\nsample estimates:\nmean of x \n 20.09062 \n\n# Two-sample t-test\nt.test(mpg ~ as.factor(am), data = mtcars)\n\n\n    Welch Two Sample t-test\n\ndata:  mpg by as.factor(am)\nt = -3.7671, df = 18.332, p-value = 0.001374\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -11.280194  -3.209684\nsample estimates:\nmean in group 0 mean in group 1 \n       17.14737        24.39231 \n\n\n\n\n\nChi-square tests are used for categorical data to test relationships between variables.\n\n# Create a contingency table\ntable_data &lt;- table(mtcars$cyl, mtcars$gear)\n\n# Chi-square test\nchisq.test(table_data)\n\nWarning in chisq.test(table_data): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  table_data\nX-squared = 18.036, df = 4, p-value = 0.001214\n\n\n\n\n\nPCA reduces the dimensionality of the data while preserving as much variance as possible. An excellent tutorial on PCA can be found by clicking this link: https://www.youtube.com/watch?\n\n# Perform PCA\npca_result &lt;- prcomp(mtcars, scale. = TRUE)\n\n# Summary of PCA\nsummary(pca_result)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.5707 1.6280 0.79196 0.51923 0.47271 0.46000 0.3678\nProportion of Variance 0.6008 0.2409 0.05702 0.02451 0.02031 0.01924 0.0123\nCumulative Proportion  0.6008 0.8417 0.89873 0.92324 0.94356 0.96279 0.9751\n                           PC8    PC9    PC10   PC11\nStandard deviation     0.35057 0.2776 0.22811 0.1485\nProportion of Variance 0.01117 0.0070 0.00473 0.0020\nCumulative Proportion  0.98626 0.9933 0.99800 1.0000\n\n\n\n\n\nK-means clustering groups data into k clusters.\n\n# Perform K-means clustering on the mtcars dataset\nset.seed(123)\nkmeans_result &lt;- kmeans(mtcars[, c(\"mpg\", \"hp\")], centers = 3)\n\n# Add cluster results to the original mtcars data\nmtcars$cluster &lt;- as.factor(kmeans_result$cluster)\n\n# Print the first few rows to check the results\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb cluster\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4       3\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4       3\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1       3\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1       3\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2       2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1       3\n\n\n\n\n\nLogistic regression models the probability of a binary outcome.\n\n# Logistic regression\nlogit_model &lt;- glm(am ~ hp + wt, data = mtcars, family = binomial)\nsummary(logit_model)\n\n\nCall:\nglm(formula = am ~ hp + wt, family = binomial, data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) 18.86630    7.44356   2.535  0.01126 * \nhp           0.03626    0.01773   2.044  0.04091 * \nwt          -8.08348    3.06868  -2.634  0.00843 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 10.059  on 29  degrees of freedom\nAIC: 16.059\n\nNumber of Fisher Scoring iterations: 8\n\n\n\n\n\n\n# Decomposition\n\n# Sample time series data\nts_data &lt;- ts(AirPassengers, frequency = 12)\n\n# Decompose the time series\ndecomposed &lt;- decompose(ts_data)\nplot(decomposed)\n\n\n\n\n\n\n\n# Forecasting\n\n# Install and load the forecast package\nlibrary(forecast)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n# Fit an ARIMA model\nfit &lt;- auto.arima(ts_data)\nforecasted &lt;- forecast(fit, h = 12)\nplot(forecasted)\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s explore one of my favorite, and also one of the most essential packages in R, ggplot2.\n\n\nFirst, you need to install and load the ggplot2 package:\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\ninstall.packages(\"ggplot2\")\n\n\nThe downloaded binary packages are in\n    /var/folders/k3/v39j6g_x4bv7mv_xq03986d00000gn/T//RtmpQAa8X7/downloaded_packages\n\nlibrary(ggplot2)\n\n\n\n\nggplot2 follows the grammar of graphics, which means you build plots layer by layer. The essential components are:\n\nData: The dataset you’re plotting.\nAesthetics (aes): The mapping of variables to visual properties like x and y coordinates, colors, sizes, etc.\nGeometries (geom): The type of plot you want to create (e.g., points, lines, bars).\nFacets: Subplots based on the values of one or more variables.\nScales: Control how data values are mapped to visual properties.\nCoordinate Systems: Control the coordinate space.\nThemes: Control the appearance of the plot.\n\n\n\n\nScatter Plot\n\n# Load example data\ndata(mtcars)\n\n# Create a scatter plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\n\n\n\n\nLine Plot\n\n# Create a line plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_line() +\n  labs(title = \"Line Plot of MPG vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\n\n\n\n\nBar Plot\n\n# Create a bar plot\nggplot(data = mtcars, aes(x = factor(cyl))) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Cylinder Counts\",\n       x = \"Number of Cylinders\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\nHistogram\n\n# Create a histogram\nggplot(data = mtcars, aes(x = mpg)) +\n  geom_histogram(binwidth = 2) +\n  labs(title = \"Histogram of MPG\",\n       x = \"Miles Per Gallon (MPG)\",\n       y = \"Frequency\")\n\n\n\n\n\n\n\n\nBox Plot\n\n# Create a box plot\nggplot(data = mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot() +\n  labs(title = \"Box Plot of MPG by Cylinder Count\",\n       x = \"Number of Cylinders\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\n\n\n\n\n\n\n\nAdding Colors\n\n# Scatter plot with color\nggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight by Cylinder Count\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\",\n       color = \"Cylinders\")\n\n\n\n\n\n\n\n\nAdding Size\n\n# Scatter plot with color and size\nggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl), size = hp)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight by Cylinder Count\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\",\n       color = \"Cylinders\",\n       size = \"Horsepower\")\n\n\n\n\n\n\n\n\nFaceting\n\n# Faceted scatter plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  facet_wrap(~ cyl) +\n  labs(title = \"Scatter Plot of MPG vs Weight by Cylinder Count\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\n\n\n\n\nThemes\n\n# Scatter plot with theme\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\") +\n  theme_minimal()"
  },
  {
    "objectID": "begin.html#learning-r",
    "href": "begin.html#learning-r",
    "title": "Fear Not!",
    "section": "",
    "text": "Asking for Help\nPackages\nReading Data\nTidying Data\nFunctions\nData Visualization\n\n\n\n\nMost experts mention asking for help at the end of their tutorials, but I believe it deserves to be first. Learning R involves seeking a lot of assistance, and there are countless online resources available to answer your questions. Below are some of my favorite resources:\n\nThe Help Center in RStudio: Located in the bottom right pane of your window, the Help Center provides detailed explanations of each command in R. You can learn about the purpose of each function, as well as their sub-commands and properties. You can also type any command with the ? symbol in front of it and it will direct you to the same window (e.g., ?plot()).\nThe R Project Website: Provides comprehensive documentation and manuals.\nRStudio Community: An active forum where you can ask questions.\nR-Bloggers: A resource for tutorials, tips, and tricks from the R community.\nYouTube: Useful tutorials for visual learners.\nReddit: A community where everyone deals with problems using R. Visit Reddit to view a wide range of questions and answers from people around the globe.\nGenerative AI: An easy tool that can pick out the flaw in your code and guide you toward a solution. However, I recommend you use with caution, as AI can occasionally produce unreliable solutions to your code.\n\nBy leveraging these resources, you can make your learning journey with R more efficient and less daunting.\n\n\n\nR packages are collections of functions, data, and documentation that extend the capabilities of base R, making it easier and more efficient to perform a wide range of tasks.\n\nSimplify Complex Tasks\nImprove Data Analysis\nImprove Data Visualization\nStreamline Workflow\n\nHere are some of the most common or important packages in R used in the social sciences:\n\nggplot2: Customizable plots\ndplyr: Data manipulation\ntidyr: Cleaning data\nstringr: String manipulation\nlubridate: Dates and times\npsych: Psychometrics and psychological research\nlme4: Linear and generalized linear mixed-effects models\nafex: Factorial analysis\nbayesplotorbrms: Bayesian statistics\n\nThe tidyverse is a collection of R packages designed for data science. The core packages included in tidyverse are ggplot2, tidyr, readr, dplyr, andstringr. You can also download these packages separately.\n\n\n\n\n\nYou can find your data either using the file.choose() function or by clicking on Files in the bottom right window. Make sure to save your new data in your local Environment in the top right window.\n\n\n\nThe most common format for data files is CSV (Comma-Separated Values). The readr package from the tidyverse provides functions to read CSV files efficiently.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Read CSV file\ndata &lt;- read.csv(\"exercise_data.csv\")\n\n\n\n\n\nlibrary(readxl)\n\n# Read the first sheet\ndata &lt;- read_excel(\"/Users/owencallahan/Downloads/Fitness.xlsx\")\n\n# Read a specific sheet by name\ndata_sheet &lt;- read_excel(\"/Users/owencallahan/Downloads/Fitness.xlsx\", sheet = \"Sheet1\")\n\n# Read a specific sheet by index\ndata_index &lt;- read_excel(\"/Users/owencallahan/Downloads/Fitness.xlsx\", sheet = 1)\n\n\n\n\n\ndata &lt;- c(1,2,3,4)\nwrite.csv(data, \"my_data.csv\")\n\n\n\n\n\nTidying data is an essential part of data analysis in R, making your data easier to work with and analyze. We’ll use the tidyverse suite of packages, which includes dplyr, tidyr, and readr among others. Below is a step-by-step tutorial with code examples to demonstrate how to tidy data in R.\n\n\nPivoting longer:\n\nwide_data &lt;- tibble(\n  id = 1:3,\n  age = c(25, 30, 35),\n  height = c(175, 180, 165),\n  weight = c(70, 80, 65)\n)\n\n# Convert to long format\nlong_data &lt;- wide_data %&gt;%\n  pivot_longer(cols = c(age, height, weight), names_to = \"measure\", values_to = \"value\")\n\nprint(long_data)\n\n# A tibble: 9 × 3\n     id measure value\n  &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n1     1 age        25\n2     1 height    175\n3     1 weight     70\n4     2 age        30\n5     2 height    180\n6     2 weight     80\n7     3 age        35\n8     3 height    165\n9     3 weight     65\n\n\nPivoting wider:\n\n# Convert back to wide format\nwide_again &lt;- long_data %&gt;%\n  pivot_wider(names_from = measure, values_from = value)\n\nprint(wide_again)\n\n# A tibble: 3 × 4\n     id   age height weight\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     1    25    175     70\n2     2    30    180     80\n3     3    35    165     65\n\n\nSeparate columns:\n\ndata &lt;- tibble(\n  name = c(\"John_Doe\", \"Jane_Smith\", \"Alice_Johnson\")\n)\n\n# Separate into first and last names\nseparated_data &lt;- data %&gt;%\n  separate(name, into = c(\"first_name\", \"last_name\"), sep = \"_\")\n\nprint(separated_data)\n\n# A tibble: 3 × 2\n  first_name last_name\n  &lt;chr&gt;      &lt;chr&gt;    \n1 John       Doe      \n2 Jane       Smith    \n3 Alice      Johnson  \n\n\nUnite columns:\n\nunited_data &lt;- separated_data %&gt;%\n  unite(\"full_name\", first_name, last_name, sep = \" \")\n\n\n\n\nSelecting Columns\n\nselected_data &lt;- mtcars %&gt;%\n  select(cyl, gear)\n\nhead(selected_data)\n\n                  cyl gear\nMazda RX4           6    4\nMazda RX4 Wag       6    4\nDatsun 710          4    4\nHornet 4 Drive      6    3\nHornet Sportabout   8    3\nValiant             6    3\n\n\nFiltering Rows\n\nfiltered_data &lt;- mtcars %&gt;%\n  filter(gear &lt; 4)\n\nhead(filtered_data)\n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360        14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 450SE        16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL        17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n\n\nMutating Columns\n\nmutated_data &lt;- mtcars %&gt;%\n  mutate(speed.scale = qsec/wt)\n\nhead(mutated_data %&gt;%\n        select(speed.scale))\n\n                  speed.scale\nMazda RX4            6.282443\nMazda RX4 Wag        5.920000\nDatsun 710           8.021552\nHornet 4 Drive       6.046656\nHornet Sportabout    4.947674\nValiant              5.843931\n\n\nSummarizing Data\n\nsummary_data &lt;- mtcars %&gt;%\n  group_by(gear) %&gt;%\n  summarize(\n    average_wt = mean(wt),\n    count = n()\n  )\n\nprint(summary_data)\n\n# A tibble: 3 × 3\n   gear average_wt count\n  &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1     3       3.89    15\n2     4       2.62    12\n3     5       2.63     5\n\n\n\n\n\n\n# Identify missing values\nmissing_data &lt;- mtcars %&gt;%\n  filter(is.na(wt))\n\n# Remove rows with missing values\ncleaned_data &lt;- data %&gt;%\n  drop_na()\n\n# Replace missing values with a specific value\nfilled_data &lt;- data %&gt;%\n  replace_na(list(height = 170, weight = 70))\n\n\n\n\n\ndata1 &lt;- tibble(\n  id = 1:3,\n  name = c(\"John\", \"Jane\", \"Alice\")\n)\n\ndata2 &lt;- tibble(\n  id = 1:3,\n  score = c(85, 90, 78)\n)\n\n# Inner join\njoined_data &lt;- data1 %&gt;%\n  inner_join(data2, by = \"id\")\n\nprint(joined_data)\n\n# A tibble: 3 × 3\n     id name  score\n  &lt;int&gt; &lt;chr&gt; &lt;dbl&gt;\n1     1 John     85\n2     2 Jane     90\n3     3 Alice    78\n\n\n\n\n\n\nWhen I learned how to create my own functions, I felt like the creative side of R expanded beyond my expectations. I could tailor a command to address exactly what I needed to perform on a given set of data.\n\n\nThe function() command is a fundamental building block in R, enabling users to create their own functions. This is crucial for both simplifying repetitive tasks and organizing code in a clean, efficient manner. Here’s what the function() command can do for new learners using R:\n\nEncapsulate Repetitive Tasks\n\nCreating functions allows you to encapsulate repetitive code into reusable blocks. This reduces redundancy and makes your scripts more concise and readable. For example, if you frequently perform the same data transformation, you can write a function for it and call it whenever needed.\n\nOrganize Code\n\nFunctions help in organizing code logically. By breaking down complex procedures into smaller, manageable functions, you make your code more modular and easier to debug and maintain.\n\nImprove Readability\n\nUsing functions can significantly enhance the readability of your code. Descriptive function names and clear parameter definitions help others (and your future self) understand the purpose and usage of the code more quickly.\n\nParameterization\n\nFunctions allow you to use parameters to make your code more flexible. Instead of hard-coding values, you can pass different arguments to your functions, making them adaptable to various inputs and scenarios.\n\nEnhance Reproducibility\n\nFunctions contribute to reproducibility in your analyses. By encapsulating specific tasks, you ensure that the same operations can be repeated with different data or settings, leading to consistent results.\n\nPromote Code Reuse\n\nOnce you write a function, you can reuse it across different projects. This saves time and effort, as you don’t need to rewrite the same code for similar tasks.\n\n\nExample of Using function() in R\n\n# Define a function to calculate the mean of a numeric vector\ncalculate_mean &lt;- function(numbers) {\n  mean_value &lt;- mean(numbers)\n  return(mean_value)\n}\n\n# Use the function with a numeric vector\nsample_data &lt;- c(4, 8, 15, 16, 23, 42)\naverage &lt;- calculate_mean(sample_data)\nprint(average)\n\n[1] 18\n\n\n\n\n\nCorrelation measures the strength and direction of the relationship between two variables.\n\n# Correlation matrix\ncor_matrix &lt;- cor(mtcars)\nprint(cor_matrix)\n\n            mpg        cyl       disp         hp        drat         wt\nmpg   1.0000000 -0.8521620 -0.8475514 -0.7761684  0.68117191 -0.8676594\ncyl  -0.8521620  1.0000000  0.9020329  0.8324475 -0.69993811  0.7824958\ndisp -0.8475514  0.9020329  1.0000000  0.7909486 -0.71021393  0.8879799\nhp   -0.7761684  0.8324475  0.7909486  1.0000000 -0.44875912  0.6587479\ndrat  0.6811719 -0.6999381 -0.7102139 -0.4487591  1.00000000 -0.7124406\nwt   -0.8676594  0.7824958  0.8879799  0.6587479 -0.71244065  1.0000000\nqsec  0.4186840 -0.5912421 -0.4336979 -0.7082234  0.09120476 -0.1747159\nvs    0.6640389 -0.8108118 -0.7104159 -0.7230967  0.44027846 -0.5549157\nam    0.5998324 -0.5226070 -0.5912270 -0.2432043  0.71271113 -0.6924953\ngear  0.4802848 -0.4926866 -0.5555692 -0.1257043  0.69961013 -0.5832870\ncarb -0.5509251  0.5269883  0.3949769  0.7498125 -0.09078980  0.4276059\n            qsec         vs          am       gear        carb\nmpg   0.41868403  0.6640389  0.59983243  0.4802848 -0.55092507\ncyl  -0.59124207 -0.8108118 -0.52260705 -0.4926866  0.52698829\ndisp -0.43369788 -0.7104159 -0.59122704 -0.5555692  0.39497686\nhp   -0.70822339 -0.7230967 -0.24320426 -0.1257043  0.74981247\ndrat  0.09120476  0.4402785  0.71271113  0.6996101 -0.09078980\nwt   -0.17471588 -0.5549157 -0.69249526 -0.5832870  0.42760594\nqsec  1.00000000  0.7445354 -0.22986086 -0.2126822 -0.65624923\nvs    0.74453544  1.0000000  0.16834512  0.2060233 -0.56960714\nam   -0.22986086  0.1683451  1.00000000  0.7940588  0.05753435\ngear -0.21268223  0.2060233  0.79405876  1.0000000  0.27407284\ncarb -0.65624923 -0.5696071  0.05753435  0.2740728  1.00000000\n\n# Correlation between two variables\ncor(mtcars$mpg, mtcars$hp)\n\n[1] -0.7761684\n\n\n\n\n\nLinear regression models the relationship between a dependent variable and one or more independent variables.\n\n# Simple linear regression\nmodel &lt;- lm(mpg ~ hp, data = mtcars)\nsummary(model)\n\n\nCall:\nlm(formula = mpg ~ hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7121 -2.1122 -0.8854  1.5819  8.2360 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 30.09886    1.63392  18.421  &lt; 2e-16 ***\nhp          -0.06823    0.01012  -6.742 1.79e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 30 degrees of freedom\nMultiple R-squared:  0.6024,    Adjusted R-squared:  0.5892 \nF-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n\n# Multiple linear regression\nmodel2 &lt;- lm(mpg ~ hp + wt, data = mtcars)\nsummary(model2)\n\n\nCall:\nlm(formula = mpg ~ hp + wt, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.941 -1.600 -0.182  1.050  5.854 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***\nhp          -0.03177    0.00903  -3.519  0.00145 ** \nwt          -3.87783    0.63273  -6.129 1.12e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.593 on 29 degrees of freedom\nMultiple R-squared:  0.8268,    Adjusted R-squared:  0.8148 \nF-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12\n\n\n\n\n\nANOVA tests the difference in means among groups.\n\n# One-way ANOVA\nanova_model &lt;- aov(mpg ~ as.factor(cyl), data = mtcars)\nsummary(anova_model)\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nas.factor(cyl)  2  824.8   412.4    39.7 4.98e-09 ***\nResiduals      29  301.3    10.4                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Two-way ANOVA\nanova_model2 &lt;- aov(mpg ~ as.factor(cyl) + as.factor(gear), data = mtcars)\nsummary(anova_model2)\n\n                Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nas.factor(cyl)   2  824.8   412.4   38.00 1.41e-08 ***\nas.factor(gear)  2    8.3     4.1    0.38    0.687    \nResiduals       27  293.0    10.9                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nT-tests compare the means of two groups.\n\n# One-sample t-test\nt.test(mtcars$mpg, mu = 20)\n\n\n    One Sample t-test\n\ndata:  mtcars$mpg\nt = 0.08506, df = 31, p-value = 0.9328\nalternative hypothesis: true mean is not equal to 20\n95 percent confidence interval:\n 17.91768 22.26357\nsample estimates:\nmean of x \n 20.09062 \n\n# Two-sample t-test\nt.test(mpg ~ as.factor(am), data = mtcars)\n\n\n    Welch Two Sample t-test\n\ndata:  mpg by as.factor(am)\nt = -3.7671, df = 18.332, p-value = 0.001374\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -11.280194  -3.209684\nsample estimates:\nmean in group 0 mean in group 1 \n       17.14737        24.39231 \n\n\n\n\n\nChi-square tests are used for categorical data to test relationships between variables.\n\n# Create a contingency table\ntable_data &lt;- table(mtcars$cyl, mtcars$gear)\n\n# Chi-square test\nchisq.test(table_data)\n\nWarning in chisq.test(table_data): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  table_data\nX-squared = 18.036, df = 4, p-value = 0.001214\n\n\n\n\n\nPCA reduces the dimensionality of the data while preserving as much variance as possible. An excellent tutorial on PCA can be found by clicking this link: https://www.youtube.com/watch?\n\n# Perform PCA\npca_result &lt;- prcomp(mtcars, scale. = TRUE)\n\n# Summary of PCA\nsummary(pca_result)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6    PC7\nStandard deviation     2.5707 1.6280 0.79196 0.51923 0.47271 0.46000 0.3678\nProportion of Variance 0.6008 0.2409 0.05702 0.02451 0.02031 0.01924 0.0123\nCumulative Proportion  0.6008 0.8417 0.89873 0.92324 0.94356 0.96279 0.9751\n                           PC8    PC9    PC10   PC11\nStandard deviation     0.35057 0.2776 0.22811 0.1485\nProportion of Variance 0.01117 0.0070 0.00473 0.0020\nCumulative Proportion  0.98626 0.9933 0.99800 1.0000\n\n\n\n\n\nK-means clustering groups data into k clusters.\n\n# Perform K-means clustering on the mtcars dataset\nset.seed(123)\nkmeans_result &lt;- kmeans(mtcars[, c(\"mpg\", \"hp\")], centers = 3)\n\n# Add cluster results to the original mtcars data\nmtcars$cluster &lt;- as.factor(kmeans_result$cluster)\n\n# Print the first few rows to check the results\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb cluster\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4       3\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4       3\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1       3\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1       3\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2       2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1       3\n\n\n\n\n\nLogistic regression models the probability of a binary outcome.\n\n# Logistic regression\nlogit_model &lt;- glm(am ~ hp + wt, data = mtcars, family = binomial)\nsummary(logit_model)\n\n\nCall:\nglm(formula = am ~ hp + wt, family = binomial, data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) 18.86630    7.44356   2.535  0.01126 * \nhp           0.03626    0.01773   2.044  0.04091 * \nwt          -8.08348    3.06868  -2.634  0.00843 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 10.059  on 29  degrees of freedom\nAIC: 16.059\n\nNumber of Fisher Scoring iterations: 8\n\n\n\n\n\n\n# Decomposition\n\n# Sample time series data\nts_data &lt;- ts(AirPassengers, frequency = 12)\n\n# Decompose the time series\ndecomposed &lt;- decompose(ts_data)\nplot(decomposed)\n\n\n\n\n\n\n\n# Forecasting\n\n# Install and load the forecast package\nlibrary(forecast)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n# Fit an ARIMA model\nfit &lt;- auto.arima(ts_data)\nforecasted &lt;- forecast(fit, h = 12)\nplot(forecasted)\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s explore one of my favorite, and also one of the most essential packages in R, ggplot2.\n\n\nFirst, you need to install and load the ggplot2 package:\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\ninstall.packages(\"ggplot2\")\n\n\nThe downloaded binary packages are in\n    /var/folders/k3/v39j6g_x4bv7mv_xq03986d00000gn/T//RtmpQAa8X7/downloaded_packages\n\nlibrary(ggplot2)\n\n\n\n\nggplot2 follows the grammar of graphics, which means you build plots layer by layer. The essential components are:\n\nData: The dataset you’re plotting.\nAesthetics (aes): The mapping of variables to visual properties like x and y coordinates, colors, sizes, etc.\nGeometries (geom): The type of plot you want to create (e.g., points, lines, bars).\nFacets: Subplots based on the values of one or more variables.\nScales: Control how data values are mapped to visual properties.\nCoordinate Systems: Control the coordinate space.\nThemes: Control the appearance of the plot.\n\n\n\n\nScatter Plot\n\n# Load example data\ndata(mtcars)\n\n# Create a scatter plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\n\n\n\n\nLine Plot\n\n# Create a line plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_line() +\n  labs(title = \"Line Plot of MPG vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\n\n\n\n\nBar Plot\n\n# Create a bar plot\nggplot(data = mtcars, aes(x = factor(cyl))) +\n  geom_bar() +\n  labs(title = \"Bar Plot of Cylinder Counts\",\n       x = \"Number of Cylinders\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\nHistogram\n\n# Create a histogram\nggplot(data = mtcars, aes(x = mpg)) +\n  geom_histogram(binwidth = 2) +\n  labs(title = \"Histogram of MPG\",\n       x = \"Miles Per Gallon (MPG)\",\n       y = \"Frequency\")\n\n\n\n\n\n\n\n\nBox Plot\n\n# Create a box plot\nggplot(data = mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot() +\n  labs(title = \"Box Plot of MPG by Cylinder Count\",\n       x = \"Number of Cylinders\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\n\n\n\n\n\n\n\nAdding Colors\n\n# Scatter plot with color\nggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl))) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight by Cylinder Count\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\",\n       color = \"Cylinders\")\n\n\n\n\n\n\n\n\nAdding Size\n\n# Scatter plot with color and size\nggplot(data = mtcars, aes(x = wt, y = mpg, color = factor(cyl), size = hp)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight by Cylinder Count\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\",\n       color = \"Cylinders\",\n       size = \"Horsepower\")\n\n\n\n\n\n\n\n\nFaceting\n\n# Faceted scatter plot\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  facet_wrap(~ cyl) +\n  labs(title = \"Scatter Plot of MPG vs Weight by Cylinder Count\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\")\n\n\n\n\n\n\n\n\nThemes\n\n# Scatter plot with theme\nggplot(data = mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Scatter Plot of MPG vs Weight\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles Per Gallon (MPG)\") +\n  theme_minimal()"
  },
  {
    "objectID": "timeseries.html",
    "href": "timeseries.html",
    "title": "Forecasting Tuna Fisheries",
    "section": "",
    "text": "Forecasting Tuna Fisheries\nThis project was part of a statistics course in environmental econometrics. My task was to develop a robust model to predict trends in environmental data, as well as write a paper that critically analyses the findings and offers potential contributing factors.\nThe best resource I found that helped me understand the process of this analysis was in an online book (Gijón Air Pollution - An exercise of visualization and forecasting). Check out chapter 8 for ARIMA models! (https://bookdown.org/sergioberdiales/tfm-kschool_gijon_air_pollution/forecasting-models-arima.html)\nDataset retrieved from TCASHION on Kaggle (https://www.kaggle.com/datasets/tcashion/tokyo-wholesale-tuna-prices/code)\nThe Tsukiji fish market, which operated until 2018, was the largest wholesale fish market in the world, specializing in sashimi tunas.\nSpecies Covered:\n\nBluefin Tuna\nSouthern Bluefin Tuna\nBigeye Tuna\n\nMeasures\n\nQuantity - Metric tonnes (i.e, 1000 kilograms)\nPrice - Yen / kilogram\n\n\n1. Install and Load Packages\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\nlibrary(fpp2)\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n── Attaching packages ────────────────────────────────────────────── fpp2 2.5 ──\n\n\n✔ ggplot2   3.5.1      ✔ fma       2.5   \n✔ forecast  8.22.0     ✔ expsmooth 2.3   \n\n\n\n\nlibrary(TTR)\nlibrary(forecast)\nlibrary(tseries)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr   1.1.4     ✔ stringr 1.5.1\n✔ forcats 1.0.0     ✔ tibble  3.2.1\n✔ purrr   1.0.2     ✔ tidyr   1.3.1\n✔ readr   2.1.5     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n2. Plot Quantity and Price\n\ntuna&lt;-read.csv(\"/Users/owencallahan/Desktop/R/tokyo_wholesale_tuna_prices.csv\")\n\n# Combine month and year collumn\ntuna &lt;- tuna %&gt;%\n  mutate(date = make_date(year, month))\n\n# Sort data by date\ntuna &lt;- tuna %&gt;% arrange(date)\n\n# Plot Quantity\ntuna_quantity &lt;- tuna %&gt;%\n  filter(measure == \"Quantity\")\n\nggplot(tuna_quantity, aes(x = date, y = value)) +\n  geom_line() +\n  labs(title = \"Tuna Quantity Over Time\", x = \"Date\", y = \"Quantity\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Plot Price\ntuna_price &lt;- tuna %&gt;%\n  filter(measure == \"Price\")\n\nggplot(tuna_price, aes(x = date, y = value)) +\n  geom_line() +\n  labs(title = \"Tuna Prices Over Time\", x = \"Date\", y = \"Price\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n3. Model Selection\n\nTrend: Look for any long-term movements or patterns in the data.\nSeasonality: Check for repeating patterns at fixed intervals\nRandomness: Assess if there are irregular fluctuations or noise.\n\nDecomposition: helps in separating a time series into its components:\n\nWhen to Use:\n\nIf there is a clear trend and/or seasonal pattern in the data, decomposition can help isolate these components for better analysis and forecasting.\n\n\nMoving Average Smoothing: is used to reduce short-term fluctuations in the data:\n\nWhen to Use:\n\nTo smooth out irregular fluctuations or noise in the data, making underlying patterns more apparent.\n\n\nExponential Smoothing: assigns exponentially decreasing weights to older observations:\n\nWhen to Use:\n\nWhen the data has no clear trend or seasonality but exhibits random fluctuations.\nFor short-term forecasting where recent data points are more relevant.\n\n\n\n# Quantity\nts_quan &lt;- ts(tuna_quantity$value, start = 2003, end = 2017,frequency = 12)  \ndecomposed1 &lt;- decompose(ts_quan)\nplot(decomposed1)\n\n\n\n\n\n\n\n# Price\nts_price &lt;- ts(tuna_price$value, start = 2003, end = 2017, frequency = 12)  \ndecomposed2 &lt;- decompose(ts_price)\nplot(decomposed2) \n\n\n\n\n\n\n\n\nKey Takeaways\n\nSeasonal Component: Appears consistent, suggesting recurring patterns at regular intervals.\nTrend Component: Appears less predictable, indicating potential long-term shifts or fluctuations that are not easily captured by a simple pattern.\nRandom Component: Shows variability, indicating unexplained fluctuations or noise in the data.\n\n\n\n4. Test for Stationarity\nThe ADF test is a statistical test that assesses whether a time series is stationary. Stationarity means that the statistical properties of a time series (such as mean, variance, and autocorrelation) do not change over time.\n\nA more negative ADF statistic indicates stronger evidence for stationarity.\n\n\n# ADF test\nadf_test &lt;- adf.test(ts_price)\nprint(adf_test)\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_price\nDickey-Fuller = -2.7664, Lag order = 5, p-value = 0.2563\nalternative hypothesis: stationary\n\nadf_test &lt;- adf.test(ts_quan)\nprint(adf_test)\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_quan\nDickey-Fuller = -3.0099, Lag order = 5, p-value = 0.1547\nalternative hypothesis: stationary\n\n\nKey Takeaways\n\nPrice model is not stationary\nQuantity model is not stationary\n\n\n\n5. Fit an ARIMA model\n\n# Fit model\nfit_price &lt;- auto.arima(ts_price)\nfit_quan &lt;- auto.arima(ts_quan)\n\n# Summary\nsummary(fit_price)\n\nSeries: ts_price \nARIMA(0,0,0)(0,1,1)[12] with drift \n\nCoefficients:\n         sma1    drift\n      -0.8494  -1.3377\ns.e.   0.0865   0.8690\n\nsigma^2 = 235806:  log likelihood = -1200.48\nAIC=2406.96   AICc=2407.12   BIC=2416.13\n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE     MAPE     MASE        ACF1\nTraining set 12.06902 465.0506 303.0941 -3.201486 13.59532 0.840122 -0.05292377\n\nsummary(fit_quan)\n\nSeries: ts_quan \nARIMA(0,0,0)(1,1,0)[12] with drift \n\nCoefficients:\n         sar1   drift\n      -0.4004  0.1308\ns.e.   0.0729  0.6294\n\nsigma^2 = 16957:  log likelihood = -987.28\nAIC=1980.57   AICc=1980.72   BIC=1989.73\n\nTraining set error measures:\n                    ME     RMSE      MAE      MPE     MAPE      MASE\nTraining set 0.9639297 124.7093 87.62616 -578.987 607.8686 0.9004508\n                    ACF1\nTraining set -0.01891516\n\n# Forecasting Price\nfore_price &lt;- forecast(fit_price, h = 12)\nautoplot(fore_price) +\n  labs(title = \"Forecasted Tuna Prices using ARIMA\", x = \"Date\", y = \"Price\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Forecasting Quantity\nfore_quan &lt;- forecast(fit_quan, h = 12) \nautoplot(fore_quan) +\n  labs(title = \"Forecasted Tuna Quantity using ARIMA\", x = \"Date\", y = \"Price\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n6. Evaluate Results\n\nResidual Plot: Inspect the plot to ensure the residuals are centered around zero and exhibit constant variance. If there are patterns or trends in the residuals, it suggests that the model may need improvement.\nMean Squared Error (MSE): A smaller MSE indicates that the model’s predictions are closer to the actual values, implying better model performance.\n\n\n# Residuals\nresiduals_p &lt;- residuals(fit_price)\nresiduals_q &lt;- residuals(fit_quan)\n\n# Plot residuals\nautoplot(residuals_p) + \n  labs(title = \"Price Residuals of ARIMA Model\", x = \"Date\", y = \"Residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\nautoplot(residuals_q) + \n  labs(title = \"Quantity Residuals of ARIMA Model\", x = \"Date\", y = \"Residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Calculate performance metrics\nMSE_p &lt;- mean(residuals_p^2)\nRMSE_p &lt;- sqrt(MSE_p)\nMAE_p &lt;- mean(abs(residuals_p))\n\nMSE_q &lt;- mean(residuals_q^2)\nRMSE_q &lt;- sqrt(MSE_q)\nMAE_q &lt;- mean(abs(residuals_q))\n\n# R-squared\nSS_total &lt;- sum((ts_price - mean(ts_price))^2)\nSS_residual &lt;- sum(residuals_p^2)\nR_squared_p &lt;- 1 - (SS_residual / SS_total)\n\nSS_total_q &lt;- sum((ts_quan - mean(ts_quan))^2)\nSS_residual_q &lt;- sum(residuals_q^2)\nR_squared_q &lt;- 1 - (SS_residual_q / SS_total_q)\n\ncat(\"MSE:\", MSE_p, \"\\n\")\n\nMSE: 216272.1 \n\ncat(\"RMSE:\", RMSE_p, \"\\n\")\n\nRMSE: 465.0506 \n\ncat(\"MAE:\", MAE_p, \"\\n\")\n\nMAE: 303.0941 \n\ncat(\"R-squared:\", R_squared_p, \"\\n\")\n\nR-squared: 0.6573022 \n\ncat(\"MSE:\", MSE_q, \"\\n\")\n\nMSE: 15552.41 \n\ncat(\"RMSE:\", RMSE_q, \"\\n\")\n\nRMSE: 124.7093 \n\ncat(\"MAE:\", MAE_q, \"\\n\")\n\nMAE: 87.62616 \n\ncat(\"R-squared:\", R_squared_q, \"\\n\")\n\nR-squared: 0.7745865 \n\n\nKey Takeaways\n\nPrice Model:\n\nThe price model shows moderate explanatory power (R² = 0.657), but the high MSE and RMSE indicate that the residuals are substantial, suggesting potential improvements or alternative modeling techniques might be needed.\n\nQuantity Model:\n\nThe quantity model shows better performance with higher R² (0.775) and lower error metrics (MSE, RMSE, MAE), indicating a good fit and more accurate predictions compared to the price model.\n\n\n\n\n7. Exploratory Data Analysis with Different Species\n\nbt_price &lt;- tuna %&gt;%\n  filter(measure == \"Price\",\n         species == \"Bluefin Tuna\")\nts_bt &lt;- ts(bt_price$value, start = 2003, end = 2017, frequency = 12)  \ndecomposed3 &lt;- decompose(ts_bt)\nplot(decomposed3) \n\n\n\n\n\n\n\nsbt_price &lt;- tuna %&gt;%\n  filter(measure == \"Price\",\n         species == \"Southern Bluefin Tuna\")\nts_sbt &lt;- ts(sbt_price$value, start = 2003, end = 2017, frequency = 12)  \ndecomposed4 &lt;- decompose(ts_sbt)\nplot(decomposed4) \n\n\n\n\n\n\n\nbgt_price &lt;- tuna %&gt;%\n  filter(measure == \"Price\",\n         species == \"Bigeye Tuna\")\nts_bgt &lt;- ts(bgt_price$value, start = 2003, end = 2017, frequency = 12)  \ndecomposed5 &lt;- decompose(ts_bgt)\nplot(decomposed5) \n\n\n\n\n\n\n\n\nKey Takeaways\n\nSeasonal Component: Appears consistent, suggesting recurring patterns at regular intervals.\nTrend Component: Bluefin Tuna has a predictable trend, however Southern Bluefin and Bigeye Tuna appears less predictable, both indicating potential long-term jump in price over time.\nRandom Component: Shows variability, indicating unexplained fluctuations or noise in the data.\n\n\n# Quickly check for stationarity\nadf.test(ts_bt)\n\nWarning in adf.test(ts_bt): p-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_bt\nDickey-Fuller = -4.0301, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\nadf.test(ts_sbt)\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_sbt\nDickey-Fuller = -1.5708, Lag order = 5, p-value = 0.7555\nalternative hypothesis: stationary\n\nadf.test(ts_bgt)\n\nWarning in adf.test(ts_bgt): p-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts_bgt\nDickey-Fuller = -6.438, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\nKey Takeaways\n\nBluefin Tuna: Stationary\nSouthern Bluefin: Non-Stationary\nBigeye Tuna: Stationary\n\n\n# Fit model\nfit_bt &lt;- auto.arima(ts_bt)\nfit_sbt &lt;- auto.arima(ts_sbt)\nfit_bgt &lt;- auto.arima(ts_bgt)\n\nsummary(fit_bt)\n\nSeries: ts_bt \nARIMA(4,0,0)(2,0,1)[12] with non-zero mean \n\nCoefficients:\n         ar1      ar2     ar3      ar4     sar1     sar2    sma1       mean\n      0.0691  -0.0463  0.7589  -0.1061  -0.7768  -0.3637  0.7588  3114.3466\ns.e.  0.0788   0.0502  0.0496   0.0795   0.1202   0.0820  0.1127    75.9551\n\nsigma^2 = 162826:  log likelihood = -1254.42\nAIC=2526.83   AICc=2527.96   BIC=2555\n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 2.517485 393.8503 296.0585 -1.518045 9.976024 0.4364676\n                    ACF1\nTraining set 0.002743469\n\n# Forecasting Different Specices\nfore_price3 &lt;- forecast(fit_bt, h = 12) \nautoplot(fore_price3) +\n  labs(title = \"Forecasted Bluefin Tuna Prices using ARIMA\", x = \"Date\", y = \"Price\") +\n  theme_minimal()\n\n\n\n\n\n\n\nfore_price4 &lt;- forecast(fit_sbt, h = 12) \nautoplot(fore_price4) +\n  labs(title = \"Forecasted Southern Bluefin Tuna Prices using ARIMA\", x = \"Date\", y = \"Price\") +\n  theme_minimal()\n\n\n\n\n\n\n\nfore_price5 &lt;- forecast(fit_bgt, h = 12)\nautoplot(fore_price5) +\n  labs(title = \"Forecasted Bigeye Tuna Prices using ARIMA\", x = \"Date\", y = \"Price\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# Residuals\nresiduals_1 &lt;- residuals(fit_bt)\nresiduals_2 &lt;- residuals(fit_sbt)\nresiduals_3 &lt;- residuals(fit_bgt)\n\n# Plot residuals\nautoplot(residuals_1) + \n  labs(title = \"Bluefin Tuna Residuals of ARIMA Model\", x = \"Date\", y = \"Residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\nautoplot(residuals_2) + \n  labs(title = \"Southern Bluefin Tuna Residuals of ARIMA Model\", x = \"Date\", y = \"Residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\nautoplot(residuals_3) + \n  labs(title = \"Bigeye Tuna Residuals of ARIMA Model\", x = \"Date\", y = \"Residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n8. Model Improvement\n\n# Price model\nfit_price1 &lt;- auto.arima(ts_price, stepwise = FALSE, approximation = FALSE)\n\n# Quantity model\nfit_quantity1 &lt;- auto.arima(ts_quan, stepwise = FALSE, approximation = FALSE)\n\n# Generate forecasts and calculate residuals again\nfore_price &lt;- forecast(fit_price1, h = 12)\nresiduals_p1 &lt;- residuals(fit_price1)\n\nfore_quantity &lt;- forecast(fit_quantity1, h = 12)\nresiduals_q1 &lt;- residuals(fit_quantity1)\n\n# Performance metrics\nMSE_p1 &lt;- mean(residuals_p1^2)\nRMSE_p1 &lt;- sqrt(MSE_p1)\nMAE_p1 &lt;- mean(abs(residuals_p1))\nSS_total_p1 &lt;- sum((ts_price - mean(ts_price))^2)\nSS_residual_p1 &lt;- sum(residuals_p1^2)\nR_squared_p1 &lt;- 1 - (SS_residual_p1 / SS_total_p1)\n\nMSE_q1 &lt;- mean(residuals_q1^2)\nRMSE_q1 &lt;- sqrt(MSE_q1)\nMAE_q1 &lt;- mean(abs(residuals_q1))\nSS_total_q1 &lt;- sum((ts_quan - mean(ts_quan))^2)\nSS_residual_q1 &lt;- sum(residuals_q1^2)\nR_squared_q1 &lt;- 1 - (SS_residual_q1 / SS_total_q1)\n\ncat(\"Updated Metrics for Price Model\\n\")\n\nUpdated Metrics for Price Model\n\ncat(\"MSE:\", MSE_p1, \"\\n\")\n\nMSE: 165858.2 \n\ncat(\"RMSE:\", RMSE_p1, \"\\n\")\n\nRMSE: 407.2569 \n\ncat(\"MAE:\", MAE_p1, \"\\n\")\n\nMAE: 296.6826 \n\ncat(\"R-squared:\", R_squared_p1, \"\\n\")\n\nR-squared: 0.7371865 \n\ncat(\"Updated Metrics for Quantity Model\\n\")\n\nUpdated Metrics for Quantity Model\n\ncat(\"MSE:\", MSE_q1, \"\\n\")\n\nMSE: 9458.524 \n\ncat(\"RMSE:\", RMSE_q1, \"\\n\")\n\nRMSE: 97.25494 \n\ncat(\"MAE:\", MAE_q1, \"\\n\")\n\nMAE: 70.89373 \n\ncat(\"R-squared:\", R_squared_q1, \"\\n\")\n\nR-squared: 0.86291 \n\n\n\n\n9. Diagnostics\n\n# Plot residuals\nautoplot(residuals_p1) + \n  labs(title = \"Residuals of ARIMA Model for Price\", x = \"Date\", y = \"Residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\nautoplot(residuals_q1) + \n  labs(title = \"Residuals of ARIMA Model for Quantity\", x = \"Date\", y = \"Residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# ACF and PACF of residuals\nacf(residuals_p1, main=\"ACF of Residuals for Price\")\n\n\n\n\n\n\n\npacf(residuals_p1, main=\"PACF of Residuals for Price\")\n\n\n\n\n\n\n\nacf(residuals_q1, main=\"ACF of Residuals for Quantity\")\n\n\n\n\n\n\n\npacf(residuals_q1, main=\"PACF of Residuals for Quantity\")\n\n\n\n\n\n\n\n# Ljung-Box test for autocorrelation\nBox.test(residuals_p1, lag = 20, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  residuals_p1\nX-squared = 91.573, df = 20, p-value = 3.93e-11\n\nBox.test(residuals_q1, lag = 20, type = \"Ljung-Box\")\n\n\n    Box-Ljung test\n\ndata:  residuals_q1\nX-squared = 48.397, df = 20, p-value = 0.0003741\n\n\nIn this analysis, I aimed to model and forecast tuna prices and quantities using ARIMA models. After refining the models, there were significant improvements in the performance metrics for both the price and quantity models.\nThe updated price model shows a substantial reduction in error metrics and an increase in explanatory power, with the R-squared value improving from 0.657 to 0.737. This indicates that the model now explains approximately 73.72% of the variance in the price data, suggesting a more accurate and reliable model.\nThe quantity model also shows improved performance, with lower error metrics and a higher R-squared value, now at 0.86291. This indicates that the model explains about 86.29% of the variance in the quantity data.\nThe p-values for both tests are significantly lower than 0.05, indicating that there is still some autocorrelation in the residuals, suggesting that the models could be further refined. However, the current models still provide a substantial improvement over the initial attempts."
  }
]